{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Exit Prediction from Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv files\n",
    "df = pd.read_csv('BankCustomers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Head of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of Dataset\n",
    "df.shape\n",
    "\n",
    "#Contains 10000 rows and 14 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Value counts of target variable\n",
    "df['Exited'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histogram of Age column\n",
    "df['Age'].plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7055\n",
       "0    2945\n",
       "Name: HasCrCard, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#People having Credit Card or not\n",
    "df['HasCrCard'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowNumber          0\n",
       "CustomerId         0\n",
       "Surname            0\n",
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the Unnecessary columns\n",
    "df.drop(['RowNumber','CustomerId','Surname'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  \n",
       "1          0               1        112542.58       0  \n",
       "2          1               0        113931.57       1  \n",
       "3          0               0         93826.63       0  \n",
       "4          1               1         79084.10       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geography , Gender Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "France     5014\n",
       "Germany    2509\n",
       "Spain      2477\n",
       "Name: Geography, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Geography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "geography = pd.get_dummies(df['Geography'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = pd.get_dummies(df['Gender'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df,geography,gender],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  Germany  Spain  Male  \n",
       "0          1               1        101348.88       1        0      0     0  \n",
       "1          0               1        112542.58       0        0      1     0  \n",
       "2          1               0        113931.57       1        0      0     0  \n",
       "3          0               0         93826.63       0        0      0     0  \n",
       "4          1               1         79084.10       0        0      1     0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Geography','Gender'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Age  Tenure    Balance  NumOfProducts  HasCrCard  \\\n",
       "0          619   42       2       0.00              1          1   \n",
       "1          608   41       1   83807.86              1          0   \n",
       "2          502   42       8  159660.80              3          1   \n",
       "3          699   39       1       0.00              2          0   \n",
       "4          850   43       2  125510.82              1          1   \n",
       "\n",
       "   IsActiveMember  EstimatedSalary  Exited  Germany  Spain  Male  \n",
       "0               1        101348.88       1        0      0     0  \n",
       "1               1        112542.58       0        0      1     0  \n",
       "2               0        113931.57       1        0      0     0  \n",
       "3               0         93826.63       0        0      0     0  \n",
       "4               1         79084.10       0        0      1     0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_reorder = ['CreditScore', 'Germany', 'Spain','Male','Age','Tenure','Balance','NumOfProducts','HasCrCard',\n",
    "       'IsActiveMember','EstimatedSalary','Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[column_names_reorder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore  Germany  Spain  Male  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619        0      0     0   42       2       0.00              1   \n",
       "1          608        0      1     0   41       1   83807.86              1   \n",
       "2          502        0      0     0   42       8  159660.80              3   \n",
       "3          699        0      0     0   39       1       0.00              2   \n",
       "4          850        0      1     0   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  \n",
       "1          0               1        112542.58       0  \n",
       "2          1               0        113931.57       1  \n",
       "3          0               0         93826.63       0  \n",
       "4          1               1         79084.10       0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Exited'],axis=1)\n",
    "y = df['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving column names to a variables\n",
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CreditScore', 'Germany', 'Spain', 'Male', 'Age', 'Tenure', 'Balance',\n",
       "       'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for X_scaled_train\n",
    "X_scaled_train = pd.DataFrame(X_scaled_train,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.356500</td>\n",
       "      <td>-0.579467</td>\n",
       "      <td>-0.576388</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>-0.655786</td>\n",
       "      <td>0.345680</td>\n",
       "      <td>-1.218471</td>\n",
       "      <td>0.808436</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>0.974817</td>\n",
       "      <td>1.367670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.203898</td>\n",
       "      <td>1.725723</td>\n",
       "      <td>-0.576388</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>0.294938</td>\n",
       "      <td>-0.348369</td>\n",
       "      <td>0.696838</td>\n",
       "      <td>0.808436</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>0.974817</td>\n",
       "      <td>1.661254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.961472</td>\n",
       "      <td>-0.579467</td>\n",
       "      <td>1.734942</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>-1.416365</td>\n",
       "      <td>-0.695393</td>\n",
       "      <td>0.618629</td>\n",
       "      <td>-0.916688</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>-1.025834</td>\n",
       "      <td>-0.252807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.940717</td>\n",
       "      <td>-0.579467</td>\n",
       "      <td>-0.576388</td>\n",
       "      <td>-1.094993</td>\n",
       "      <td>-1.131148</td>\n",
       "      <td>1.386753</td>\n",
       "      <td>0.953212</td>\n",
       "      <td>-0.916688</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>-1.025834</td>\n",
       "      <td>0.915393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.397337</td>\n",
       "      <td>-0.579467</td>\n",
       "      <td>-0.576388</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>1.625953</td>\n",
       "      <td>1.386753</td>\n",
       "      <td>1.057449</td>\n",
       "      <td>-0.916688</td>\n",
       "      <td>-1.540351</td>\n",
       "      <td>-1.025834</td>\n",
       "      <td>-1.059600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore   Germany     Spain      Male       Age    Tenure   Balance  \\\n",
       "0     0.356500 -0.579467 -0.576388  0.913248 -0.655786  0.345680 -1.218471   \n",
       "1    -0.203898  1.725723 -0.576388  0.913248  0.294938 -0.348369  0.696838   \n",
       "2    -0.961472 -0.579467  1.734942  0.913248 -1.416365 -0.695393  0.618629   \n",
       "3    -0.940717 -0.579467 -0.576388 -1.094993 -1.131148  1.386753  0.953212   \n",
       "4    -1.397337 -0.579467 -0.576388  0.913248  1.625953  1.386753  1.057449   \n",
       "\n",
       "   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0       0.808436   0.649203        0.974817         1.367670  \n",
       "1       0.808436   0.649203        0.974817         1.661254  \n",
       "2      -0.916688   0.649203       -1.025834        -0.252807  \n",
       "3      -0.916688   0.649203       -1.025834         0.915393  \n",
       "4      -0.916688  -1.540351       -1.025834        -1.059600  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for X_scaled_test\n",
    "X_scaled_test = pd.DataFrame(X_scaled_test,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.577496</td>\n",
       "      <td>1.725723</td>\n",
       "      <td>-0.576388</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>-0.655786</td>\n",
       "      <td>-0.695393</td>\n",
       "      <td>0.329937</td>\n",
       "      <td>0.808436</td>\n",
       "      <td>-1.540351</td>\n",
       "      <td>-1.025834</td>\n",
       "      <td>-1.019605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.297297</td>\n",
       "      <td>-0.579467</td>\n",
       "      <td>-0.576388</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>0.390011</td>\n",
       "      <td>-1.389442</td>\n",
       "      <td>-1.218471</td>\n",
       "      <td>0.808436</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>0.974817</td>\n",
       "      <td>0.798883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.525607</td>\n",
       "      <td>-0.579467</td>\n",
       "      <td>1.734942</td>\n",
       "      <td>-1.094993</td>\n",
       "      <td>0.485083</td>\n",
       "      <td>-0.348369</td>\n",
       "      <td>-1.218471</td>\n",
       "      <td>0.808436</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>-1.025834</td>\n",
       "      <td>-0.727980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.511492</td>\n",
       "      <td>1.725723</td>\n",
       "      <td>-0.576388</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>1.911170</td>\n",
       "      <td>1.039728</td>\n",
       "      <td>0.689272</td>\n",
       "      <td>0.808436</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>0.974817</td>\n",
       "      <td>1.221387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.951094</td>\n",
       "      <td>-0.579467</td>\n",
       "      <td>1.734942</td>\n",
       "      <td>-1.094993</td>\n",
       "      <td>-1.131148</td>\n",
       "      <td>0.692704</td>\n",
       "      <td>0.782839</td>\n",
       "      <td>-0.916688</td>\n",
       "      <td>0.649203</td>\n",
       "      <td>0.974817</td>\n",
       "      <td>0.247560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore   Germany     Spain      Male       Age    Tenure   Balance  \\\n",
       "0    -0.577496  1.725723 -0.576388  0.913248 -0.655786 -0.695393  0.329937   \n",
       "1    -0.297297 -0.579467 -0.576388  0.913248  0.390011 -1.389442 -1.218471   \n",
       "2    -0.525607 -0.579467  1.734942 -1.094993  0.485083 -0.348369 -1.218471   \n",
       "3    -1.511492  1.725723 -0.576388  0.913248  1.911170  1.039728  0.689272   \n",
       "4    -0.951094 -0.579467  1.734942 -1.094993 -1.131148  0.692704  0.782839   \n",
       "\n",
       "   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "0       0.808436  -1.540351       -1.025834        -1.019605  \n",
       "1       0.808436   0.649203        0.974817         0.798883  \n",
       "2       0.808436   0.649203       -1.025834        -0.727980  \n",
       "3       0.808436   0.649203        0.974817         1.221387  \n",
       "4      -0.916688   0.649203        0.974817         0.247560  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing models and Layers \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "#Adding the input layer and the first hidden layer\n",
    "\n",
    "#input_dim is the total features to predict the label\n",
    "#units are the neurons in the first hidden layer\n",
    "# kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights. (0 to 1)\n",
    "#In case of statistical distribution, the library will generate numbers from that statistical distribution and use as starting weights.\n",
    "model.add(Dense(activation = 'relu', input_dim=11 , units = 6, kernel_initializer = 'uniform'))\n",
    "\n",
    "#Adding second hidden layer\n",
    "model.add(Dense(activation = 'relu', units=6 , kernel_initializer='uniform'))\n",
    "\n",
    "#Adding the output layer\n",
    "model.add(Dense(activation='sigmoid', units=1, kernel_initializer='uniform'))\n",
    "\n",
    "#Compiling the ANN\n",
    "#(binary_crossentropy used when just binary label is there to predict)(just one output)\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 6)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "63/63 [==============================] - 31s 25ms/step - loss: 0.6882 - accuracy: 0.7581 - val_loss: 0.6711 - val_accuracy: 0.8035\n",
      "Epoch 2/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6638 - accuracy: 0.7914 - val_loss: 0.6233 - val_accuracy: 0.8035\n",
      "Epoch 3/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6015 - accuracy: 0.7948 - val_loss: 0.5112 - val_accuracy: 0.8035\n",
      "Epoch 4/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.7956 - val_loss: 0.4387 - val_accuracy: 0.8035\n",
      "Epoch 5/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8032 - val_loss: 0.4274 - val_accuracy: 0.8035\n",
      "Epoch 6/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4560 - accuracy: 0.7855 - val_loss: 0.4234 - val_accuracy: 0.8035\n",
      "Epoch 7/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.7874 - val_loss: 0.4205 - val_accuracy: 0.8035\n",
      "Epoch 8/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7909 - val_loss: 0.4185 - val_accuracy: 0.8035\n",
      "Epoch 9/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.7927 - val_loss: 0.4168 - val_accuracy: 0.8035\n",
      "Epoch 10/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.7950 - val_loss: 0.4157 - val_accuracy: 0.8035\n",
      "Epoch 11/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.7885 - val_loss: 0.4150 - val_accuracy: 0.8035\n",
      "Epoch 12/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.7780 - val_loss: 0.4142 - val_accuracy: 0.8035\n",
      "Epoch 13/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.7879 - val_loss: 0.4133 - val_accuracy: 0.8035\n",
      "Epoch 14/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8007 - val_loss: 0.4128 - val_accuracy: 0.8025\n",
      "Epoch 15/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.7963 - val_loss: 0.4120 - val_accuracy: 0.8060\n",
      "Epoch 16/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.7968 - val_loss: 0.4111 - val_accuracy: 0.8065\n",
      "Epoch 17/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8077 - val_loss: 0.4105 - val_accuracy: 0.8090\n",
      "Epoch 18/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4315 - accuracy: 0.8107 - val_loss: 0.4093 - val_accuracy: 0.8135\n",
      "Epoch 19/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.8087 - val_loss: 0.4084 - val_accuracy: 0.8175\n",
      "Epoch 20/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8140 - val_loss: 0.4074 - val_accuracy: 0.8170\n",
      "Epoch 21/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8147 - val_loss: 0.4062 - val_accuracy: 0.8225\n",
      "Epoch 22/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4426 - accuracy: 0.8127 - val_loss: 0.4047 - val_accuracy: 0.8225\n",
      "Epoch 23/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.8265 - val_loss: 0.4034 - val_accuracy: 0.8230\n",
      "Epoch 24/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8394 - val_loss: 0.4035 - val_accuracy: 0.8270\n",
      "Epoch 25/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8304 - val_loss: 0.4013 - val_accuracy: 0.8270\n",
      "Epoch 26/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8323 - val_loss: 0.4001 - val_accuracy: 0.8285\n",
      "Epoch 27/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4090 - accuracy: 0.8354 - val_loss: 0.3988 - val_accuracy: 0.8310\n",
      "Epoch 28/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4098 - accuracy: 0.8320 - val_loss: 0.3976 - val_accuracy: 0.8335\n",
      "Epoch 29/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4234 - accuracy: 0.8300 - val_loss: 0.3966 - val_accuracy: 0.8330\n",
      "Epoch 30/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4128 - accuracy: 0.8370 - val_loss: 0.3957 - val_accuracy: 0.8350\n",
      "Epoch 31/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4057 - accuracy: 0.8357 - val_loss: 0.3952 - val_accuracy: 0.8365\n",
      "Epoch 32/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8362 - val_loss: 0.3947 - val_accuracy: 0.8360\n",
      "Epoch 33/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4006 - accuracy: 0.8390 - val_loss: 0.3940 - val_accuracy: 0.8365\n",
      "Epoch 34/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8436 - val_loss: 0.3937 - val_accuracy: 0.8385\n",
      "Epoch 35/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3948 - accuracy: 0.8385 - val_loss: 0.3929 - val_accuracy: 0.8370\n",
      "Epoch 36/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8397 - val_loss: 0.3920 - val_accuracy: 0.8370\n",
      "Epoch 37/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4082 - accuracy: 0.8349 - val_loss: 0.3914 - val_accuracy: 0.8405\n",
      "Epoch 38/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3956 - accuracy: 0.8386 - val_loss: 0.3908 - val_accuracy: 0.8385\n",
      "Epoch 39/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4103 - accuracy: 0.8315 - val_loss: 0.3901 - val_accuracy: 0.8385\n",
      "Epoch 40/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4036 - accuracy: 0.8378 - val_loss: 0.3900 - val_accuracy: 0.8385\n",
      "Epoch 41/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3905 - accuracy: 0.8432 - val_loss: 0.3900 - val_accuracy: 0.8390\n",
      "Epoch 42/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4094 - accuracy: 0.8323 - val_loss: 0.3894 - val_accuracy: 0.8425\n",
      "Epoch 43/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4043 - accuracy: 0.8360 - val_loss: 0.3890 - val_accuracy: 0.8400\n",
      "Epoch 44/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3958 - accuracy: 0.8423 - val_loss: 0.3893 - val_accuracy: 0.8415\n",
      "Epoch 45/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4035 - accuracy: 0.8361 - val_loss: 0.3891 - val_accuracy: 0.8405\n",
      "Epoch 46/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3937 - accuracy: 0.8382 - val_loss: 0.3892 - val_accuracy: 0.8420\n",
      "Epoch 47/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3917 - accuracy: 0.8431 - val_loss: 0.3889 - val_accuracy: 0.8430\n",
      "Epoch 48/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4030 - accuracy: 0.8365 - val_loss: 0.3887 - val_accuracy: 0.8420\n",
      "Epoch 49/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4044 - accuracy: 0.8376 - val_loss: 0.3885 - val_accuracy: 0.8420\n",
      "Epoch 50/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3953 - accuracy: 0.8378 - val_loss: 0.3883 - val_accuracy: 0.8420\n",
      "Epoch 51/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4010 - accuracy: 0.8377 - val_loss: 0.3880 - val_accuracy: 0.8430\n",
      "Epoch 52/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3932 - accuracy: 0.8456 - val_loss: 0.3882 - val_accuracy: 0.8410\n",
      "Epoch 53/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3996 - accuracy: 0.8368 - val_loss: 0.3876 - val_accuracy: 0.8420\n",
      "Epoch 54/600\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3976 - accuracy: 0.83 - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8389 - val_loss: 0.3880 - val_accuracy: 0.8410\n",
      "Epoch 55/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4137 - accuracy: 0.8312 - val_loss: 0.3874 - val_accuracy: 0.8415\n",
      "Epoch 56/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4015 - accuracy: 0.8380 - val_loss: 0.3873 - val_accuracy: 0.8405\n",
      "Epoch 57/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8385 - val_loss: 0.3874 - val_accuracy: 0.8405\n",
      "Epoch 58/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4088 - accuracy: 0.8350 - val_loss: 0.3872 - val_accuracy: 0.8420\n",
      "Epoch 59/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8308 - val_loss: 0.3870 - val_accuracy: 0.8410\n",
      "Epoch 60/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8367 - val_loss: 0.3874 - val_accuracy: 0.8395\n",
      "Epoch 61/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4073 - accuracy: 0.8327 - val_loss: 0.3868 - val_accuracy: 0.8410\n",
      "Epoch 62/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3955 - accuracy: 0.8392 - val_loss: 0.3870 - val_accuracy: 0.8400\n",
      "Epoch 63/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3906 - accuracy: 0.8451 - val_loss: 0.3870 - val_accuracy: 0.8405\n",
      "Epoch 64/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3854 - accuracy: 0.8479 - val_loss: 0.3872 - val_accuracy: 0.8420\n",
      "Epoch 65/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8375 - val_loss: 0.3867 - val_accuracy: 0.8410\n",
      "Epoch 66/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3905 - accuracy: 0.8439 - val_loss: 0.3869 - val_accuracy: 0.8400\n",
      "Epoch 67/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4025 - accuracy: 0.8376 - val_loss: 0.3865 - val_accuracy: 0.8410\n",
      "Epoch 68/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3946 - accuracy: 0.8408 - val_loss: 0.3864 - val_accuracy: 0.8410\n",
      "Epoch 69/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3948 - accuracy: 0.8398 - val_loss: 0.3865 - val_accuracy: 0.8410\n",
      "Epoch 70/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8390 - val_loss: 0.3863 - val_accuracy: 0.8395\n",
      "Epoch 71/600\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.84 - 0s 3ms/step - loss: 0.3944 - accuracy: 0.8418 - val_loss: 0.3863 - val_accuracy: 0.8410\n",
      "Epoch 72/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8414 - val_loss: 0.3863 - val_accuracy: 0.8400\n",
      "Epoch 73/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3889 - accuracy: 0.8429 - val_loss: 0.3860 - val_accuracy: 0.8390\n",
      "Epoch 74/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3984 - accuracy: 0.8387 - val_loss: 0.3863 - val_accuracy: 0.8385\n",
      "Epoch 75/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3967 - accuracy: 0.8417 - val_loss: 0.3863 - val_accuracy: 0.8395\n",
      "Epoch 76/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8371 - val_loss: 0.3860 - val_accuracy: 0.8385\n",
      "Epoch 77/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8305 - val_loss: 0.3858 - val_accuracy: 0.8395\n",
      "Epoch 78/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8427 - val_loss: 0.3861 - val_accuracy: 0.8390\n",
      "Epoch 79/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8381 - val_loss: 0.3861 - val_accuracy: 0.8385\n",
      "Epoch 80/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8397 - val_loss: 0.3864 - val_accuracy: 0.8380\n",
      "Epoch 81/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8375 - val_loss: 0.3861 - val_accuracy: 0.8380\n",
      "Epoch 82/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8449 - val_loss: 0.3866 - val_accuracy: 0.8400\n",
      "Epoch 83/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8355 - val_loss: 0.3861 - val_accuracy: 0.8380\n",
      "Epoch 84/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8449 - val_loss: 0.3869 - val_accuracy: 0.8390\n",
      "Epoch 85/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8419 - val_loss: 0.3860 - val_accuracy: 0.8385\n",
      "Epoch 86/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8379 - val_loss: 0.3863 - val_accuracy: 0.8390\n",
      "Epoch 87/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8407 - val_loss: 0.3866 - val_accuracy: 0.8380\n",
      "Epoch 88/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8388 - val_loss: 0.3862 - val_accuracy: 0.8385\n",
      "Epoch 89/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8341 - val_loss: 0.3864 - val_accuracy: 0.8405\n",
      "Epoch 90/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8413 - val_loss: 0.3863 - val_accuracy: 0.8400\n",
      "Epoch 91/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.8421 - val_loss: 0.3871 - val_accuracy: 0.8400\n",
      "Epoch 92/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8399 - val_loss: 0.3868 - val_accuracy: 0.8385\n",
      "Epoch 93/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8396 - val_loss: 0.3868 - val_accuracy: 0.8405\n",
      "Epoch 94/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8353 - val_loss: 0.3868 - val_accuracy: 0.8385\n",
      "Epoch 95/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.8414 - val_loss: 0.3865 - val_accuracy: 0.8410\n",
      "Epoch 96/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8341 - val_loss: 0.3868 - val_accuracy: 0.8390\n",
      "Epoch 97/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8349 - val_loss: 0.3869 - val_accuracy: 0.8405\n",
      "Epoch 98/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.8488 - val_loss: 0.3870 - val_accuracy: 0.8395\n",
      "Epoch 99/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8343 - val_loss: 0.3867 - val_accuracy: 0.8410\n",
      "Epoch 100/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8421 - val_loss: 0.3870 - val_accuracy: 0.8405\n",
      "Epoch 101/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8352 - val_loss: 0.3868 - val_accuracy: 0.8405\n",
      "Epoch 102/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3937 - accuracy: 0.8382 - val_loss: 0.3871 - val_accuracy: 0.8405\n",
      "Epoch 103/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8457 - val_loss: 0.3877 - val_accuracy: 0.8410\n",
      "Epoch 104/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4108 - accuracy: 0.8323 - val_loss: 0.3871 - val_accuracy: 0.8405\n",
      "Epoch 105/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8382 - val_loss: 0.3875 - val_accuracy: 0.8420\n",
      "Epoch 106/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8425 - val_loss: 0.3878 - val_accuracy: 0.8410\n",
      "Epoch 107/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3848 - accuracy: 0.8447 - val_loss: 0.3879 - val_accuracy: 0.8390\n",
      "Epoch 108/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8419 - val_loss: 0.3878 - val_accuracy: 0.8405\n",
      "Epoch 109/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8411 - val_loss: 0.3878 - val_accuracy: 0.8390\n",
      "Epoch 110/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8353 - val_loss: 0.3876 - val_accuracy: 0.8410\n",
      "Epoch 111/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8381 - val_loss: 0.3874 - val_accuracy: 0.8405\n",
      "Epoch 112/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8398 - val_loss: 0.3879 - val_accuracy: 0.8400\n",
      "Epoch 113/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8446 - val_loss: 0.3878 - val_accuracy: 0.8395\n",
      "Epoch 114/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8401 - val_loss: 0.3877 - val_accuracy: 0.8400\n",
      "Epoch 115/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8395 - val_loss: 0.3876 - val_accuracy: 0.8390\n",
      "Epoch 116/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3955 - accuracy: 0.8414 - val_loss: 0.3876 - val_accuracy: 0.8375\n",
      "Epoch 117/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.8528 - val_loss: 0.3879 - val_accuracy: 0.8375\n",
      "Epoch 118/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8387 - val_loss: 0.3884 - val_accuracy: 0.8375\n",
      "Epoch 119/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8386 - val_loss: 0.3874 - val_accuracy: 0.8390\n",
      "Epoch 120/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8360 - val_loss: 0.3874 - val_accuracy: 0.8400\n",
      "Epoch 121/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3963 - accuracy: 0.8365 - val_loss: 0.3874 - val_accuracy: 0.8385\n",
      "Epoch 122/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8475 - val_loss: 0.3878 - val_accuracy: 0.8395\n",
      "Epoch 123/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8451 - val_loss: 0.3871 - val_accuracy: 0.8385\n",
      "Epoch 124/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8400 - val_loss: 0.3870 - val_accuracy: 0.8380\n",
      "Epoch 125/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8442 - val_loss: 0.3870 - val_accuracy: 0.8375\n",
      "Epoch 126/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8378 - val_loss: 0.3869 - val_accuracy: 0.8395\n",
      "Epoch 127/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8483 - val_loss: 0.3873 - val_accuracy: 0.8390\n",
      "Epoch 128/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8517 - val_loss: 0.3878 - val_accuracy: 0.8360\n",
      "Epoch 129/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3852 - accuracy: 0.8473 - val_loss: 0.3872 - val_accuracy: 0.8365\n",
      "Epoch 130/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8431 - val_loss: 0.3867 - val_accuracy: 0.8375\n",
      "Epoch 131/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3820 - accuracy: 0.8457 - val_loss: 0.3866 - val_accuracy: 0.8370\n",
      "Epoch 132/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4013 - accuracy: 0.8365 - val_loss: 0.3860 - val_accuracy: 0.8380\n",
      "Epoch 133/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.8487 - val_loss: 0.3869 - val_accuracy: 0.8380\n",
      "Epoch 134/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8433 - val_loss: 0.3857 - val_accuracy: 0.8390\n",
      "Epoch 135/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3823 - accuracy: 0.8473 - val_loss: 0.3864 - val_accuracy: 0.8390\n",
      "Epoch 136/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.8472 - val_loss: 0.3861 - val_accuracy: 0.8395\n",
      "Epoch 137/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3912 - accuracy: 0.8456 - val_loss: 0.3863 - val_accuracy: 0.8400\n",
      "Epoch 138/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8386 - val_loss: 0.3853 - val_accuracy: 0.8405\n",
      "Epoch 139/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8492 - val_loss: 0.3857 - val_accuracy: 0.8410\n",
      "Epoch 140/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8442 - val_loss: 0.3864 - val_accuracy: 0.8390\n",
      "Epoch 141/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8456 - val_loss: 0.3860 - val_accuracy: 0.8395\n",
      "Epoch 142/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8416 - val_loss: 0.3853 - val_accuracy: 0.8410\n",
      "Epoch 143/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8436 - val_loss: 0.3853 - val_accuracy: 0.8405\n",
      "Epoch 144/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8417 - val_loss: 0.3854 - val_accuracy: 0.8395\n",
      "Epoch 145/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.8373 - val_loss: 0.3851 - val_accuracy: 0.8410\n",
      "Epoch 146/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8471 - val_loss: 0.3855 - val_accuracy: 0.8390\n",
      "Epoch 147/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8500 - val_loss: 0.3848 - val_accuracy: 0.8405\n",
      "Epoch 148/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8455 - val_loss: 0.3855 - val_accuracy: 0.8415\n",
      "Epoch 149/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8456 - val_loss: 0.3860 - val_accuracy: 0.8395\n",
      "Epoch 150/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8359 - val_loss: 0.3856 - val_accuracy: 0.8415\n",
      "Epoch 151/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8461 - val_loss: 0.3849 - val_accuracy: 0.8400\n",
      "Epoch 152/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8440 - val_loss: 0.3856 - val_accuracy: 0.8425\n",
      "Epoch 153/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8507 - val_loss: 0.3858 - val_accuracy: 0.8415\n",
      "Epoch 154/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3759 - accuracy: 0.8495 - val_loss: 0.3843 - val_accuracy: 0.8400\n",
      "Epoch 155/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8462 - val_loss: 0.3838 - val_accuracy: 0.8400\n",
      "Epoch 156/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8440 - val_loss: 0.3836 - val_accuracy: 0.8405\n",
      "Epoch 157/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8438 - val_loss: 0.3826 - val_accuracy: 0.8400\n",
      "Epoch 158/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8576 - val_loss: 0.3849 - val_accuracy: 0.8435\n",
      "Epoch 159/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3772 - accuracy: 0.8482 - val_loss: 0.3820 - val_accuracy: 0.8385\n",
      "Epoch 160/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3750 - accuracy: 0.8492 - val_loss: 0.3847 - val_accuracy: 0.8420\n",
      "Epoch 161/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3830 - accuracy: 0.8447 - val_loss: 0.3818 - val_accuracy: 0.8405\n",
      "Epoch 162/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3804 - accuracy: 0.8467 - val_loss: 0.3839 - val_accuracy: 0.8435\n",
      "Epoch 163/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8440 - val_loss: 0.3822 - val_accuracy: 0.8425\n",
      "Epoch 164/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8501 - val_loss: 0.3818 - val_accuracy: 0.8410\n",
      "Epoch 165/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3758 - accuracy: 0.8476 - val_loss: 0.3811 - val_accuracy: 0.8410\n",
      "Epoch 166/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3770 - accuracy: 0.8436 - val_loss: 0.3800 - val_accuracy: 0.8400\n",
      "Epoch 167/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3752 - accuracy: 0.8465 - val_loss: 0.3801 - val_accuracy: 0.8405\n",
      "Epoch 168/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.8466 - val_loss: 0.3797 - val_accuracy: 0.8405\n",
      "Epoch 169/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8430 - val_loss: 0.3793 - val_accuracy: 0.8410\n",
      "Epoch 170/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8488 - val_loss: 0.3781 - val_accuracy: 0.8405\n",
      "Epoch 171/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8421 - val_loss: 0.3774 - val_accuracy: 0.8400\n",
      "Epoch 172/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3697 - accuracy: 0.8481 - val_loss: 0.3769 - val_accuracy: 0.8410\n",
      "Epoch 173/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3707 - accuracy: 0.8474 - val_loss: 0.3769 - val_accuracy: 0.8420\n",
      "Epoch 174/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3695 - accuracy: 0.8456 - val_loss: 0.3756 - val_accuracy: 0.8410\n",
      "Epoch 175/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8444 - val_loss: 0.3742 - val_accuracy: 0.8410\n",
      "Epoch 176/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3696 - accuracy: 0.8467 - val_loss: 0.3735 - val_accuracy: 0.8410\n",
      "Epoch 177/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8515 - val_loss: 0.3715 - val_accuracy: 0.8420\n",
      "Epoch 178/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3705 - accuracy: 0.8449 - val_loss: 0.3711 - val_accuracy: 0.8430\n",
      "Epoch 179/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3658 - accuracy: 0.8481 - val_loss: 0.3694 - val_accuracy: 0.8435\n",
      "Epoch 180/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8470 - val_loss: 0.3689 - val_accuracy: 0.8435\n",
      "Epoch 181/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3702 - accuracy: 0.8464 - val_loss: 0.3679 - val_accuracy: 0.8450\n",
      "Epoch 182/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3691 - accuracy: 0.8508 - val_loss: 0.3669 - val_accuracy: 0.8470\n",
      "Epoch 183/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3674 - accuracy: 0.8488 - val_loss: 0.3659 - val_accuracy: 0.8490\n",
      "Epoch 184/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3615 - accuracy: 0.8531 - val_loss: 0.3650 - val_accuracy: 0.8490\n",
      "Epoch 185/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8548 - val_loss: 0.3634 - val_accuracy: 0.8510\n",
      "Epoch 186/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3547 - accuracy: 0.8569 - val_loss: 0.3624 - val_accuracy: 0.8525\n",
      "Epoch 187/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.8499 - val_loss: 0.3613 - val_accuracy: 0.8530\n",
      "Epoch 188/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.8492 - val_loss: 0.3603 - val_accuracy: 0.8510\n",
      "Epoch 189/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3563 - accuracy: 0.8559 - val_loss: 0.3589 - val_accuracy: 0.8530\n",
      "Epoch 190/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8543 - val_loss: 0.3568 - val_accuracy: 0.8520\n",
      "Epoch 191/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8575 - val_loss: 0.3562 - val_accuracy: 0.8540\n",
      "Epoch 192/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3568 - accuracy: 0.8537 - val_loss: 0.3558 - val_accuracy: 0.8555\n",
      "Epoch 193/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3604 - accuracy: 0.8515 - val_loss: 0.3543 - val_accuracy: 0.8535\n",
      "Epoch 194/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8548 - val_loss: 0.3537 - val_accuracy: 0.8545\n",
      "Epoch 195/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8566 - val_loss: 0.3532 - val_accuracy: 0.8535\n",
      "Epoch 196/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3403 - accuracy: 0.8644 - val_loss: 0.3517 - val_accuracy: 0.8530\n",
      "Epoch 197/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3551 - accuracy: 0.8574 - val_loss: 0.3510 - val_accuracy: 0.8540\n",
      "Epoch 198/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8617 - val_loss: 0.3528 - val_accuracy: 0.8545\n",
      "Epoch 199/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8536 - val_loss: 0.3505 - val_accuracy: 0.8520\n",
      "Epoch 200/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8598 - val_loss: 0.3504 - val_accuracy: 0.8550\n",
      "Epoch 201/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8553 - val_loss: 0.3498 - val_accuracy: 0.8495\n",
      "Epoch 202/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3493 - accuracy: 0.8568 - val_loss: 0.3490 - val_accuracy: 0.8525\n",
      "Epoch 203/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8648 - val_loss: 0.3492 - val_accuracy: 0.8555\n",
      "Epoch 204/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3531 - accuracy: 0.8580 - val_loss: 0.3487 - val_accuracy: 0.8535\n",
      "Epoch 205/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3581 - accuracy: 0.8591 - val_loss: 0.3474 - val_accuracy: 0.8510\n",
      "Epoch 206/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8587 - val_loss: 0.3478 - val_accuracy: 0.8535\n",
      "Epoch 207/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.8600 - val_loss: 0.3466 - val_accuracy: 0.8535\n",
      "Epoch 208/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8658 - val_loss: 0.3466 - val_accuracy: 0.8555\n",
      "Epoch 209/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8529 - val_loss: 0.3458 - val_accuracy: 0.8555\n",
      "Epoch 210/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8633 - val_loss: 0.3455 - val_accuracy: 0.8570\n",
      "Epoch 211/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8502 - val_loss: 0.3460 - val_accuracy: 0.8555\n",
      "Epoch 212/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8535 - val_loss: 0.3455 - val_accuracy: 0.8545\n",
      "Epoch 213/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8623 - val_loss: 0.3449 - val_accuracy: 0.8530\n",
      "Epoch 214/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3519 - accuracy: 0.8530 - val_loss: 0.3455 - val_accuracy: 0.8560\n",
      "Epoch 215/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8592 - val_loss: 0.3450 - val_accuracy: 0.8565\n",
      "Epoch 216/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8601 - val_loss: 0.3451 - val_accuracy: 0.8560\n",
      "Epoch 217/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8615 - val_loss: 0.3440 - val_accuracy: 0.8555\n",
      "Epoch 218/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3440 - accuracy: 0.8620 - val_loss: 0.3443 - val_accuracy: 0.8560\n",
      "Epoch 219/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3632 - accuracy: 0.8500 - val_loss: 0.3440 - val_accuracy: 0.8565\n",
      "Epoch 220/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3496 - accuracy: 0.8533 - val_loss: 0.3439 - val_accuracy: 0.8560\n",
      "Epoch 221/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3519 - accuracy: 0.8529 - val_loss: 0.3420 - val_accuracy: 0.8550\n",
      "Epoch 222/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3284 - accuracy: 0.8698 - val_loss: 0.3434 - val_accuracy: 0.8575\n",
      "Epoch 223/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3455 - accuracy: 0.8581 - val_loss: 0.3421 - val_accuracy: 0.8555\n",
      "Epoch 224/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3428 - accuracy: 0.8577 - val_loss: 0.3411 - val_accuracy: 0.8575\n",
      "Epoch 225/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3470 - accuracy: 0.8604 - val_loss: 0.3423 - val_accuracy: 0.8600\n",
      "Epoch 226/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3523 - accuracy: 0.8540 - val_loss: 0.3405 - val_accuracy: 0.8570\n",
      "Epoch 227/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3438 - accuracy: 0.8577 - val_loss: 0.3401 - val_accuracy: 0.8620\n",
      "Epoch 228/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3478 - accuracy: 0.8586 - val_loss: 0.3394 - val_accuracy: 0.8580\n",
      "Epoch 229/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8641 - val_loss: 0.3411 - val_accuracy: 0.8575\n",
      "Epoch 230/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.8629 - val_loss: 0.3397 - val_accuracy: 0.8570\n",
      "Epoch 231/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3606 - accuracy: 0.8499 - val_loss: 0.3386 - val_accuracy: 0.8600\n",
      "Epoch 232/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3464 - accuracy: 0.8550 - val_loss: 0.3388 - val_accuracy: 0.8605\n",
      "Epoch 233/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3444 - accuracy: 0.8619 - val_loss: 0.3381 - val_accuracy: 0.8615\n",
      "Epoch 234/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3398 - accuracy: 0.8606 - val_loss: 0.3383 - val_accuracy: 0.8585\n",
      "Epoch 235/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8577 - val_loss: 0.3389 - val_accuracy: 0.8585\n",
      "Epoch 236/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.8614 - val_loss: 0.3389 - val_accuracy: 0.8575\n",
      "Epoch 237/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3454 - accuracy: 0.8571 - val_loss: 0.3383 - val_accuracy: 0.8590\n",
      "Epoch 238/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8570 - val_loss: 0.3391 - val_accuracy: 0.8615\n",
      "Epoch 239/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3409 - accuracy: 0.8572 - val_loss: 0.3373 - val_accuracy: 0.8585\n",
      "Epoch 240/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3437 - accuracy: 0.8577 - val_loss: 0.3385 - val_accuracy: 0.8585\n",
      "Epoch 241/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3341 - accuracy: 0.8615 - val_loss: 0.3383 - val_accuracy: 0.8595\n",
      "Epoch 242/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3301 - accuracy: 0.8654 - val_loss: 0.3394 - val_accuracy: 0.8595\n",
      "Epoch 243/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3281 - accuracy: 0.8664 - val_loss: 0.3386 - val_accuracy: 0.8605\n",
      "Epoch 244/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3428 - accuracy: 0.8638 - val_loss: 0.3376 - val_accuracy: 0.8605\n",
      "Epoch 245/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8652 - val_loss: 0.3391 - val_accuracy: 0.8615\n",
      "Epoch 246/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3470 - accuracy: 0.8578 - val_loss: 0.3382 - val_accuracy: 0.8605\n",
      "Epoch 247/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3335 - accuracy: 0.8632 - val_loss: 0.3378 - val_accuracy: 0.8600\n",
      "Epoch 248/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3380 - accuracy: 0.8613 - val_loss: 0.3380 - val_accuracy: 0.8580\n",
      "Epoch 249/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8651 - val_loss: 0.3382 - val_accuracy: 0.8605\n",
      "Epoch 250/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3514 - accuracy: 0.8548 - val_loss: 0.3385 - val_accuracy: 0.8625\n",
      "Epoch 251/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3344 - accuracy: 0.8625 - val_loss: 0.3372 - val_accuracy: 0.8625\n",
      "Epoch 252/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8640 - val_loss: 0.3377 - val_accuracy: 0.8640\n",
      "Epoch 253/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3416 - accuracy: 0.8615 - val_loss: 0.3368 - val_accuracy: 0.8605\n",
      "Epoch 254/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3351 - accuracy: 0.8615 - val_loss: 0.3378 - val_accuracy: 0.8610\n",
      "Epoch 255/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3496 - accuracy: 0.8585 - val_loss: 0.3378 - val_accuracy: 0.8610\n",
      "Epoch 256/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3399 - accuracy: 0.8663 - val_loss: 0.3375 - val_accuracy: 0.8635\n",
      "Epoch 257/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8631 - val_loss: 0.3374 - val_accuracy: 0.8620\n",
      "Epoch 258/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.8666 - val_loss: 0.3370 - val_accuracy: 0.8640\n",
      "Epoch 259/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8634 - val_loss: 0.3378 - val_accuracy: 0.8640\n",
      "Epoch 260/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8655 - val_loss: 0.3369 - val_accuracy: 0.8635\n",
      "Epoch 261/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.8654 - val_loss: 0.3388 - val_accuracy: 0.8645\n",
      "Epoch 262/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3385 - accuracy: 0.8624 - val_loss: 0.3372 - val_accuracy: 0.8635\n",
      "Epoch 263/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3340 - accuracy: 0.8642 - val_loss: 0.3383 - val_accuracy: 0.8665\n",
      "Epoch 264/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8643 - val_loss: 0.3381 - val_accuracy: 0.8650\n",
      "Epoch 265/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8710 - val_loss: 0.3402 - val_accuracy: 0.8635\n",
      "Epoch 266/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8616 - val_loss: 0.3408 - val_accuracy: 0.8620\n",
      "Epoch 267/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8584 - val_loss: 0.3397 - val_accuracy: 0.8625\n",
      "Epoch 268/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8652 - val_loss: 0.3376 - val_accuracy: 0.8640\n",
      "Epoch 269/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3417 - accuracy: 0.8602 - val_loss: 0.3384 - val_accuracy: 0.8605\n",
      "Epoch 270/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8627 - val_loss: 0.3393 - val_accuracy: 0.8625\n",
      "Epoch 271/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3474 - accuracy: 0.8554 - val_loss: 0.3391 - val_accuracy: 0.8595\n",
      "Epoch 272/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3508 - accuracy: 0.8577 - val_loss: 0.3383 - val_accuracy: 0.8615\n",
      "Epoch 273/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8635 - val_loss: 0.3380 - val_accuracy: 0.8645\n",
      "Epoch 274/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3261 - accuracy: 0.8680 - val_loss: 0.3386 - val_accuracy: 0.8615\n",
      "Epoch 275/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3405 - accuracy: 0.8602 - val_loss: 0.3379 - val_accuracy: 0.8610\n",
      "Epoch 276/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3285 - accuracy: 0.8680 - val_loss: 0.3397 - val_accuracy: 0.8610\n",
      "Epoch 277/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.8665 - val_loss: 0.3385 - val_accuracy: 0.8610\n",
      "Epoch 278/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8590 - val_loss: 0.3381 - val_accuracy: 0.8625\n",
      "Epoch 279/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8630 - val_loss: 0.3410 - val_accuracy: 0.8625\n",
      "Epoch 280/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8607 - val_loss: 0.3384 - val_accuracy: 0.8625\n",
      "Epoch 281/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8653 - val_loss: 0.3393 - val_accuracy: 0.8595\n",
      "Epoch 282/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8628 - val_loss: 0.3400 - val_accuracy: 0.8625\n",
      "Epoch 283/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8632 - val_loss: 0.3390 - val_accuracy: 0.8615\n",
      "Epoch 284/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8632 - val_loss: 0.3389 - val_accuracy: 0.8630\n",
      "Epoch 285/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8637 - val_loss: 0.3408 - val_accuracy: 0.8635\n",
      "Epoch 286/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8672 - val_loss: 0.3408 - val_accuracy: 0.8635\n",
      "Epoch 287/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3300 - accuracy: 0.8660 - val_loss: 0.3387 - val_accuracy: 0.8615\n",
      "Epoch 288/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.8626 - val_loss: 0.3396 - val_accuracy: 0.8595\n",
      "Epoch 289/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3396 - accuracy: 0.8582 - val_loss: 0.3388 - val_accuracy: 0.8600\n",
      "Epoch 290/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3220 - accuracy: 0.8715 - val_loss: 0.3403 - val_accuracy: 0.8620\n",
      "Epoch 291/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8676 - val_loss: 0.3388 - val_accuracy: 0.8610\n",
      "Epoch 292/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8593 - val_loss: 0.3397 - val_accuracy: 0.8595\n",
      "Epoch 293/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3388 - accuracy: 0.8624 - val_loss: 0.3390 - val_accuracy: 0.8640\n",
      "Epoch 294/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8649 - val_loss: 0.3382 - val_accuracy: 0.8625\n",
      "Epoch 295/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8663 - val_loss: 0.3392 - val_accuracy: 0.8625\n",
      "Epoch 296/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3465 - accuracy: 0.8578 - val_loss: 0.3392 - val_accuracy: 0.8605\n",
      "Epoch 297/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8600 - val_loss: 0.3396 - val_accuracy: 0.8625\n",
      "Epoch 298/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8589 - val_loss: 0.3426 - val_accuracy: 0.8605\n",
      "Epoch 299/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8623 - val_loss: 0.3425 - val_accuracy: 0.8595\n",
      "Epoch 300/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8614 - val_loss: 0.3390 - val_accuracy: 0.8590\n",
      "Epoch 301/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3376 - accuracy: 0.8619 - val_loss: 0.3388 - val_accuracy: 0.8610\n",
      "Epoch 302/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3394 - accuracy: 0.8589 - val_loss: 0.3402 - val_accuracy: 0.8605\n",
      "Epoch 303/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8611 - val_loss: 0.3394 - val_accuracy: 0.8610\n",
      "Epoch 304/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8615 - val_loss: 0.3403 - val_accuracy: 0.8640\n",
      "Epoch 305/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8605 - val_loss: 0.3400 - val_accuracy: 0.8635\n",
      "Epoch 306/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8650 - val_loss: 0.3391 - val_accuracy: 0.8625\n",
      "Epoch 307/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8606 - val_loss: 0.3391 - val_accuracy: 0.8600\n",
      "Epoch 308/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.8725 - val_loss: 0.3422 - val_accuracy: 0.8575\n",
      "Epoch 309/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8665 - val_loss: 0.3393 - val_accuracy: 0.8635\n",
      "Epoch 310/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8608 - val_loss: 0.3400 - val_accuracy: 0.8635\n",
      "Epoch 311/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8632 - val_loss: 0.3409 - val_accuracy: 0.8610\n",
      "Epoch 312/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8694 - val_loss: 0.3396 - val_accuracy: 0.8610\n",
      "Epoch 313/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8581 - val_loss: 0.3417 - val_accuracy: 0.8610\n",
      "Epoch 314/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3257 - accuracy: 0.8667 - val_loss: 0.3403 - val_accuracy: 0.8635\n",
      "Epoch 315/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3443 - accuracy: 0.8598 - val_loss: 0.3406 - val_accuracy: 0.8605\n",
      "Epoch 316/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8602 - val_loss: 0.3409 - val_accuracy: 0.8590\n",
      "Epoch 317/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8623 - val_loss: 0.3424 - val_accuracy: 0.8610\n",
      "Epoch 318/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8579 - val_loss: 0.3407 - val_accuracy: 0.8590\n",
      "Epoch 319/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8676 - val_loss: 0.3408 - val_accuracy: 0.8635\n",
      "Epoch 320/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3384 - accuracy: 0.8627 - val_loss: 0.3421 - val_accuracy: 0.8620\n",
      "Epoch 321/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8685 - val_loss: 0.3408 - val_accuracy: 0.8620\n",
      "Epoch 322/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8657 - val_loss: 0.3414 - val_accuracy: 0.8585\n",
      "Epoch 323/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8615 - val_loss: 0.3399 - val_accuracy: 0.8590\n",
      "Epoch 324/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3221 - accuracy: 0.8672 - val_loss: 0.3453 - val_accuracy: 0.8585\n",
      "Epoch 325/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8647 - val_loss: 0.3399 - val_accuracy: 0.8630\n",
      "Epoch 326/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8590 - val_loss: 0.3413 - val_accuracy: 0.8585\n",
      "Epoch 327/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8607 - val_loss: 0.3396 - val_accuracy: 0.8600\n",
      "Epoch 328/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3360 - accuracy: 0.8623 - val_loss: 0.3412 - val_accuracy: 0.8620\n",
      "Epoch 329/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8691 - val_loss: 0.3403 - val_accuracy: 0.8595\n",
      "Epoch 330/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8600 - val_loss: 0.3406 - val_accuracy: 0.8650\n",
      "Epoch 331/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8689 - val_loss: 0.3413 - val_accuracy: 0.8620\n",
      "Epoch 332/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3234 - accuracy: 0.8689 - val_loss: 0.3412 - val_accuracy: 0.8560\n",
      "Epoch 333/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8594 - val_loss: 0.3403 - val_accuracy: 0.8615\n",
      "Epoch 334/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8674 - val_loss: 0.3412 - val_accuracy: 0.8610\n",
      "Epoch 335/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3288 - accuracy: 0.8670 - val_loss: 0.3399 - val_accuracy: 0.8650\n",
      "Epoch 336/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8647 - val_loss: 0.3428 - val_accuracy: 0.8615\n",
      "Epoch 337/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8613 - val_loss: 0.3402 - val_accuracy: 0.8635\n",
      "Epoch 338/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3346 - accuracy: 0.8630 - val_loss: 0.3411 - val_accuracy: 0.8610\n",
      "Epoch 339/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8577 - val_loss: 0.3416 - val_accuracy: 0.8615\n",
      "Epoch 340/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8629 - val_loss: 0.3424 - val_accuracy: 0.8615\n",
      "Epoch 341/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8576 - val_loss: 0.3403 - val_accuracy: 0.8605\n",
      "Epoch 342/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8631 - val_loss: 0.3413 - val_accuracy: 0.8635\n",
      "Epoch 343/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3356 - accuracy: 0.8642 - val_loss: 0.3415 - val_accuracy: 0.8600\n",
      "Epoch 344/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8632 - val_loss: 0.3412 - val_accuracy: 0.8610\n",
      "Epoch 345/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8722 - val_loss: 0.3405 - val_accuracy: 0.8595\n",
      "Epoch 346/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3323 - accuracy: 0.8642 - val_loss: 0.3407 - val_accuracy: 0.8615\n",
      "Epoch 347/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3350 - accuracy: 0.8600 - val_loss: 0.3420 - val_accuracy: 0.8610\n",
      "Epoch 348/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3378 - accuracy: 0.8617 - val_loss: 0.3428 - val_accuracy: 0.8595\n",
      "Epoch 349/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8661 - val_loss: 0.3416 - val_accuracy: 0.8580\n",
      "Epoch 350/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3300 - accuracy: 0.8635 - val_loss: 0.3409 - val_accuracy: 0.8615\n",
      "Epoch 351/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3342 - accuracy: 0.8653 - val_loss: 0.3402 - val_accuracy: 0.8630\n",
      "Epoch 352/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8598 - val_loss: 0.3421 - val_accuracy: 0.8610\n",
      "Epoch 353/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3383 - accuracy: 0.8614 - val_loss: 0.3410 - val_accuracy: 0.8600\n",
      "Epoch 354/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8656 - val_loss: 0.3428 - val_accuracy: 0.8585\n",
      "Epoch 355/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8650 - val_loss: 0.3425 - val_accuracy: 0.8620\n",
      "Epoch 356/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8599 - val_loss: 0.3406 - val_accuracy: 0.8620\n",
      "Epoch 357/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3335 - accuracy: 0.8657 - val_loss: 0.3415 - val_accuracy: 0.8615\n",
      "Epoch 358/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8634 - val_loss: 0.3406 - val_accuracy: 0.8630\n",
      "Epoch 359/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3250 - accuracy: 0.8643 - val_loss: 0.3412 - val_accuracy: 0.8640\n",
      "Epoch 360/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8676 - val_loss: 0.3431 - val_accuracy: 0.8610\n",
      "Epoch 361/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8622 - val_loss: 0.3406 - val_accuracy: 0.8600\n",
      "Epoch 362/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8666 - val_loss: 0.3423 - val_accuracy: 0.8595\n",
      "Epoch 363/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8620 - val_loss: 0.3438 - val_accuracy: 0.8610\n",
      "Epoch 364/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8631 - val_loss: 0.3403 - val_accuracy: 0.8605\n",
      "Epoch 365/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.8584 - val_loss: 0.3412 - val_accuracy: 0.8625\n",
      "Epoch 366/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8595 - val_loss: 0.3410 - val_accuracy: 0.8610\n",
      "Epoch 367/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.8638 - val_loss: 0.3410 - val_accuracy: 0.8605\n",
      "Epoch 368/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8648 - val_loss: 0.3410 - val_accuracy: 0.8640\n",
      "Epoch 369/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8594 - val_loss: 0.3406 - val_accuracy: 0.8610\n",
      "Epoch 370/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8632 - val_loss: 0.3405 - val_accuracy: 0.8620\n",
      "Epoch 371/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8664 - val_loss: 0.3408 - val_accuracy: 0.8640\n",
      "Epoch 372/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3420 - accuracy: 0.8598 - val_loss: 0.3407 - val_accuracy: 0.8630\n",
      "Epoch 373/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3418 - accuracy: 0.8581 - val_loss: 0.3416 - val_accuracy: 0.8630\n",
      "Epoch 374/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8624 - val_loss: 0.3407 - val_accuracy: 0.8605\n",
      "Epoch 375/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8619 - val_loss: 0.3420 - val_accuracy: 0.8590\n",
      "Epoch 376/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3429 - accuracy: 0.8600 - val_loss: 0.3421 - val_accuracy: 0.8610\n",
      "Epoch 377/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8615 - val_loss: 0.3403 - val_accuracy: 0.8605\n",
      "Epoch 378/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8600 - val_loss: 0.3419 - val_accuracy: 0.8595\n",
      "Epoch 379/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3449 - accuracy: 0.8577 - val_loss: 0.3413 - val_accuracy: 0.8630\n",
      "Epoch 380/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8617 - val_loss: 0.3412 - val_accuracy: 0.8610\n",
      "Epoch 381/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8643 - val_loss: 0.3413 - val_accuracy: 0.8605\n",
      "Epoch 382/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3347 - accuracy: 0.8663 - val_loss: 0.3436 - val_accuracy: 0.8600\n",
      "Epoch 383/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8671 - val_loss: 0.3415 - val_accuracy: 0.8625\n",
      "Epoch 384/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8646 - val_loss: 0.3401 - val_accuracy: 0.8605\n",
      "Epoch 385/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3357 - accuracy: 0.8609 - val_loss: 0.3422 - val_accuracy: 0.8615\n",
      "Epoch 386/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3388 - accuracy: 0.8603 - val_loss: 0.3408 - val_accuracy: 0.8630\n",
      "Epoch 387/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3324 - accuracy: 0.8667 - val_loss: 0.3422 - val_accuracy: 0.8610\n",
      "Epoch 388/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8648 - val_loss: 0.3402 - val_accuracy: 0.8605\n",
      "Epoch 389/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3243 - accuracy: 0.8678 - val_loss: 0.3421 - val_accuracy: 0.8600\n",
      "Epoch 390/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8595 - val_loss: 0.3411 - val_accuracy: 0.8600\n",
      "Epoch 391/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8733 - val_loss: 0.3410 - val_accuracy: 0.8615\n",
      "Epoch 392/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8628 - val_loss: 0.3412 - val_accuracy: 0.8600\n",
      "Epoch 393/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8616 - val_loss: 0.3411 - val_accuracy: 0.8625\n",
      "Epoch 394/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8544 - val_loss: 0.3402 - val_accuracy: 0.8615\n",
      "Epoch 395/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3300 - accuracy: 0.8644 - val_loss: 0.3411 - val_accuracy: 0.8625\n",
      "Epoch 396/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8634 - val_loss: 0.3412 - val_accuracy: 0.8615\n",
      "Epoch 397/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3259 - accuracy: 0.8633 - val_loss: 0.3409 - val_accuracy: 0.8610\n",
      "Epoch 398/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8663 - val_loss: 0.3410 - val_accuracy: 0.8645\n",
      "Epoch 399/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8639 - val_loss: 0.3407 - val_accuracy: 0.8615\n",
      "Epoch 400/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8606 - val_loss: 0.3409 - val_accuracy: 0.8620\n",
      "Epoch 401/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8657 - val_loss: 0.3407 - val_accuracy: 0.8620\n",
      "Epoch 402/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3406 - accuracy: 0.8592 - val_loss: 0.3420 - val_accuracy: 0.8595\n",
      "Epoch 403/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8623 - val_loss: 0.3419 - val_accuracy: 0.8615\n",
      "Epoch 404/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8684 - val_loss: 0.3429 - val_accuracy: 0.8585\n",
      "Epoch 405/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8613 - val_loss: 0.3432 - val_accuracy: 0.8615\n",
      "Epoch 406/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.8641 - val_loss: 0.3418 - val_accuracy: 0.8610\n",
      "Epoch 407/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8642 - val_loss: 0.3417 - val_accuracy: 0.8620\n",
      "Epoch 408/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3218 - accuracy: 0.8684 - val_loss: 0.3417 - val_accuracy: 0.8635\n",
      "Epoch 409/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3346 - accuracy: 0.8616 - val_loss: 0.3408 - val_accuracy: 0.8635\n",
      "Epoch 410/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8568 - val_loss: 0.3413 - val_accuracy: 0.8605\n",
      "Epoch 411/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3476 - accuracy: 0.8581 - val_loss: 0.3411 - val_accuracy: 0.8630\n",
      "Epoch 412/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8630 - val_loss: 0.3434 - val_accuracy: 0.8620\n",
      "Epoch 413/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8665 - val_loss: 0.3430 - val_accuracy: 0.8610\n",
      "Epoch 414/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8624 - val_loss: 0.3417 - val_accuracy: 0.8590\n",
      "Epoch 415/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8683 - val_loss: 0.3449 - val_accuracy: 0.8605\n",
      "Epoch 416/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3310 - accuracy: 0.8621 - val_loss: 0.3429 - val_accuracy: 0.8585\n",
      "Epoch 417/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3337 - accuracy: 0.8631 - val_loss: 0.3445 - val_accuracy: 0.8580\n",
      "Epoch 418/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3359 - accuracy: 0.8568 - val_loss: 0.3414 - val_accuracy: 0.8610\n",
      "Epoch 419/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3328 - accuracy: 0.8647 - val_loss: 0.3431 - val_accuracy: 0.8605\n",
      "Epoch 420/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3207 - accuracy: 0.8725 - val_loss: 0.3448 - val_accuracy: 0.8615\n",
      "Epoch 421/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3236 - accuracy: 0.8681 - val_loss: 0.3438 - val_accuracy: 0.8600\n",
      "Epoch 422/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3245 - accuracy: 0.8695 - val_loss: 0.3420 - val_accuracy: 0.8620\n",
      "Epoch 423/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3365 - accuracy: 0.8637 - val_loss: 0.3429 - val_accuracy: 0.8600\n",
      "Epoch 424/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3340 - accuracy: 0.8605 - val_loss: 0.3413 - val_accuracy: 0.8615\n",
      "Epoch 425/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3296 - accuracy: 0.8683 - val_loss: 0.3445 - val_accuracy: 0.8600\n",
      "Epoch 426/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3373 - accuracy: 0.8616 - val_loss: 0.3410 - val_accuracy: 0.8620\n",
      "Epoch 427/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3337 - accuracy: 0.8629 - val_loss: 0.3427 - val_accuracy: 0.8610\n",
      "Epoch 428/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3356 - accuracy: 0.8634 - val_loss: 0.3425 - val_accuracy: 0.8620\n",
      "Epoch 429/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3257 - accuracy: 0.8666 - val_loss: 0.3430 - val_accuracy: 0.8605\n",
      "Epoch 430/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8668 - val_loss: 0.3427 - val_accuracy: 0.8595\n",
      "Epoch 431/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8674 - val_loss: 0.3433 - val_accuracy: 0.8605\n",
      "Epoch 432/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8614 - val_loss: 0.3418 - val_accuracy: 0.8625\n",
      "Epoch 433/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3373 - accuracy: 0.8636 - val_loss: 0.3408 - val_accuracy: 0.8595\n",
      "Epoch 434/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3363 - accuracy: 0.8660 - val_loss: 0.3424 - val_accuracy: 0.8600\n",
      "Epoch 435/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8530 - val_loss: 0.3424 - val_accuracy: 0.8630\n",
      "Epoch 436/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.8650 - val_loss: 0.3416 - val_accuracy: 0.8615\n",
      "Epoch 437/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3329 - accuracy: 0.8658 - val_loss: 0.3415 - val_accuracy: 0.8620\n",
      "Epoch 438/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3392 - accuracy: 0.8605 - val_loss: 0.3414 - val_accuracy: 0.8605\n",
      "Epoch 439/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8581 - val_loss: 0.3422 - val_accuracy: 0.8605\n",
      "Epoch 440/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.8659 - val_loss: 0.3426 - val_accuracy: 0.8605\n",
      "Epoch 441/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8640 - val_loss: 0.3425 - val_accuracy: 0.8595\n",
      "Epoch 442/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3278 - accuracy: 0.8675 - val_loss: 0.3436 - val_accuracy: 0.8610\n",
      "Epoch 443/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3386 - accuracy: 0.8621 - val_loss: 0.3420 - val_accuracy: 0.8610\n",
      "Epoch 444/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3229 - accuracy: 0.8679 - val_loss: 0.3423 - val_accuracy: 0.8615\n",
      "Epoch 445/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3334 - accuracy: 0.8665 - val_loss: 0.3423 - val_accuracy: 0.8585\n",
      "Epoch 446/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8598 - val_loss: 0.3419 - val_accuracy: 0.8620\n",
      "Epoch 447/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8664 - val_loss: 0.3425 - val_accuracy: 0.8630\n",
      "Epoch 448/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8601 - val_loss: 0.3409 - val_accuracy: 0.8620\n",
      "Epoch 449/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3217 - accuracy: 0.8671 - val_loss: 0.3436 - val_accuracy: 0.8605\n",
      "Epoch 450/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3354 - accuracy: 0.8635 - val_loss: 0.3425 - val_accuracy: 0.8600\n",
      "Epoch 451/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.8569 - val_loss: 0.3414 - val_accuracy: 0.8610\n",
      "Epoch 452/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3291 - accuracy: 0.8626 - val_loss: 0.3433 - val_accuracy: 0.8565\n",
      "Epoch 453/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3319 - accuracy: 0.8652 - val_loss: 0.3422 - val_accuracy: 0.8615\n",
      "Epoch 454/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3309 - accuracy: 0.8633 - val_loss: 0.3438 - val_accuracy: 0.8570\n",
      "Epoch 455/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3445 - accuracy: 0.8597 - val_loss: 0.3414 - val_accuracy: 0.8605\n",
      "Epoch 456/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8596 - val_loss: 0.3426 - val_accuracy: 0.8600\n",
      "Epoch 457/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8714 - val_loss: 0.3429 - val_accuracy: 0.8595\n",
      "Epoch 458/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3297 - accuracy: 0.8675 - val_loss: 0.3423 - val_accuracy: 0.8630\n",
      "Epoch 459/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3204 - accuracy: 0.8683 - val_loss: 0.3455 - val_accuracy: 0.8580\n",
      "Epoch 460/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3405 - accuracy: 0.8599 - val_loss: 0.3414 - val_accuracy: 0.8595\n",
      "Epoch 461/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3397 - accuracy: 0.8613 - val_loss: 0.3418 - val_accuracy: 0.8595\n",
      "Epoch 462/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3475 - accuracy: 0.8571 - val_loss: 0.3426 - val_accuracy: 0.8605\n",
      "Epoch 463/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3365 - accuracy: 0.8616 - val_loss: 0.3420 - val_accuracy: 0.8610\n",
      "Epoch 464/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.8698 - val_loss: 0.3431 - val_accuracy: 0.8610\n",
      "Epoch 465/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8647 - val_loss: 0.3432 - val_accuracy: 0.8610\n",
      "Epoch 466/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.8649 - val_loss: 0.3436 - val_accuracy: 0.8580\n",
      "Epoch 467/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3284 - accuracy: 0.8658 - val_loss: 0.3426 - val_accuracy: 0.8600\n",
      "Epoch 468/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.8587 - val_loss: 0.3419 - val_accuracy: 0.8625\n",
      "Epoch 469/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8668 - val_loss: 0.3420 - val_accuracy: 0.8585\n",
      "Epoch 470/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8670 - val_loss: 0.3426 - val_accuracy: 0.8560\n",
      "Epoch 471/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8697 - val_loss: 0.3455 - val_accuracy: 0.8595\n",
      "Epoch 472/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8622 - val_loss: 0.3427 - val_accuracy: 0.8580\n",
      "Epoch 473/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3248 - accuracy: 0.8692 - val_loss: 0.3429 - val_accuracy: 0.8625\n",
      "Epoch 474/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3426 - accuracy: 0.8580 - val_loss: 0.3428 - val_accuracy: 0.8615\n",
      "Epoch 475/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3358 - accuracy: 0.8629 - val_loss: 0.3426 - val_accuracy: 0.8540\n",
      "Epoch 476/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8644 - val_loss: 0.3417 - val_accuracy: 0.8610\n",
      "Epoch 477/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8694 - val_loss: 0.3435 - val_accuracy: 0.8605\n",
      "Epoch 478/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3195 - accuracy: 0.8732 - val_loss: 0.3438 - val_accuracy: 0.8605\n",
      "Epoch 479/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.8682 - val_loss: 0.3438 - val_accuracy: 0.8615\n",
      "Epoch 480/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8682 - val_loss: 0.3431 - val_accuracy: 0.8630\n",
      "Epoch 481/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8639 - val_loss: 0.3446 - val_accuracy: 0.8625\n",
      "Epoch 482/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8669 - val_loss: 0.3430 - val_accuracy: 0.8600\n",
      "Epoch 483/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8662 - val_loss: 0.3419 - val_accuracy: 0.8605\n",
      "Epoch 484/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3247 - accuracy: 0.8671 - val_loss: 0.3443 - val_accuracy: 0.8560\n",
      "Epoch 485/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8574 - val_loss: 0.3428 - val_accuracy: 0.8585\n",
      "Epoch 486/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8671 - val_loss: 0.3421 - val_accuracy: 0.8610\n",
      "Epoch 487/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8615 - val_loss: 0.3430 - val_accuracy: 0.8560\n",
      "Epoch 488/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3365 - accuracy: 0.8640 - val_loss: 0.3432 - val_accuracy: 0.8620\n",
      "Epoch 489/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3223 - accuracy: 0.8713 - val_loss: 0.3436 - val_accuracy: 0.8605\n",
      "Epoch 490/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8592 - val_loss: 0.3425 - val_accuracy: 0.8600\n",
      "Epoch 491/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8652 - val_loss: 0.3424 - val_accuracy: 0.8615\n",
      "Epoch 492/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8657 - val_loss: 0.3426 - val_accuracy: 0.8565\n",
      "Epoch 493/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8632 - val_loss: 0.3434 - val_accuracy: 0.8615\n",
      "Epoch 494/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8656 - val_loss: 0.3435 - val_accuracy: 0.8590\n",
      "Epoch 495/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8652 - val_loss: 0.3434 - val_accuracy: 0.8590\n",
      "Epoch 496/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8650 - val_loss: 0.3423 - val_accuracy: 0.8595\n",
      "Epoch 497/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8669 - val_loss: 0.3438 - val_accuracy: 0.8585\n",
      "Epoch 498/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3390 - accuracy: 0.8627 - val_loss: 0.3431 - val_accuracy: 0.8585\n",
      "Epoch 499/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8693 - val_loss: 0.3425 - val_accuracy: 0.8615\n",
      "Epoch 500/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3262 - accuracy: 0.8658 - val_loss: 0.3438 - val_accuracy: 0.8605\n",
      "Epoch 501/600\n",
      "63/63 [==============================] - ETA: 0s - loss: 0.3275 - accuracy: 0.87 - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8687 - val_loss: 0.3428 - val_accuracy: 0.8550\n",
      "Epoch 502/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3295 - accuracy: 0.8641 - val_loss: 0.3427 - val_accuracy: 0.8615\n",
      "Epoch 503/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3282 - accuracy: 0.8678 - val_loss: 0.3436 - val_accuracy: 0.8625\n",
      "Epoch 504/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8631 - val_loss: 0.3429 - val_accuracy: 0.8600\n",
      "Epoch 505/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8672 - val_loss: 0.3449 - val_accuracy: 0.8630\n",
      "Epoch 506/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3266 - accuracy: 0.8670 - val_loss: 0.3445 - val_accuracy: 0.8625\n",
      "Epoch 507/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8685 - val_loss: 0.3440 - val_accuracy: 0.8610\n",
      "Epoch 508/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3320 - accuracy: 0.8623 - val_loss: 0.3437 - val_accuracy: 0.8595\n",
      "Epoch 509/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3366 - accuracy: 0.8627 - val_loss: 0.3435 - val_accuracy: 0.8565\n",
      "Epoch 510/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3304 - accuracy: 0.8661 - val_loss: 0.3421 - val_accuracy: 0.8600\n",
      "Epoch 511/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8635 - val_loss: 0.3431 - val_accuracy: 0.8610\n",
      "Epoch 512/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8642 - val_loss: 0.3432 - val_accuracy: 0.8605\n",
      "Epoch 513/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8656 - val_loss: 0.3430 - val_accuracy: 0.8605\n",
      "Epoch 514/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8592 - val_loss: 0.3419 - val_accuracy: 0.8615\n",
      "Epoch 515/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8657 - val_loss: 0.3444 - val_accuracy: 0.8595\n",
      "Epoch 516/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.8641 - val_loss: 0.3423 - val_accuracy: 0.8595\n",
      "Epoch 517/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3286 - accuracy: 0.8694 - val_loss: 0.3432 - val_accuracy: 0.8600\n",
      "Epoch 518/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3341 - accuracy: 0.8664 - val_loss: 0.3433 - val_accuracy: 0.8615\n",
      "Epoch 519/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8620 - val_loss: 0.3445 - val_accuracy: 0.8615\n",
      "Epoch 520/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8636 - val_loss: 0.3459 - val_accuracy: 0.8590\n",
      "Epoch 521/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8619 - val_loss: 0.3436 - val_accuracy: 0.8610\n",
      "Epoch 522/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.8628 - val_loss: 0.3439 - val_accuracy: 0.8630\n",
      "Epoch 523/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3317 - accuracy: 0.8678 - val_loss: 0.3434 - val_accuracy: 0.8565\n",
      "Epoch 524/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8661 - val_loss: 0.3442 - val_accuracy: 0.8575\n",
      "Epoch 525/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8655 - val_loss: 0.3436 - val_accuracy: 0.8595\n",
      "Epoch 526/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8604 - val_loss: 0.3446 - val_accuracy: 0.8605\n",
      "Epoch 527/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8656 - val_loss: 0.3429 - val_accuracy: 0.8600\n",
      "Epoch 528/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3297 - accuracy: 0.8656 - val_loss: 0.3449 - val_accuracy: 0.8615\n",
      "Epoch 529/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8634 - val_loss: 0.3440 - val_accuracy: 0.8615\n",
      "Epoch 530/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8666 - val_loss: 0.3466 - val_accuracy: 0.8595\n",
      "Epoch 531/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8660 - val_loss: 0.3458 - val_accuracy: 0.8595\n",
      "Epoch 532/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3298 - accuracy: 0.8639 - val_loss: 0.3442 - val_accuracy: 0.8605\n",
      "Epoch 533/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3328 - accuracy: 0.8636 - val_loss: 0.3443 - val_accuracy: 0.8625\n",
      "Epoch 534/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8597 - val_loss: 0.3435 - val_accuracy: 0.8595\n",
      "Epoch 535/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8666 - val_loss: 0.3446 - val_accuracy: 0.8595\n",
      "Epoch 536/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8632 - val_loss: 0.3439 - val_accuracy: 0.8590\n",
      "Epoch 537/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3324 - accuracy: 0.8654 - val_loss: 0.3430 - val_accuracy: 0.8565\n",
      "Epoch 538/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8655 - val_loss: 0.3453 - val_accuracy: 0.8605\n",
      "Epoch 539/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8656 - val_loss: 0.3428 - val_accuracy: 0.8615\n",
      "Epoch 540/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3258 - accuracy: 0.8706 - val_loss: 0.3442 - val_accuracy: 0.8595\n",
      "Epoch 541/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8611 - val_loss: 0.3440 - val_accuracy: 0.8585\n",
      "Epoch 542/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8668 - val_loss: 0.3462 - val_accuracy: 0.8585\n",
      "Epoch 543/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8586 - val_loss: 0.3429 - val_accuracy: 0.8595\n",
      "Epoch 544/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3253 - accuracy: 0.8669 - val_loss: 0.3452 - val_accuracy: 0.8580\n",
      "Epoch 545/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3439 - accuracy: 0.8579 - val_loss: 0.3426 - val_accuracy: 0.8580\n",
      "Epoch 546/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8622 - val_loss: 0.3450 - val_accuracy: 0.8585\n",
      "Epoch 547/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8680 - val_loss: 0.3446 - val_accuracy: 0.8570\n",
      "Epoch 548/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8639 - val_loss: 0.3452 - val_accuracy: 0.8575\n",
      "Epoch 549/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8648 - val_loss: 0.3439 - val_accuracy: 0.8580\n",
      "Epoch 550/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8612 - val_loss: 0.3435 - val_accuracy: 0.8570\n",
      "Epoch 551/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8707 - val_loss: 0.3450 - val_accuracy: 0.8595\n",
      "Epoch 552/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8671 - val_loss: 0.3433 - val_accuracy: 0.8575\n",
      "Epoch 553/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3380 - accuracy: 0.8634 - val_loss: 0.3442 - val_accuracy: 0.8600\n",
      "Epoch 554/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8636 - val_loss: 0.3444 - val_accuracy: 0.8585\n",
      "Epoch 555/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8614 - val_loss: 0.3446 - val_accuracy: 0.8610\n",
      "Epoch 556/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3343 - accuracy: 0.8642 - val_loss: 0.3437 - val_accuracy: 0.8600\n",
      "Epoch 557/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.8646 - val_loss: 0.3450 - val_accuracy: 0.8565\n",
      "Epoch 558/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8613 - val_loss: 0.3443 - val_accuracy: 0.8595\n",
      "Epoch 559/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8607 - val_loss: 0.3441 - val_accuracy: 0.8635\n",
      "Epoch 560/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8588 - val_loss: 0.3443 - val_accuracy: 0.8590\n",
      "Epoch 561/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3417 - accuracy: 0.8649 - val_loss: 0.3455 - val_accuracy: 0.8605\n",
      "Epoch 562/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8602 - val_loss: 0.3456 - val_accuracy: 0.8605\n",
      "Epoch 563/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8603 - val_loss: 0.3467 - val_accuracy: 0.8595\n",
      "Epoch 564/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3238 - accuracy: 0.8700 - val_loss: 0.3451 - val_accuracy: 0.8605\n",
      "Epoch 565/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8628 - val_loss: 0.3442 - val_accuracy: 0.8565\n",
      "Epoch 566/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.8656 - val_loss: 0.3477 - val_accuracy: 0.8610\n",
      "Epoch 567/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8635 - val_loss: 0.3457 - val_accuracy: 0.8590\n",
      "Epoch 568/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3331 - accuracy: 0.8607 - val_loss: 0.3455 - val_accuracy: 0.8620\n",
      "Epoch 569/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8635 - val_loss: 0.3438 - val_accuracy: 0.8605\n",
      "Epoch 570/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3248 - accuracy: 0.8711 - val_loss: 0.3464 - val_accuracy: 0.8600\n",
      "Epoch 571/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3290 - accuracy: 0.8664 - val_loss: 0.3449 - val_accuracy: 0.8615\n",
      "Epoch 572/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3358 - accuracy: 0.8646 - val_loss: 0.3437 - val_accuracy: 0.8575\n",
      "Epoch 573/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3426 - accuracy: 0.8609 - val_loss: 0.3436 - val_accuracy: 0.8600\n",
      "Epoch 574/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8627 - val_loss: 0.3442 - val_accuracy: 0.8610\n",
      "Epoch 575/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8619 - val_loss: 0.3442 - val_accuracy: 0.8605\n",
      "Epoch 576/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3371 - accuracy: 0.8632 - val_loss: 0.3436 - val_accuracy: 0.8620\n",
      "Epoch 577/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3390 - accuracy: 0.8633 - val_loss: 0.3453 - val_accuracy: 0.8605\n",
      "Epoch 578/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3340 - accuracy: 0.8650 - val_loss: 0.3443 - val_accuracy: 0.8600\n",
      "Epoch 579/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3388 - accuracy: 0.8676 - val_loss: 0.3436 - val_accuracy: 0.8615\n",
      "Epoch 580/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3310 - accuracy: 0.8665 - val_loss: 0.3457 - val_accuracy: 0.8615\n",
      "Epoch 581/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8622 - val_loss: 0.3445 - val_accuracy: 0.8565\n",
      "Epoch 582/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.8730 - val_loss: 0.3442 - val_accuracy: 0.8600\n",
      "Epoch 583/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3371 - accuracy: 0.8626 - val_loss: 0.3442 - val_accuracy: 0.8620\n",
      "Epoch 584/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3344 - accuracy: 0.8602 - val_loss: 0.3441 - val_accuracy: 0.8580\n",
      "Epoch 585/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8706 - val_loss: 0.3450 - val_accuracy: 0.8590\n",
      "Epoch 586/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3337 - accuracy: 0.8664 - val_loss: 0.3459 - val_accuracy: 0.8605\n",
      "Epoch 587/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3357 - accuracy: 0.8601 - val_loss: 0.3445 - val_accuracy: 0.8610\n",
      "Epoch 588/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3399 - accuracy: 0.8645 - val_loss: 0.3439 - val_accuracy: 0.8610\n",
      "Epoch 589/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3167 - accuracy: 0.8727 - val_loss: 0.3455 - val_accuracy: 0.8600\n",
      "Epoch 590/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3330 - accuracy: 0.8633 - val_loss: 0.3469 - val_accuracy: 0.8610\n",
      "Epoch 591/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3218 - accuracy: 0.8676 - val_loss: 0.3454 - val_accuracy: 0.8580\n",
      "Epoch 592/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3301 - accuracy: 0.8657 - val_loss: 0.3440 - val_accuracy: 0.8595\n",
      "Epoch 593/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8651 - val_loss: 0.3468 - val_accuracy: 0.8595\n",
      "Epoch 594/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8621 - val_loss: 0.3459 - val_accuracy: 0.8595\n",
      "Epoch 595/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3229 - accuracy: 0.8689 - val_loss: 0.3448 - val_accuracy: 0.8590\n",
      "Epoch 596/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.8627 - val_loss: 0.3448 - val_accuracy: 0.8615\n",
      "Epoch 597/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8621 - val_loss: 0.3448 - val_accuracy: 0.8605\n",
      "Epoch 598/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.8629 - val_loss: 0.3449 - val_accuracy: 0.8600\n",
      "Epoch 599/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3226 - accuracy: 0.8671 - val_loss: 0.3461 - val_accuracy: 0.8595\n",
      "Epoch 600/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8610 - val_loss: 0.3451 - val_accuracy: 0.8615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17088af3370>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the ANN to the data\n",
    "model.fit(X_scaled_train,y_train,batch_size=128,epochs=600,validation_data=(X_scaled_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683261</td>\n",
       "      <td>0.787875</td>\n",
       "      <td>0.671070</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.653593</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.623309</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.576839</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.511175</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.481232</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.438697</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450307</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.427437</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.683261  0.787875  0.671070        0.8035\n",
       "1  0.653593  0.794500  0.623309        0.8035\n",
       "2  0.576839  0.794500  0.511175        0.8035\n",
       "3  0.481232  0.794500  0.438697        0.8035\n",
       "4  0.450307  0.794500  0.427437        0.8035"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Losses and Accuracy from Model\n",
    "loss_accuracy = pd.DataFrame(model.history.history)\n",
    "loss_accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving Training Loss and Validation Loss\n",
    "losses = loss_accuracy[['loss','val_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.683261</td>\n",
       "      <td>0.671070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.653593</td>\n",
       "      <td>0.623309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.576839</td>\n",
       "      <td>0.511175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.481232</td>\n",
       "      <td>0.438697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450307</td>\n",
       "      <td>0.427437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss\n",
       "0  0.683261  0.671070\n",
       "1  0.653593  0.623309\n",
       "2  0.576839  0.511175\n",
       "3  0.481232  0.438697\n",
       "4  0.450307  0.427437"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot of Loss\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for 600 epochs, model started to overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now to prevent overfitting we will need to use the concept of CallBacks**\n",
    "\n",
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "#Adding the input layer and the first hidden layer\n",
    "\n",
    "#input_dim is the total features to predict the label\n",
    "#units are the neurons in the first hidden layer\n",
    "# kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights. (0 to 1)\n",
    "#In case of statistical distribution, the library will generate numbers from that statistical distribution and use as starting weights.\n",
    "model.add(Dense(activation = 'relu', input_dim=11 , units = 6, kernel_initializer = 'uniform'))\n",
    "\n",
    "#Adding second hidden layer\n",
    "model.add(Dense(activation = 'relu', units=6 , kernel_initializer='uniform'))\n",
    "\n",
    "#Adding the output layer\n",
    "model.add(Dense(activation='sigmoid', units=1, kernel_initializer='uniform'))\n",
    "\n",
    "#Compiling the ANN\n",
    "#(binary_crossentropy used when just binary label is there to predict)(just one output)\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing EarlyStopping\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored metric has stopped improving.\n",
      " |  \n",
      " |  Assuming the goal of a training is to minimize the loss. With this, the\n",
      " |  metric to be monitored would be `'loss'`, and mode would be `'min'`. A\n",
      " |  `model.fit()` training loop will check at end of every epoch whether\n",
      " |  the loss is no longer decreasing, considering the `min_delta` and\n",
      " |  `patience` if applicable. Once it's found no longer decreasing,\n",
      " |  `model.stop_training` is marked True and the training terminates.\n",
      " |  \n",
      " |  The quantity to be monitored needs to be available in `logs` dict.\n",
      " |  To make it so, pass the loss or metrics at `model.compile()`.\n",
      " |  \n",
      " |  Args:\n",
      " |    monitor: Quantity to be monitored.\n",
      " |    min_delta: Minimum change in the monitored quantity\n",
      " |        to qualify as an improvement, i.e. an absolute\n",
      " |        change of less than min_delta, will count as no\n",
      " |        improvement.\n",
      " |    patience: Number of epochs with no improvement\n",
      " |        after which training will be stopped.\n",
      " |    verbose: verbosity mode.\n",
      " |    mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |        training will stop when the quantity\n",
      " |        monitored has stopped decreasing; in `\"max\"`\n",
      " |        mode it will stop when the quantity\n",
      " |        monitored has stopped increasing; in `\"auto\"`\n",
      " |        mode, the direction is automatically inferred\n",
      " |        from the name of the monitored quantity.\n",
      " |    baseline: Baseline value for the monitored quantity.\n",
      " |        Training will stop if the model doesn't show improvement over the\n",
      " |        baseline.\n",
      " |    restore_best_weights: Whether to restore model weights from\n",
      " |        the epoch with the best value of the monitored quantity.\n",
      " |        If False, the model weights obtained at the last step of\n",
      " |        training are used. An epoch will be restored regardless\n",
      " |        of the performance relative to the `baseline`. If no epoch\n",
      " |        improves on `baseline`, training will run for `patience`\n",
      " |        epochs and restore weights from the best epoch in that set.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
      " |  >>> # This callback will stop the training when there is no improvement in\n",
      " |  >>> # the loss for three consecutive epochs.\n",
      " |  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n",
      " |  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n",
      " |  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n",
      " |  ...                     epochs=10, batch_size=1, callbacks=[callback],\n",
      " |  ...                     verbose=0)\n",
      " |  >>> len(history.history['loss'])  # Only 4 epochs are run.\n",
      " |  4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`. For training epoch, the values of the\n",
      " |           `Model`'s metrics are returned. Example : `{'loss': 0.2, 'accuracy':\n",
      " |             0.7}`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are monitoring 'validation_loss' and mode is 'min' that means, stop if the loss is not decreasing\n",
    "#mode should be 'max' in accuracy as stop training if the accuracy is not increasing\n",
    "#patience=20 means we will wait for 20 epochs even if we detected stopping point\n",
    "early_stop = EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.6879 - accuracy: 0.7562 - val_loss: 0.6669 - val_accuracy: 0.8035\n",
      "Epoch 2/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6554 - accuracy: 0.7945 - val_loss: 0.6015 - val_accuracy: 0.8035\n",
      "Epoch 3/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5791 - accuracy: 0.7981 - val_loss: 0.5049 - val_accuracy: 0.8035\n",
      "Epoch 4/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4985 - accuracy: 0.7923 - val_loss: 0.4469 - val_accuracy: 0.8035\n",
      "Epoch 5/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.7995 - val_loss: 0.4234 - val_accuracy: 0.8035\n",
      "Epoch 6/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.8010 - val_loss: 0.4119 - val_accuracy: 0.8170\n",
      "Epoch 7/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.8140 - val_loss: 0.4036 - val_accuracy: 0.8220\n",
      "Epoch 8/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8224 - val_loss: 0.3983 - val_accuracy: 0.8280\n",
      "Epoch 9/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8159 - val_loss: 0.3935 - val_accuracy: 0.8280\n",
      "Epoch 10/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8207 - val_loss: 0.3899 - val_accuracy: 0.8280\n",
      "Epoch 11/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8216 - val_loss: 0.3870 - val_accuracy: 0.8290\n",
      "Epoch 12/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8204 - val_loss: 0.3840 - val_accuracy: 0.8305\n",
      "Epoch 13/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3950 - accuracy: 0.8293 - val_loss: 0.3818 - val_accuracy: 0.8300\n",
      "Epoch 14/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8255 - val_loss: 0.3795 - val_accuracy: 0.8320\n",
      "Epoch 15/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8186 - val_loss: 0.3773 - val_accuracy: 0.8345\n",
      "Epoch 16/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8272 - val_loss: 0.3760 - val_accuracy: 0.8340\n",
      "Epoch 17/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8292 - val_loss: 0.3735 - val_accuracy: 0.8355\n",
      "Epoch 18/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3827 - accuracy: 0.8333 - val_loss: 0.3713 - val_accuracy: 0.8355\n",
      "Epoch 19/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8294 - val_loss: 0.3696 - val_accuracy: 0.8380\n",
      "Epoch 20/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8377 - val_loss: 0.3695 - val_accuracy: 0.8370\n",
      "Epoch 21/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8271 - val_loss: 0.3669 - val_accuracy: 0.8375\n",
      "Epoch 22/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3803 - accuracy: 0.8263 - val_loss: 0.3651 - val_accuracy: 0.8370\n",
      "Epoch 23/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8262 - val_loss: 0.3629 - val_accuracy: 0.8440\n",
      "Epoch 24/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3724 - accuracy: 0.8385 - val_loss: 0.3620 - val_accuracy: 0.8480\n",
      "Epoch 25/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3779 - accuracy: 0.8393 - val_loss: 0.3610 - val_accuracy: 0.8495\n",
      "Epoch 26/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3656 - accuracy: 0.8455 - val_loss: 0.3606 - val_accuracy: 0.8520\n",
      "Epoch 27/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3689 - accuracy: 0.8441 - val_loss: 0.3585 - val_accuracy: 0.8505\n",
      "Epoch 28/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8462 - val_loss: 0.3576 - val_accuracy: 0.8490\n",
      "Epoch 29/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3705 - accuracy: 0.8499 - val_loss: 0.3572 - val_accuracy: 0.8540\n",
      "Epoch 30/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8439 - val_loss: 0.3562 - val_accuracy: 0.8545\n",
      "Epoch 31/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3688 - accuracy: 0.8470 - val_loss: 0.3561 - val_accuracy: 0.8545\n",
      "Epoch 32/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3692 - accuracy: 0.8478 - val_loss: 0.3534 - val_accuracy: 0.8540\n",
      "Epoch 33/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8481 - val_loss: 0.3533 - val_accuracy: 0.8515\n",
      "Epoch 34/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.8571 - val_loss: 0.3531 - val_accuracy: 0.8555\n",
      "Epoch 35/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3655 - accuracy: 0.8475 - val_loss: 0.3522 - val_accuracy: 0.8550\n",
      "Epoch 36/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3650 - accuracy: 0.8536 - val_loss: 0.3510 - val_accuracy: 0.8555\n",
      "Epoch 37/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3649 - accuracy: 0.8511 - val_loss: 0.3508 - val_accuracy: 0.8560\n",
      "Epoch 38/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3524 - accuracy: 0.8555 - val_loss: 0.3508 - val_accuracy: 0.8565\n",
      "Epoch 39/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3572 - accuracy: 0.8565 - val_loss: 0.3499 - val_accuracy: 0.8555\n",
      "Epoch 40/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8491 - val_loss: 0.3494 - val_accuracy: 0.8570\n",
      "Epoch 41/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8518 - val_loss: 0.3489 - val_accuracy: 0.8575\n",
      "Epoch 42/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3625 - accuracy: 0.8497 - val_loss: 0.3503 - val_accuracy: 0.8545\n",
      "Epoch 43/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3498 - accuracy: 0.8580 - val_loss: 0.3484 - val_accuracy: 0.8595\n",
      "Epoch 44/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8540 - val_loss: 0.3476 - val_accuracy: 0.8580\n",
      "Epoch 45/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8504 - val_loss: 0.3481 - val_accuracy: 0.8560\n",
      "Epoch 46/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8645 - val_loss: 0.3501 - val_accuracy: 0.8550\n",
      "Epoch 47/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3405 - accuracy: 0.8635 - val_loss: 0.3481 - val_accuracy: 0.8560\n",
      "Epoch 48/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.8622 - val_loss: 0.3474 - val_accuracy: 0.8595\n",
      "Epoch 49/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3332 - accuracy: 0.8642 - val_loss: 0.3476 - val_accuracy: 0.8565\n",
      "Epoch 50/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8548 - val_loss: 0.3468 - val_accuracy: 0.8540\n",
      "Epoch 51/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8636 - val_loss: 0.3454 - val_accuracy: 0.8590\n",
      "Epoch 52/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8573 - val_loss: 0.3448 - val_accuracy: 0.8570\n",
      "Epoch 53/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8546 - val_loss: 0.3454 - val_accuracy: 0.8600\n",
      "Epoch 54/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8574 - val_loss: 0.3452 - val_accuracy: 0.8575\n",
      "Epoch 55/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8589 - val_loss: 0.3444 - val_accuracy: 0.8595\n",
      "Epoch 56/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3377 - accuracy: 0.8610 - val_loss: 0.3434 - val_accuracy: 0.8595\n",
      "Epoch 57/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8548 - val_loss: 0.3448 - val_accuracy: 0.8595\n",
      "Epoch 58/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8546 - val_loss: 0.3434 - val_accuracy: 0.8615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3540 - accuracy: 0.8534 - val_loss: 0.3443 - val_accuracy: 0.8585\n",
      "Epoch 60/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8610 - val_loss: 0.3442 - val_accuracy: 0.8585\n",
      "Epoch 61/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8573 - val_loss: 0.3431 - val_accuracy: 0.8585\n",
      "Epoch 62/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3306 - accuracy: 0.8646 - val_loss: 0.3436 - val_accuracy: 0.8575\n",
      "Epoch 63/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8568 - val_loss: 0.3431 - val_accuracy: 0.8600\n",
      "Epoch 64/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.8608 - val_loss: 0.3434 - val_accuracy: 0.8595\n",
      "Epoch 65/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8546 - val_loss: 0.3413 - val_accuracy: 0.8605\n",
      "Epoch 66/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8598 - val_loss: 0.3419 - val_accuracy: 0.8615\n",
      "Epoch 67/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3496 - accuracy: 0.8510 - val_loss: 0.3419 - val_accuracy: 0.8610\n",
      "Epoch 68/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.8571 - val_loss: 0.3415 - val_accuracy: 0.8595\n",
      "Epoch 69/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8626 - val_loss: 0.3408 - val_accuracy: 0.8630\n",
      "Epoch 70/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3495 - accuracy: 0.8583 - val_loss: 0.3410 - val_accuracy: 0.8570\n",
      "Epoch 71/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3408 - accuracy: 0.8537 - val_loss: 0.3408 - val_accuracy: 0.8600\n",
      "Epoch 72/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8544 - val_loss: 0.3409 - val_accuracy: 0.8620\n",
      "Epoch 73/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8604 - val_loss: 0.3405 - val_accuracy: 0.8575\n",
      "Epoch 74/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3370 - accuracy: 0.8638 - val_loss: 0.3410 - val_accuracy: 0.8575\n",
      "Epoch 75/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3386 - accuracy: 0.8575 - val_loss: 0.3406 - val_accuracy: 0.8630\n",
      "Epoch 76/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8598 - val_loss: 0.3417 - val_accuracy: 0.8605\n",
      "Epoch 77/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.8580 - val_loss: 0.3407 - val_accuracy: 0.8595\n",
      "Epoch 78/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8617 - val_loss: 0.3390 - val_accuracy: 0.8620\n",
      "Epoch 79/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8626 - val_loss: 0.3397 - val_accuracy: 0.8605\n",
      "Epoch 80/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8557 - val_loss: 0.3385 - val_accuracy: 0.8605\n",
      "Epoch 81/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.8632 - val_loss: 0.3402 - val_accuracy: 0.8590\n",
      "Epoch 82/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8659 - val_loss: 0.3394 - val_accuracy: 0.8595\n",
      "Epoch 83/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8568 - val_loss: 0.3397 - val_accuracy: 0.8625\n",
      "Epoch 84/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3374 - accuracy: 0.8647 - val_loss: 0.3398 - val_accuracy: 0.8615\n",
      "Epoch 85/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3415 - accuracy: 0.8594 - val_loss: 0.3404 - val_accuracy: 0.8650\n",
      "Epoch 86/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8616 - val_loss: 0.3396 - val_accuracy: 0.8610\n",
      "Epoch 87/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3500 - accuracy: 0.8580 - val_loss: 0.3389 - val_accuracy: 0.8640\n",
      "Epoch 88/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8607 - val_loss: 0.3402 - val_accuracy: 0.8620\n",
      "Epoch 89/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.8659 - val_loss: 0.3391 - val_accuracy: 0.8660\n",
      "Epoch 90/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3373 - accuracy: 0.8663 - val_loss: 0.3412 - val_accuracy: 0.8625\n",
      "Epoch 91/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.8563 - val_loss: 0.3396 - val_accuracy: 0.8605\n",
      "Epoch 92/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3483 - accuracy: 0.8569 - val_loss: 0.3406 - val_accuracy: 0.8615\n",
      "Epoch 93/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.8601 - val_loss: 0.3396 - val_accuracy: 0.8630\n",
      "Epoch 94/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8605 - val_loss: 0.3387 - val_accuracy: 0.8645\n",
      "Epoch 95/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3430 - accuracy: 0.8562 - val_loss: 0.3386 - val_accuracy: 0.8630\n",
      "Epoch 96/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3412 - accuracy: 0.8591 - val_loss: 0.3394 - val_accuracy: 0.8620\n",
      "Epoch 97/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3492 - accuracy: 0.8534 - val_loss: 0.3383 - val_accuracy: 0.8625\n",
      "Epoch 98/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3347 - accuracy: 0.8633 - val_loss: 0.3384 - val_accuracy: 0.8630\n",
      "Epoch 99/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3452 - accuracy: 0.8572 - val_loss: 0.3387 - val_accuracy: 0.8625\n",
      "Epoch 100/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3326 - accuracy: 0.8674 - val_loss: 0.3391 - val_accuracy: 0.8610\n",
      "Epoch 101/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8564 - val_loss: 0.3399 - val_accuracy: 0.8610\n",
      "Epoch 102/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3494 - accuracy: 0.8561 - val_loss: 0.3386 - val_accuracy: 0.8645\n",
      "Epoch 103/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3351 - accuracy: 0.8598 - val_loss: 0.3375 - val_accuracy: 0.8650\n",
      "Epoch 104/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8601 - val_loss: 0.3408 - val_accuracy: 0.8640\n",
      "Epoch 105/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3408 - accuracy: 0.8587 - val_loss: 0.3391 - val_accuracy: 0.8610\n",
      "Epoch 106/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3308 - accuracy: 0.8658 - val_loss: 0.3409 - val_accuracy: 0.8625\n",
      "Epoch 107/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3333 - accuracy: 0.8640 - val_loss: 0.3385 - val_accuracy: 0.8630\n",
      "Epoch 108/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3238 - accuracy: 0.8675 - val_loss: 0.3383 - val_accuracy: 0.8640\n",
      "Epoch 109/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3405 - accuracy: 0.8579 - val_loss: 0.3377 - val_accuracy: 0.8635\n",
      "Epoch 110/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3336 - accuracy: 0.8604 - val_loss: 0.3376 - val_accuracy: 0.8630\n",
      "Epoch 111/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8665 - val_loss: 0.3381 - val_accuracy: 0.8620\n",
      "Epoch 112/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8565 - val_loss: 0.3379 - val_accuracy: 0.8630\n",
      "Epoch 113/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8625 - val_loss: 0.3406 - val_accuracy: 0.8625\n",
      "Epoch 114/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.8578 - val_loss: 0.3384 - val_accuracy: 0.8630\n",
      "Epoch 115/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3343 - accuracy: 0.8646 - val_loss: 0.3385 - val_accuracy: 0.8630\n",
      "Epoch 116/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3413 - accuracy: 0.8574 - val_loss: 0.3379 - val_accuracy: 0.8650\n",
      "Epoch 117/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3431 - accuracy: 0.8602 - val_loss: 0.3399 - val_accuracy: 0.8610\n",
      "Epoch 118/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8656 - val_loss: 0.3434 - val_accuracy: 0.8580\n",
      "Epoch 119/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8607 - val_loss: 0.3389 - val_accuracy: 0.8610\n",
      "Epoch 120/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3347 - accuracy: 0.8631 - val_loss: 0.3408 - val_accuracy: 0.8605\n",
      "Epoch 121/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3387 - accuracy: 0.8596 - val_loss: 0.3396 - val_accuracy: 0.8645\n",
      "Epoch 122/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3452 - accuracy: 0.8586 - val_loss: 0.3374 - val_accuracy: 0.8630\n",
      "Epoch 123/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8605 - val_loss: 0.3407 - val_accuracy: 0.8615\n",
      "Epoch 124/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8616 - val_loss: 0.3382 - val_accuracy: 0.8620\n",
      "Epoch 125/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3357 - accuracy: 0.8638 - val_loss: 0.3390 - val_accuracy: 0.8615\n",
      "Epoch 126/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3405 - accuracy: 0.8584 - val_loss: 0.3381 - val_accuracy: 0.8620\n",
      "Epoch 127/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8646 - val_loss: 0.3386 - val_accuracy: 0.8625\n",
      "Epoch 128/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.8566 - val_loss: 0.3384 - val_accuracy: 0.8615\n",
      "Epoch 129/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3330 - accuracy: 0.8654 - val_loss: 0.3379 - val_accuracy: 0.8635\n",
      "Epoch 130/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3474 - accuracy: 0.8537 - val_loss: 0.3384 - val_accuracy: 0.8615\n",
      "Epoch 131/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3293 - accuracy: 0.8648 - val_loss: 0.3403 - val_accuracy: 0.8620\n",
      "Epoch 132/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3377 - accuracy: 0.8619 - val_loss: 0.3409 - val_accuracy: 0.8600\n",
      "Epoch 133/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3413 - accuracy: 0.8607 - val_loss: 0.3398 - val_accuracy: 0.8610\n",
      "Epoch 134/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8623 - val_loss: 0.3406 - val_accuracy: 0.8625\n",
      "Epoch 135/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3374 - accuracy: 0.8614 - val_loss: 0.3408 - val_accuracy: 0.8615\n",
      "Epoch 136/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8575 - val_loss: 0.3373 - val_accuracy: 0.8625\n",
      "Epoch 137/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3301 - accuracy: 0.8653 - val_loss: 0.3377 - val_accuracy: 0.8625\n",
      "Epoch 138/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3300 - accuracy: 0.8653 - val_loss: 0.3396 - val_accuracy: 0.8615\n",
      "Epoch 139/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3330 - accuracy: 0.8623 - val_loss: 0.3371 - val_accuracy: 0.8625\n",
      "Epoch 140/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3254 - accuracy: 0.8635 - val_loss: 0.3379 - val_accuracy: 0.8630\n",
      "Epoch 141/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.8591 - val_loss: 0.3397 - val_accuracy: 0.8630\n",
      "Epoch 142/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3334 - accuracy: 0.8633 - val_loss: 0.3398 - val_accuracy: 0.8605\n",
      "Epoch 143/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3436 - accuracy: 0.8590 - val_loss: 0.3385 - val_accuracy: 0.8620\n",
      "Epoch 144/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8651 - val_loss: 0.3373 - val_accuracy: 0.8630\n",
      "Epoch 145/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8679 - val_loss: 0.3388 - val_accuracy: 0.8615\n",
      "Epoch 146/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3447 - accuracy: 0.8545 - val_loss: 0.3383 - val_accuracy: 0.8630\n",
      "Epoch 147/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3393 - accuracy: 0.8608 - val_loss: 0.3388 - val_accuracy: 0.8620\n",
      "Epoch 148/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.3386 - accuracy: 0.8607 - val_loss: 0.3402 - val_accuracy: 0.8605\n",
      "Epoch 149/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3369 - accuracy: 0.8592 - val_loss: 0.3397 - val_accuracy: 0.8590\n",
      "Epoch 150/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3320 - accuracy: 0.8638 - val_loss: 0.3399 - val_accuracy: 0.8605\n",
      "Epoch 151/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.8711 - val_loss: 0.3378 - val_accuracy: 0.8660\n",
      "Epoch 152/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3327 - accuracy: 0.8624 - val_loss: 0.3398 - val_accuracy: 0.8615\n",
      "Epoch 153/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3392 - accuracy: 0.8599 - val_loss: 0.3374 - val_accuracy: 0.8645\n",
      "Epoch 154/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3366 - accuracy: 0.8634 - val_loss: 0.3379 - val_accuracy: 0.8635\n",
      "Epoch 155/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8635 - val_loss: 0.3398 - val_accuracy: 0.8620\n",
      "Epoch 156/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.3448 - accuracy: 0.8566 - val_loss: 0.3416 - val_accuracy: 0.8595\n",
      "Epoch 157/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8665 - val_loss: 0.3392 - val_accuracy: 0.8625\n",
      "Epoch 158/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8637 - val_loss: 0.3373 - val_accuracy: 0.8590\n",
      "Epoch 159/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8598 - val_loss: 0.3391 - val_accuracy: 0.8615\n",
      "Epoch 00159: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1708b8feaf0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the ANN to the data with Early Stopping Callback\n",
    "model.fit(X_scaled_train,y_train,batch_size=128,epochs=600,validation_data=(X_scaled_test,y_test),\n",
    "         callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that epochs reached 159 and stopped to prevent Overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682171</td>\n",
       "      <td>0.78575</td>\n",
       "      <td>0.666874</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.640864</td>\n",
       "      <td>0.79450</td>\n",
       "      <td>0.601491</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.557653</td>\n",
       "      <td>0.79450</td>\n",
       "      <td>0.504880</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.483194</td>\n",
       "      <td>0.79450</td>\n",
       "      <td>0.446911</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449812</td>\n",
       "      <td>0.79450</td>\n",
       "      <td>0.423350</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.682171   0.78575  0.666874        0.8035\n",
       "1  0.640864   0.79450  0.601491        0.8035\n",
       "2  0.557653   0.79450  0.504880        0.8035\n",
       "3  0.483194   0.79450  0.446911        0.8035\n",
       "4  0.449812   0.79450  0.423350        0.8035"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Losses and Accuracy from Model\n",
    "loss_accuracy = pd.DataFrame(model.history.history)\n",
    "loss_accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = loss_accuracy[['loss','val_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682171</td>\n",
       "      <td>0.666874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.640864</td>\n",
       "      <td>0.601491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.557653</td>\n",
       "      <td>0.504880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.483194</td>\n",
       "      <td>0.446911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449812</td>\n",
       "      <td>0.423350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss\n",
       "0  0.682171  0.666874\n",
       "1  0.640864  0.601491\n",
       "2  0.557653  0.504880\n",
       "3  0.483194  0.446911\n",
       "4  0.449812  0.423350"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAu3UlEQVR4nO3deZxU9Znv8c9Te6/0RrN1A42AyCKgDe4oGnejMRqDWxInkasOmngnXvVmJuNkmUziTOJkYkKcxCyjRryujCJOTIxI3NhlR2i2brZeWHqrru25f5xCmra7KaDpak4979erX111lqqnGupbv/qd3/kdUVWMMca4lyfdBRhjjDmxLOiNMcblLOiNMcblLOiNMcblLOiNMcblfOkuoDMlJSU6fPjwdJdhjDEnjSVLltSpav/O1vXJoB8+fDiLFy9OdxnGGHPSEJGtXa2zrhtjjHE5C3pjjHE5C3pjjHG5PtlHb4zJPNFolOrqasLhcLpL6dNCoRBlZWX4/f6U97GgN8b0CdXV1eTl5TF8+HBEJN3l9EmqSn19PdXV1VRUVKS8n3XdGGP6hHA4THFxsYV8N0SE4uLio/7WY0FvjOkzLOSP7Fj+Rq4JelXlP/70MW9vqE13KcYY06e4JuhFhCcWVPHWuj3pLsUYc5LKzc1NdwknhGuCHqAoN0BDcyTdZRhjTJ/irqDPsaA3xhw/VeWBBx5g/PjxTJgwgTlz5gCwc+dOpk2bxqRJkxg/fjzvvPMO8Xicr3zlK59s+5Of/CTN1X+aq4ZXFucEqNlnY3CNOdn903+vZs2OAz36mGMH5/OPnx2X0rYvvvgiy5cvZ8WKFdTV1TFlyhSmTZvGM888w+WXX863vvUt4vE4LS0tLF++nJqaGlatWgXAvn37erTunuDCFn1busswxpzkFi5cyM0334zX62XAgAFceOGFLFq0iClTpvCb3/yGRx55hJUrV5KXl8eIESOoqqri3nvvZf78+eTn56e7/E9xVYu+KCdIQ3MEVbVhWsacxFJteZ8oqtrp8mnTprFgwQJee+01br/9dh544AG+9KUvsWLFCt544w0ef/xxnnvuOZ588slerrh7rmrRF+cEiMaVxrZYuksxxpzEpk2bxpw5c4jH49TW1rJgwQKmTp3K1q1bKS0t5c477+SrX/0qS5cupa6ujkQiwQ033MB3v/tdli5dmu7yP8VlLfoAAA1NEfJDqc8DYYwx7V1//fW89957TJw4ERHhRz/6EQMHDuR3v/sdjz76KH6/n9zcXH7/+99TU1PDHXfcQSKRAOAHP/hBmqv/NOnqK0o6VVZW6rFceGT/z6bz2M5xXDPzu5w5rPAEVGaMOVHWrl3Laaedlu4yTgqd/a1EZImqVna2vata9DkHNjJMSmyIpTHGtOOqPnoCueRK2EbeGGNMO64Kek8ojxxaqbcWvTHGfMJlQZ9PvqeNhiYLemOMOchVQU8glwJP2ProjTGmHXcFfTCXPE/Yum6MMaadlIJeRK4QkfUislFEHupim4tEZLmIrBaRt9st3yIiK5Prjn7M5NEIOH301qI3xphDjhj0IuIFHgeuBMYCN4vI2A7bFAA/B65V1XHAFzo8zHRVndTVGM8eE8wjSy3ojTEnXndz12/ZsoXx48f3YjXdS6VFPxXYqKpVqhoBngWu67DNLcCLqroNQFXTc/WPYC6hRAv1zTaDpTHGHJTKCVNDgO3t7lcDZ3XYZjTgF5G/AHnAv6vq75PrFPgfEVHgl6r6xPGV3I1ALh4SEA3TEomRHXDV+WDGZI7XH4JdK3v2MQdOgCv/pcvVDz74IMOGDeOee+4B4JFHHkFEWLBgAXv37iUajfK9732P667r2M7tXjgc5u6772bx4sX4fD5+/OMfM336dFavXs0dd9xBJBIhkUjwwgsvMHjwYG666Saqq6uJx+P8wz/8A1/84heP62VDakHf2TSQHedN8AFnApcAWcB7IvK+qm4AzlPVHSJSCvxRRNap6oJPPYnITGAmwNChQ4/mNRwSzAMglzD1TRGyiyzojTGpmTFjBt/4xjc+CfrnnnuO+fPnc//995Ofn09dXR1nn30211577VHNjvv4448DsHLlStatW8dll13Ghg0bmD17Nl//+te59dZbiUQixONx5s2bx+DBg3nttdcA2L9/f4+8tlSSsBoob3e/DNjRyTZ1qtoMNIvIAmAisEFVd4DTnSMiL+F0BX0q6JMt/SfAmevmaF8I8EnQ54jTT19elH1MD2OMSbNuWt4nyuTJk9mzZw87duygtraWwsJCBg0axP3338+CBQvweDzU1NSwe/duBg4cmPLjLly4kHvvvReAMWPGMGzYMDZs2MA555zD97//faqrq/n85z/PqFGjmDBhAt/85jd58MEHueaaa7jgggt65LWl0ke/CBglIhUiEgBmAHM7bPMKcIGI+EQkG6drZ62I5IhIHoCI5ACXAat6pPLOBJyDI7k28sYYcwxuvPFGnn/+eebMmcOMGTN4+umnqa2tZcmSJSxfvpwBAwYQDh/dMcCuJo685ZZbmDt3LllZWVx++eX8+c9/ZvTo0SxZsoQJEybw8MMP853vfKcnXtaRW/SqGhORWcAbgBd4UlVXi8hdyfWzVXWtiMwHPgISwK9UdZWIjABeSn7N8QHPqOr8Hqm8M8GDQW9j6Y0xR2/GjBnceeed1NXV8fbbb/Pcc89RWlqK3+/nrbfeYuvWrUf9mNOmTePpp5/m4osvZsOGDWzbto1TTz2VqqoqRowYwX333UdVVRUfffQRY8aMoaioiNtuu43c3Fx++9vf9sjrSqkTW1XnAfM6LJvd4f6jwKMdllXhdOH0jkD7rhub2MwYc3TGjRtHY2MjQ4YMYdCgQdx666189rOfpbKykkmTJjFmzJijfsx77rmHu+66iwkTJuDz+fjtb39LMBhkzpw5PPXUU/j9fgYOHMi3v/1tFi1axAMPPIDH48Hv9/OLX/yiR16Xq+ajp3YDPD6F+yKzqJj+Ze6/dHTPF2eMOSFsPvrUHe189K6bAgGgwBcmHI2nuRhjjOkb3DX+MHkwtsDbxt6IBb0x5sRauXIlt99++2HLgsEgH3zwQZoq6pw7g97Txg5r0Rtz0lHVoxqjnm4TJkxg+fLlvfqcx9Ld7q6uG48HArnke8K0WovemJNKKBSivr7+mIIsU6gq9fX1hEKho9rPXS16gEAuedE2Wq1Fb8xJpaysjOrqampra9NdSp8WCoUoKys7qn3cF/TBXPJirdaiN+Yk4/f7qaioSHcZruSurhuAQC45tNJiLXpjjAHcGPTBPLIJE7YWvTHGAG4Nem2lJRpLdyXGGNMnuC/oA7lkaTOtkUS6KzHGmD7BfUEfzCWYaLUzY40xJsmFQZ9HMN5CSyRm43GNMQY3Bn0gD59G8GqMSNy6b4wxxn1Bn5zYLAcbS2+MMeDGoD94lSkJ29mxxhiDG4P+4HVjrUVvjDGAK4P+YNdNmBYLemOMcWHQJy8nmCc2xNIYY8CNQd/uYKy16I0xJsWgF5ErRGS9iGwUkYe62OYiEVkuIqtF5O2j2bdHHeyjt4OxxhgDpDBNsYh4gceBS4FqYJGIzFXVNe22KQB+DlyhqttEpDTVfXvcwVE3WNeNMcZAai36qcBGVa1S1QjwLHBdh21uAV5U1W0AqrrnKPbtWcmgz6bNum6MMYbUgn4IsL3d/erksvZGA4Ui8hcRWSIiXzqKfQEQkZkislhEFh/XFWa8flQ8hCRiwyuNMYbUrjDV2ZV6O04i4wPOBC4BsoD3ROT9FPd1Fqo+ATwBUFlZeeyT1IiAL4tQNGJ99MYYQ2pBXw2Ut7tfBuzoZJs6VW0GmkVkATAxxX17nj9EVjjCfmvRG2NMSl03i4BRIlIhIgFgBjC3wzavABeIiE9EsoGzgLUp7tvjxJdFjidmLXpjjCGFFr2qxkRkFvAG4AWeVNXVInJXcv1sVV0rIvOBj4AE8CtVXQXQ2b4n6LUc4g+R44nawVhjjCG1rhtUdR4wr8Oy2R3uPwo8msq+J5wvi2xP1IZXGmMMbjwzFpw+eonQErHrxhpjjDuD3hciiyitUbvwiDHGuDPo/VmEJELY+uiNMcalQe8LEiRCS9S6bowxxqVBn0UQOzPWGGPArUHvDxHQNgt6Y4zBrUHvy8KvNgWCMcZAiuPoTzr+EP5EG60xC3pjjHFti96nUSLRGInEsc+PZowxbuDOoPeHAAgSIWytemNMhnNn0PuyAAgRsflujDEZz51Bn2zRh4jayBtjTMZzZ9AfbNGLjbwxxhh3Bv0nLXo7acoYY9wZ9MkWfZCoteiNMRnPnUF/sEVvFwg3xhiXBn27UTfWojfGZDp3Bn27cfQ2vNIYk+ncGfTtW/R2lSljTIZLKehF5AoRWS8iG0XkoU7WXyQi+0VkefLn2+3WbRGRlcnli3uy+C590kdvFwg3xpgjTmomIl7gceBSoBpYJCJzVXVNh03fUdVruniY6apad3ylHoVkiz6LCM0W9MaYDJdKi34qsFFVq1Q1AjwLXHdiyzpOyRZ9rjdqXTfGmIyXStAPAba3u1+dXNbROSKyQkReF5Fx7ZYr8D8iskREZnb1JCIyU0QWi8ji2tralIrvUrJFn+eLW4veGJPxUpmPXjpZ1nHu36XAMFVtEpGrgJeBUcl156nqDhEpBf4oIutUdcGnHlD1CeAJgMrKyuObW9jjAW+AXInS0mYtemNMZkulRV8NlLe7XwbsaL+Bqh5Q1abk7XmAX0RKkvd3JH/vAV7C6Qo68XxZ5HjsYKwxxqQS9IuAUSJSISIBYAYwt/0GIjJQRCR5e2rycetFJEdE8pLLc4DLgFU9+QK65A+R47WgN8aYI3bdqGpMRGYBbwBe4ElVXS0idyXXzwZuBO4WkRjQCsxQVRWRAcBLyc8AH/CMqs4/Qa/lcL4QWYkozXYw1hiT4VK6ZmyyO2Zeh2Wz293+GfCzTvarAiYeZ43Hxp9FVtTmujHGGHeeGQtOix5r0RtjjHuD3p9F0GavNMYYFwe9L0RQIzS3WdAbYzKbe4Pen0WANlqjcRKJ4xuWb4wxJzP3Br0vREAjADYnvTEmo7k36P1Z+BNtAHZA1hiT0dwb9L4QvmTQt1g/vTEmg7k36P1ZeOPJoLeRN8aYDObeoPeF8CbCgNJiXTfGmAzm3qD3hxBN4MemKjbGZDb3Br1dN9YYYwA3B/3B68ZiJ00ZYzKbe4M+2aIPSoQWG0dvjMlg7g36di16u8qUMSaTuTfoD7boidrBWGNMRnNv0Cdb9AX+mB2MNcZkNPcGfbJF389nwyuNMZnNvUGfbNHn+2LWR2+MyWjuDfpkiz7PF7MpEIwxGc29QZ9s0ed5LeiNMZktpaAXkStEZL2IbBSRhzpZf5GI7BeR5cmfb6e67wnjzwGgnzdi0xQbYzKa70gbiIgXeBy4FKgGFonIXFVd02HTd1T1mmPct+cF8wDI97TadWONMRktlRb9VGCjqlapagR4Frguxcc/nn2Pjz8E3gD50motemNMRksl6IcA29vdr04u6+gcEVkhIq+LyLij3BcRmSkii0VkcW1tbQplpSCYTy4t1qI3xmS0VIJeOlnW8WrbS4FhqjoR+A/g5aPY11mo+oSqVqpqZf/+/VMoKwWhfHK02SY1M8ZktFSCvhoob3e/DNjRfgNVPaCqTcnb8wC/iJSksu8JFcwnW1tojcZJJDr9fDHGGNdLJegXAaNEpEJEAsAMYG77DURkoIhI8vbU5OPWp7LvCRXKJzvRDECrzWBpjMlQRxx1o6oxEZkFvAF4gSdVdbWI3JVcPxu4EbhbRGJAKzBDVRXodN8T9Fo+LZhPMO58gWiOxMgJHvHlGmOM66SUfMnumHkdls1ud/tnwM9S3bfXhPoRjDcB0NIWh7y0VGGMMWnl3jNjAYL5+KPJoLeRN8aYDOXuoA/l44s1IyRosbH0xpgM5e6gD+YjKLmEbapiY0zGcnfQh/IByKOFprC16I0xmcndQZ+c7yZPWqhtDKe5GGOMSQ+XB73Toi/0trKnsS3NxRhjTHq4O+hD/QAoy4qx+4AFvTEmM7k76JMt+sFZEfZY140xJkO5O+iTB2MHBCLsPmBBb4zJTO4O+mSLvtTfZl03xpiM5e6g92eBx0eRL8z+1ihhm9jMGJOB3B30IhDMp5+nFYA91qo3xmQgdwc9QCiffJyg320HZI0xGcj9QR/MJ1udOentgKwxJhO5P+hD/QglDga9dd0YYzKP+4M+mI832kjA52GPteiNMRnI/UEfykfCjQzID1rXjTEmI7k/6IN50LafAXkh67oxxmSkDAj6fGhrZEBe0EbdGGMyUkpBLyJXiMh6EdkoIg91s90UEYmLyI3tlm0RkZUislxEFvdE0UcllA+aoCw3buPojTEZ6YgXBxcRL/A4cClQDSwSkbmquqaT7X4IvNHJw0xX1boeqPfoJadBKMuK0dTm/OQGU7omujHGuEIqLfqpwEZVrVLVCPAscF0n290LvADs6cH6jl9yYrNBoQiAjbwxxmScVIJ+CLC93f3q5LJPiMgQ4Hpgdif7K/A/IrJERGZ29SQiMlNEFovI4tra2hTKSlHQmZN+QMAJ+u17W3vusY0x5iSQStBLJ8u0w/3HgAdVtbNZw85T1TOAK4G/FZFpnT2Jqj6hqpWqWtm/f/8UykpRskU/Is8p7aPt+3rusY0x5iSQSmd1NVDe7n4ZsKPDNpXAsyICUAJcJSIxVX1ZVXcAqOoeEXkJpytowXFXnqrsYgByovWMLC1n6ba9vfbUxhjTF6TSol8EjBKRChEJADOAue03UNUKVR2uqsOB54F7VPVlEckRkTwAEckBLgNW9egrOJKCoSBeqN/EGUMLWLZ9H6odv5AYY4x7HTHoVTUGzMIZTbMWeE5VV4vIXSJy1xF2HwAsFJEVwIfAa6o6/3iLPipePxQOg4YqJg8tZF9LlM11zb1agjHGpFNK4wxVdR4wr8Oyzg68oqpfaXe7Cph4HPX1jKJToGETZ0wrBGDZtn2M6J+b5qKMMaZ3uP/MWIDiU6C+ipH9c8gN+qyf3hiTUTIj6ItOgWgz3pY9TCovYNm2femuyBhjek2GBP0I53dDFZOHFrBu1wGa22LprckYY3pJZgR9cTLo6zdxVkUxCYV3Pk7PjAzGGNPbMiPo+w0Fjw8aNnH2iCJKcgO8srwm3VUZY0yvyIyg9/qgcDjUb8Ln9XDN6YP507o97G+NprsyY4w54TIj6CE5xHIzAJ+bPIRILMH8VTvTXJQxxpx4mRP0xadAQxWoMrGsH8OLs3l5WceZHIwxxn0yJ+iLRkC0GRp3ISJ8bvIQ3t9czxY7S9YY43KZE/TFI53fe5zrpdwydSgBr4fH39qYxqKMMebEy5ygL5sC3iBs+jMApfkhbp46lBeX1bCtviXNxRljzImTOUEfzIWKC2DDoTnV7r7oFLwesVa9McbVMifoAUZfAfUboc4J9gH5IW6ZOpQXllazfldjmoszxpgTI7OCftRlzu+PD12//L5LRpEX8vH3L68kkbB56o0x7pNZQV84DPqfdlj3TVFOgIevOo1FW/by/JLqNBZnjDEnRmYFPcDoy2HruxDe/8miG88oY+rwIr732ho7MGuMcZ3MC/rTroVEDFa98Mkij0d49AunIyLM/K/FtERsZktjjHtkXtAPOQMGTIDFv4F2144dVpzDT2+ezIbdjfzdcyuIW3+9McYlMi/oRaDyK7DrI6hZetiqC0f351tXj+X1Vbt44P9Z2Btj3CHzgh5gwk3gz4ElT35q1VfPr+Cbl43mxWU1PPD8CqLxRBoKNMaYnpNS0IvIFSKyXkQ2ishD3Ww3RUTiInLj0e7bq0L5MOFGWPkCNO7+1OpZF4/i7y4dzYtLa5j5e+uzN8ac3I4Y9CLiBR4HrgTGAjeLyNgutvsh8MbR7psW594HmoDX/0+nq++9ZBT/fP0E3t5Qy81PvE99U1svF2iMMT0jlRb9VGCjqlapagR4Friuk+3uBV4A9hzDvr2vZCRc9CCseRnW/nenm9xy1lB+eXsl63Y1csMv3mVrvc10aYw5+aQS9EOA7e3uVyeXfUJEhgDXA7OPdt92jzFTRBaLyOLa2toUyuoB594HAyfAa38HLQ2dbnLp2AE8c+dZ7GuN8tn/WMibaz7d1WOMMX1ZKkEvnSzrOBzlMeBBVY0fw77OQtUnVLVSVSv79++fQlk9wOuH637uhPyr3zhsuGV7Zw4rYu7fnk95UTZf+/1ifvD6WmJ2kNYYc5JIJeirgfJ298uAjpdmqgSeFZEtwI3Az0Xkcynum16DTofp/xfWvAIrnu1ys6HF2bxw97lOd87bVdzynx+w+0C4Fws1xphjk0rQLwJGiUiFiASAGcDc9huoaoWqDlfV4cDzwD2q+nIq+/YJ530dhp7rdOFUL+5ys5Dfyz9fP4GffHEiK2v2c/VP3+HdjXW9WKgxxhy9Iwa9qsaAWTijadYCz6nqahG5S0TuOpZ9j7/sHubxwo1PQk4JPHUD7F7T7ebXTy5j7qzzKMgOcNuvP+Df3/zYTq4yxvRZol30S6dTZWWlLl7cdcv6hGnYDL+5EuJRuGUOlFV2u3lzW4y/f3kVLy2r4byRxTz2xcn0zwv2UrHGGHOIiCxR1U5DKzPPjO1KUQV8+VUI5MBvr4F1r3W7eU7Qx49vmsiPbjidxVv2ctVP3+HdTdaVY4zpWyzoOyoZCV/7EwwYC8/eCu93HDF6OBHhpinlvDLrPPJDPm771Qd879U1djatMabPsKDvTG5/p2U/5mqY/yC8ej+0NXW7y5iB+cyddT43Tx3KrxZu5vLHFrBka+dj840xpjdZ0HclkA03/d45qWrxk/CLc2DTW93ukhP08f3rJzBn5tkA3PTL9/n3Nz+2idGMMWllQd8djxcu+y7c8Tp4/PBfn4O59x52darOnDWimHn3XcBnTx/ET97cwNU/fYcPN1vr3hiTHhb0qRh2Ltz9V2e8/bKn4PGzYP38bnfJC/l5bMZk/vNLlTS3xbnpl+/x6Bvr7IxaY0yvs+GVR6tmCbwyC/asgWHnweTbYfwN4At0uUtLJMZ3/nsNzy7azqTyAu6+6BQ+c9oAvJ7OZogwxpij193wSgv6YxGLwAeznb77vZth8Blw0++gYGi3u72yvIYfvr6OHfvDDC3K5sErxnDVhIGIWOAbY46PBf2JoupMczz3PhAPXP1vTuu+m+COxRO8uXY3j735Met2NTJhSD++cu5wrj59ECG/t/dqN8a4igX9iVa/CV74GuxYCiOmw2Xfg4Hju90lnlCeX7KdXy6ooqq2mf55Qe668BRumTqUrIAFvjHm6FjQ94ZEHBb9Gv78XWg7AKdeBQXDILcUzvpfztm2nVBVFm6s4+dvbeK9qnqGFWfz45smceawwl5+AcaYk5kFfW9qaYD3fw7LnoZIkxP6peNgxlNQNKLbXd/dWMcDz3/Ezv2t3FRZzhcqyzhjaKH14RtjjsiCPp02/gle+CrEY3D+1+Hse7ps3QM0hqP8y+vreHFpDa3ROMOKs/n85DI+f8YQyouye7FwY8zJxII+3fZuhfkPw/rXIHcgXPkvMPZz3R60bWqLMX/VLl5cWs17VfWowtThRXztggouHTvAWvnGmMNY0PcV2z+Eed+EnSucMfhjroGRl0DJ6G5Dv2ZfKy8vq2HOou1sa2hh8tACbpk6lEvHDqAgu+vx+8aYzGFB35fEY/DhE84Y/PqPnWX9yqFsChQOg1GXOWfidiIaT/DCkmoe/8tGtje04vMI544s4arxA7ls3ECKciz0jclUFvR91d6tsOlPTj/+7lWwvxoSMZh4i3Md24LyTndTVVbW7Oe1lTt5feUutjW04PUIZ48o4srxg7h83EC7AIoxGcaC/mQRaYEFj8K7P3UCv2Q0DJxwqMU/4kII5h22i6qyescBXl+1k3krd7G5rhkRpz//0rEDmDy0gLGD+tnYfGNczoL+ZFO/CdbPg6q3oX5jsqUfBfFCKB9C/eDCh2DSzYftpqqs393IvJW7mLdyJxv3OHPoez3CqNJcpgwv4pLTSjl7RLGdhWuMyxx30IvIFcC/A17gV6r6Lx3WXwd8F0gAMeAbqrowuW4L0AjEgVhXhbSX8UHfUSwC29+HzQugdR/sXA7Vi+CMLzsHc4N5EMiDnGIorPjkwO7uA2E+qt7PR9X7WFG9n0WbG2iNxhGBwf2yGNE/h4qSHM4cVsj0MaXkh/xpfZnGmGN3XEEvIl5gA3ApUA0sAm5W1TXttskFmlVVReR04DlVHZNctwWoVNWUL6ZqQX8E8Rj86Z+cLp6OcgfCKdNh7HVwysXgO9RXH47GeXdTHSurD7C5romqumaqaptpaovh9wojS/MYUZLD6WX9qBxeSJbfh8cDo0vz8NhMm8b0ad0FvS+F/acCG1W1KvlgzwLXAZ8Evaq2v85eDtD3+oPcxOtzLohyzt9Cc61zmcO2RjhQA1vegfWvw4o/OBOtZRU5LX4RQjmlXDzsXC4eciZMOhUKx5MQH6vXrqTu/TksaRvCKzVjeG3lzsOebmxBlJlDtvKOTmRzk58bzizjhjPKrPvHmJNEKi36G4ErVPVryfu3A2ep6qwO210P/AAoBa5W1feSyzcDe3HC/5eq+kQXzzMTmAkwdOjQM7du3Xo8ryuzxSJQ9ZYzbr+1wfkQ0IQzymfHMtC4s53H5xzo3bfVWQ9wyiXsH3szH+loJLyX3B1/ZeTan5OrTewjjz9kzeCpveNpDA3k1NIcygpD5GdnUZofYuygfIYVZxPweSjMDpATTKUdYYzpCcfbdfMF4PIOQT9VVe/tYvtpwLdV9TPJ+4NVdYeIlAJ/BO5V1QXdPad13ZxAbU1Qux7qNjg/9Rud0T2TboENb8CCH0Hr3sP3qbiQaOWd+D+cDVsXOg/jycKfCJPAwzqG805sLE/HP0O19v9kt8H9QgwuyKIgO0BFSTanlxUQ8ntpDEcJ+Dzkh/wMKcyivDAbv1dQxbqIjDlGxxv05wCPqOrlyfsPA6jqD7rZZzMwpWO/vIg8AjSp6r9295wW9GkUjzpn7tYsdQ7u9h8DpWOdA7yqsGslbP/A+YAI9YNYG1QvRre9Byh1xVPYl3sKLZEEWfs2oNEwDZrHmkgJy2IjeDcxjgbyyaeZ6Z5lrNOhrNehCAmGSi2jB/VjzLDB+HJKCPk9DC7IYkB+CK9HCPo8FOcGKMoJEPRZt5Ex7R1vH/0iYJSIVAA1wAzglg5PMBLYlDwYewYQAOpFJAfwqGpj8vZlwHeO47WYE83rh7JK56cjERh0uvPTcdX+GvjwCfpXvUX/HXOdhf1PhUAJNNdzdv1yxNOGiodI6ST89WvxxFoBqM0eRW5kD1mx/dAANMDKxHA+TJyGV+pJSCNViQHUUkAhjdRqAc/6r8UXyifk96AKzZEY5YXZXDZuAPkhP9V7W2mJOCOMxg3O56JTS8nye2mOxAj4PGT7vfi8dsnkHrfpz05j4Nz7up3Ww/SuVIdXXgU8hjO88klV/b6I3AWgqrNF5EHgS0AUaAUeUNWFIjICeCn5MD7gGVX9/pGez1r0J7mD/6fav9FjEdi90uke2vgmlJ4Gk25zLtay9lVnCufyKeDxQ+NOdMMbsGMZ0bxywv4CQo1b8bftpc3fj2B0H43+Elbkns+A8GZy4vsRj5e6WIiP2wrZkChjDSOI+bIJJsI0xjy0EsRDAj9x6shnr+Zxak4zo3LC+PP7UxDyUtGwkKLoTnYWT2VH8XlsbvTQEomTF/JRkhtkeHE2w4pzGF6cQ9DvobktRlFOoNv5hhIJpTkSI6/j0NV9250utBEXOh+uiYRznMR7Ao5rxGMQ3u98QwNniG4ifuh+qhJx8HTzTWr9fJhzm3POx9X/BlO+5nQVxiOQXXT4tpFm8GWB5xg/bBMJZ/ABeugSnok4tNQ7r7VgWLfXcU6rqrdh4x+dD8Pc0h57WDthypycVA//sDh4f/simPd3TlAOGAd5g5yQbGkgvncr3qadXT/mEbSQRTatNGuQVwJXsTJrCmXhDTSEhbmRKZTLHu7xvUKQKEt1FG0aoDzQyNBgEwM9B8iP7yUrfoAd3jKW6UgWtFZQHSvgjrxFTPF9zPrsSlolxGX1T+FPhInkDKaxZBJZNX/Fl2hjX+nZ+E/9DPERF9MQLKd6Xyulso8x8Y/xFZbDwNMP/5vEo9C4k0htFYGdi2HvFufcitFXgD+LRLgRz7M3w9Z3nRPs8ofAez939r36X+H0Lx655a0K7//CGdI7+Xb4zD8efoa2Kix/Bl79htPNl10EW/4K594Li/7TCfuRl8DZdztDfquXwFOfh5JRcONvoF8ZHNjhhJ73COdyNO1xLu6z8gWINjvLSsc5+1YvhkjjoWVf/u9DH2ZtTc5ItBHToWTkocer2wjLk9eO8Abg1Cth6LmHfwAlEofuJxJQ9WdY9KQz0KFktLNf617ntU26BRp3OdekOO2zUD710OPsXAFv/pMz7QlAdglc9zPnOcH58NuzDsrO7P5v0AULeuNO7d+A7bU0OCeVxWPgz3JalJFmJ0TEC817oLnO+YDI7Q/N9RBvgxEXOcu2f4gu/jWy6oVDo5EARRCUcKCIlmAphY0bEBKEPTk0SAG74vnU0Y8msjnNW8PIeBV+ogDE8LFahzNOqvCR4I/xM3klfi63ev/EcM8u3tdxhCWLs3UFFZ7dAETVSwQfOdL2SQ07tZh93iKyvQn6JfaSH9+Lp91o5og3l0C8iRbJ4k2dypDEDiZ6NvG29zzOj71PUKIszbmAgd5GBh9YTlNuBYmsYmL5Q4gWnQq5pXh9QQItuwgc2ILH68PbtAvvxjfQARNg9yoiwSLILiHg9xLJG0aiaTdZu5ei5WfRcsPTqMbJffIiaNxJvOJCEgMn4l/1PDTucMJv018gq8D5ZuHxOv9GjTshmA/lZ0FLnfOB5c9xPjQKhztBvr8GtiyEWCtMvBmGnAHRVlj3mtOKLz/L+aaYiMOb/+h8kEz/e6hbD+/+zPl39+fAFT+AnBJnGPLyZ5wPukCu81jxNmck2oQvON8UFv8adq9xJhz0Zzt1RZogp9T5Flq3wRnF5suCpl0wpNJpgEQanf9r598P/YY4JzuufgmyCuGCb8Lw8+GVWc633FGXO/ff/Q/nse5f7fxNjpIFvTHHon4TNFTBoElO6231S04Ynfll5+IxkWbnXIWu3pTRMOz6CBo2O6293P7QVAuNOwgXj2P73lY21zWTUDhvpDMtxZKte6nfvp6S3X+lMLabfr44Dd7+vB+poF/zFk5t+gAizTRHYZ+ngH2+/ki/ISTyy3hqWyGrG4TpoQ3cmv0h54QXEtBW/lD+CB9kXcDwYDOxln3Mrc5m575m7vDOZ4pnPf1optyzhzI5/JzGWu0HKEFi/DJ2Db9IXMskNvJV3zy8KF5JUMFOgkT5afx65nIRbcmRu5W5dZR79/LygZGoCgOzYZbvFWZEnmeXZwAz5R/xJdr4pv6OhD+bmtzxjJIaRoRX0egrYqd3CN5EGznRBgrbdpCf2Eu9pz/VwVP4oOxv8PQfDSRP2FH95KMuK+Clf26QIXV/5awPZ+FNOB+0+0rOYNnQv+G0Lb9jYMMiAGIS4OPyG9h5+iw0pz8BDTOi7i+Ubn4F35a/IBqnteBUthWdQ3brTkK0kTNwJJRNZU3BhUTwUZoXJOD1EonFKN7wLAULv0dk4GR2TXmQoo/+k7wNLzp1BvKQqXfCeV+HrAJaIjFeWlRF7vJfc0XDUwTjTTQOPIt95zxM+cTpx/Tf1YLemAygquxpbKN/btAZphoNOx9Q+YM/tW04Gmd/a5R9LVH2tkRojcSJtR5AwnuJR9po9BXR4skmEksQiSeIxBIkFEaV5lJRksOanQeoqm1mUL8QBdl+ava1sq8lSmF2AEXZtKeZtlicU/rnEvB5qN7bwv7WKFktO0kE8wnlFhLwCrGEUtfUxo59YXbsa6W+OULQ56EoJ0DA58Hv9VCcE6Bflp9IPMHelijVDS3UN0cOez0iIECiXZwNk10Uc4AaLWE3hYDgIcFnPEuo13xWaQVtdN6PP9h3gEGylyXRoTiPnBoPCRIc+pY5WrbTpFnspAif10t2wEd2wEtjOEZTW4yS3ADxpjoGyl7W6lBKcoMs/vtLU36+w/8GxzfqxhhzEhARBuSHDi3wh8D/6ZAHCPm9hPzew7cn9QOD44f0O8YquxeNJ/B55IhXUEsk1An3Dtu1RGLUNrYRTyghv5emthi79ofJDfkY3C8LEQhHLyY/5Cc/y099cxt7DjjbR+IJ9hxoY9eBMLsPhInGE3xteBFjB+fjEflk7qhoPMHoAXmE/F72NIaJxRWfV2iLJjgQjhLye+mX5SeeUFoiE2iJxGiJxGmJxGlN3vZ5hRvPdK4J3dAcoaqumea2GCeq3W0temOMcYHuWvQ2kNgYY1zOgt4YY1zOgt4YY1zOgt4YY1zOgt4YY1zOgt4YY1zOgt4YY1zOgt4YY1yuT54wJSK1wLFeS7AESPlC5L2or9YFfbe2vloX9N3a+mpd0Hdr66t1wdHVNky13SXe2umTQX88RGRxV2eHpVNfrQv6bm19tS7ou7X11bqg79bWV+uCnqvNum6MMcblLOiNMcbl3Bj0T6S7gC701bqg79bWV+uCvltbX60L+m5tfbUu6KHaXNdHb4wx5nBubNEbY4xpx4LeGGNczjVBLyJXiMh6EdkoIg+luZZyEXlLRNaKyGoR+XpyeZGI/FFEPk7+LkxTfV4RWSYir/axugpE5HkRWZf8253TF2oTkfuT/46rROQPIhJKV10i8qSI7BGRVe2WdVmLiDycfE+sF5HL01Dbo8l/z49E5CURKejt2jqrq926b4qIikhJb9fVXW0icm/y+VeLyI+OuzZVPel/AC+wCRgBBIAVwNg01jMIOCN5Ow/YAIwFfgQ8lFz+EPDDNNX3v4FngFeT9/tKXb8Dvpa8HQAK0l0bMATYDGQl7z8HfCVddQHTgDOAVe2WdVpL8v/cCiAIVCTfI95eru0ywJe8/cN01NZZXcnl5cAbOCdnlvShv9l04E0gmLxfery19dob5kT+AOcAb7S7/zDwcLrralfPK8ClwHpgUHLZIGB9GmopA/4EXNwu6PtCXfnJQJUOy9NaWzLotwNFONdYfjUZXmmrCxjeIRg6raXj+yAZauf0Zm0d1l0PPJ2O2jqrC3gemAhsaRf0af+b4TQmPtPJdsdcm1u6bg6+GQ+qTi5LOxEZDkwGPgAGqOpOgOTv1K/G3HMeA/4PkGi3rC/UNQKoBX6T7Fb6lYjkpLs2Va0B/hXYBuwE9qvq/6S7rg66qqWvvS/+Bng9eTuttYnItUCNqq7osKov/M1GAxeIyAci8raITDne2twS9J1dMj7t40ZFJBd4AfiGqh7oA/VcA+xR1SXprqUTPpyvsL9Q1clAM043RFol+7uvw/mqPBjIEZHb0ltVyvrM+0JEvgXEgKcPLupks16pTUSygW8B3+5sdSfLevtv5gMKgbOBB4DnREQ4jtrcEvTVOP1tB5UBO9JUCwAi4scJ+adV9cXk4t0iMii5fhCwp5fLOg+4VkS2AM8CF4vIU32gLnD+DatV9YPk/edxgj/dtX0G2KyqtaoaBV4Ezu0DdbXXVS194n0hIl8GrgFu1WSfQ5prOwXng3tF8r1QBiwVkYFpruugauBFdXyI8+275Hhqc0vQLwJGiUiFiASAGcDcdBWT/PT9NbBWVX/cbtVc4MvJ21/G6bvvNar6sKqWqepwnL/Rn1X1tnTXlaxtF7BdRE5NLroEWNMHatsGnC0i2cl/10uAtX2grva6qmUuMENEgiJSAYwCPuzNwkTkCuBB4FpVbWm3Km21qepKVS1V1eHJ90I1zuCJXemsq52XcY6hISKjcQYm1B1XbSfyIENv/gBX4Yxu2QR8K821nI/zleojYHny5yqgGOdA6MfJ30VprPEiDh2M7RN1AZOAxcm/28s4X1/TXhvwT8A6YBXwXzijHtJSF/AHnGMFUZyA+mp3teB0UWzCOWB7ZRpq24jTr3zwfTC7t2vrrK4O67eQPBjbR/5mAeCp5P+3pcDFx1ubTYFgjDEu55auG2OMMV2woDfGGJezoDfGGJezoDfGGJezoDfGGJezoDfGGJezoDfGGJf7/081eJ8x4GbTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "#Adding the input layer and the first hidden layer\n",
    "\n",
    "#input_dim is the total features to predict the label\n",
    "#units are the neurons in the first hidden layer\n",
    "# kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights. (0 to 1)\n",
    "#In case of statistical distribution, the library will generate numbers from that statistical distribution and use as starting weights.\n",
    "model.add(Dense(activation = 'relu', input_dim=11 , units = 6, kernel_initializer = 'uniform'))\n",
    "model.add(Dropout(0.2)) #Dropping 205 neurons\n",
    "\n",
    "#Adding second hidden layer\n",
    "model.add(Dense(activation = 'relu', units=6 , kernel_initializer='uniform'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adding the output layer\n",
    "model.add(Dense(activation='sigmoid', units=1, kernel_initializer='uniform'))\n",
    "\n",
    "#Compiling the ANN\n",
    "#(binary_crossentropy used when just binary label is there to predict)(just one output)\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "63/63 [==============================] - 2s 6ms/step - loss: 0.6874 - accuracy: 0.7818 - val_loss: 0.6611 - val_accuracy: 0.8035\n",
      "Epoch 2/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6403 - accuracy: 0.8007 - val_loss: 0.5485 - val_accuracy: 0.8035\n",
      "Epoch 3/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7930 - val_loss: 0.4498 - val_accuracy: 0.8035\n",
      "Epoch 4/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7945 - val_loss: 0.4302 - val_accuracy: 0.8035\n",
      "Epoch 5/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.7959 - val_loss: 0.4247 - val_accuracy: 0.8035\n",
      "Epoch 6/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4510 - accuracy: 0.7969 - val_loss: 0.4216 - val_accuracy: 0.8035\n",
      "Epoch 7/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4365 - accuracy: 0.8032 - val_loss: 0.4202 - val_accuracy: 0.8035\n",
      "Epoch 8/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4486 - accuracy: 0.7931 - val_loss: 0.4184 - val_accuracy: 0.8035\n",
      "Epoch 9/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4435 - accuracy: 0.7928 - val_loss: 0.4174 - val_accuracy: 0.8035\n",
      "Epoch 10/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.7893 - val_loss: 0.4164 - val_accuracy: 0.8035\n",
      "Epoch 11/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7933 - val_loss: 0.4164 - val_accuracy: 0.8035\n",
      "Epoch 12/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7921 - val_loss: 0.4155 - val_accuracy: 0.8035\n",
      "Epoch 13/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.7983 - val_loss: 0.4153 - val_accuracy: 0.8035\n",
      "Epoch 14/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7965 - val_loss: 0.4144 - val_accuracy: 0.8035\n",
      "Epoch 15/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4330 - accuracy: 0.8046 - val_loss: 0.4144 - val_accuracy: 0.8035\n",
      "Epoch 16/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.7965 - val_loss: 0.4143 - val_accuracy: 0.8035\n",
      "Epoch 17/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.7945 - val_loss: 0.4130 - val_accuracy: 0.8035\n",
      "Epoch 18/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.8007 - val_loss: 0.4127 - val_accuracy: 0.8035\n",
      "Epoch 19/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.7910 - val_loss: 0.4120 - val_accuracy: 0.8035\n",
      "Epoch 20/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4442 - accuracy: 0.7934 - val_loss: 0.4118 - val_accuracy: 0.8035\n",
      "Epoch 21/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4456 - accuracy: 0.7926 - val_loss: 0.4106 - val_accuracy: 0.8035\n",
      "Epoch 22/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4389 - accuracy: 0.7942 - val_loss: 0.4105 - val_accuracy: 0.8035\n",
      "Epoch 23/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4408 - accuracy: 0.7991 - val_loss: 0.4100 - val_accuracy: 0.8035\n",
      "Epoch 24/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4502 - accuracy: 0.7892 - val_loss: 0.4099 - val_accuracy: 0.8035\n",
      "Epoch 25/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.7946 - val_loss: 0.4094 - val_accuracy: 0.8035\n",
      "Epoch 26/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4468 - accuracy: 0.7898 - val_loss: 0.4089 - val_accuracy: 0.8035\n",
      "Epoch 27/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4337 - accuracy: 0.7963 - val_loss: 0.4091 - val_accuracy: 0.8035\n",
      "Epoch 28/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4418 - accuracy: 0.7975 - val_loss: 0.4087 - val_accuracy: 0.8035\n",
      "Epoch 29/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4398 - accuracy: 0.7928 - val_loss: 0.4083 - val_accuracy: 0.8035\n",
      "Epoch 30/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4448 - accuracy: 0.7932 - val_loss: 0.4083 - val_accuracy: 0.8035\n",
      "Epoch 31/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4356 - accuracy: 0.7981 - val_loss: 0.4079 - val_accuracy: 0.8035\n",
      "Epoch 32/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4399 - accuracy: 0.7950 - val_loss: 0.4076 - val_accuracy: 0.8035\n",
      "Epoch 33/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4269 - accuracy: 0.8040 - val_loss: 0.4081 - val_accuracy: 0.8035\n",
      "Epoch 34/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4498 - accuracy: 0.7910 - val_loss: 0.4071 - val_accuracy: 0.8035\n",
      "Epoch 35/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.7995 - val_loss: 0.4072 - val_accuracy: 0.8035\n",
      "Epoch 36/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.7866 - val_loss: 0.4073 - val_accuracy: 0.8035\n",
      "Epoch 37/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.7916 - val_loss: 0.4075 - val_accuracy: 0.8035\n",
      "Epoch 38/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.7915 - val_loss: 0.4073 - val_accuracy: 0.8035\n",
      "Epoch 39/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4449 - accuracy: 0.7882 - val_loss: 0.4070 - val_accuracy: 0.8035\n",
      "Epoch 40/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4235 - accuracy: 0.8039 - val_loss: 0.4070 - val_accuracy: 0.8035\n",
      "Epoch 41/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4233 - accuracy: 0.7979 - val_loss: 0.4081 - val_accuracy: 0.8035\n",
      "Epoch 42/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4365 - accuracy: 0.7975 - val_loss: 0.4070 - val_accuracy: 0.8035\n",
      "Epoch 43/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4400 - accuracy: 0.7960 - val_loss: 0.4069 - val_accuracy: 0.8035\n",
      "Epoch 44/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4320 - accuracy: 0.7959 - val_loss: 0.4062 - val_accuracy: 0.8035\n",
      "Epoch 45/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.7993 - val_loss: 0.4061 - val_accuracy: 0.8035\n",
      "Epoch 46/600\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4419 - accuracy: 0.7908 - val_loss: 0.4062 - val_accuracy: 0.8035\n",
      "Epoch 47/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4367 - accuracy: 0.7921 - val_loss: 0.4054 - val_accuracy: 0.8035\n",
      "Epoch 48/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4412 - accuracy: 0.7961 - val_loss: 0.4055 - val_accuracy: 0.8035\n",
      "Epoch 49/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.7893 - val_loss: 0.4058 - val_accuracy: 0.8035\n",
      "Epoch 50/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4364 - accuracy: 0.7935 - val_loss: 0.4054 - val_accuracy: 0.8035\n",
      "Epoch 51/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4351 - accuracy: 0.8007 - val_loss: 0.4056 - val_accuracy: 0.8035\n",
      "Epoch 52/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4242 - accuracy: 0.8001 - val_loss: 0.4053 - val_accuracy: 0.8035\n",
      "Epoch 53/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4348 - accuracy: 0.7997 - val_loss: 0.4063 - val_accuracy: 0.8035\n",
      "Epoch 54/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.7962 - val_loss: 0.4063 - val_accuracy: 0.8035\n",
      "Epoch 55/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4338 - accuracy: 0.7949 - val_loss: 0.4062 - val_accuracy: 0.8035\n",
      "Epoch 56/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4307 - accuracy: 0.7948 - val_loss: 0.4055 - val_accuracy: 0.8035\n",
      "Epoch 57/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4326 - accuracy: 0.8007 - val_loss: 0.4053 - val_accuracy: 0.8035\n",
      "Epoch 58/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.8027 - val_loss: 0.4053 - val_accuracy: 0.8035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.7926 - val_loss: 0.4061 - val_accuracy: 0.8035\n",
      "Epoch 60/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4342 - accuracy: 0.7948 - val_loss: 0.4049 - val_accuracy: 0.8035\n",
      "Epoch 61/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4419 - accuracy: 0.7890 - val_loss: 0.4045 - val_accuracy: 0.8035\n",
      "Epoch 62/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4421 - accuracy: 0.7896 - val_loss: 0.4051 - val_accuracy: 0.8035\n",
      "Epoch 63/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4282 - accuracy: 0.8055 - val_loss: 0.4057 - val_accuracy: 0.8035\n",
      "Epoch 64/600\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4345 - accuracy: 0.7980 - val_loss: 0.4054 - val_accuracy: 0.8035\n",
      "Epoch 65/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.7937 - val_loss: 0.4053 - val_accuracy: 0.8035\n",
      "Epoch 66/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4310 - accuracy: 0.7985 - val_loss: 0.4050 - val_accuracy: 0.8035\n",
      "Epoch 67/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4369 - accuracy: 0.7931 - val_loss: 0.4049 - val_accuracy: 0.8035\n",
      "Epoch 68/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4358 - accuracy: 0.7948 - val_loss: 0.4040 - val_accuracy: 0.8035\n",
      "Epoch 69/600\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4327 - accuracy: 0.7979 - val_loss: 0.4042 - val_accuracy: 0.8035\n",
      "Epoch 70/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4330 - accuracy: 0.7958 - val_loss: 0.4041 - val_accuracy: 0.8035\n",
      "Epoch 71/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.7904 - val_loss: 0.4041 - val_accuracy: 0.8035\n",
      "Epoch 72/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.7922 - val_loss: 0.4034 - val_accuracy: 0.8035\n",
      "Epoch 73/600\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.7959 - val_loss: 0.4035 - val_accuracy: 0.8035\n",
      "Epoch 74/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.7958 - val_loss: 0.4041 - val_accuracy: 0.8035\n",
      "Epoch 75/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4338 - accuracy: 0.7938 - val_loss: 0.4040 - val_accuracy: 0.8035\n",
      "Epoch 76/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.7975 - val_loss: 0.4033 - val_accuracy: 0.8035\n",
      "Epoch 77/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.7958 - val_loss: 0.4036 - val_accuracy: 0.8035\n",
      "Epoch 78/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4345 - accuracy: 0.7993 - val_loss: 0.4030 - val_accuracy: 0.8035\n",
      "Epoch 79/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.7951 - val_loss: 0.4026 - val_accuracy: 0.8035\n",
      "Epoch 80/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7947 - val_loss: 0.4033 - val_accuracy: 0.8035\n",
      "Epoch 81/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.7956 - val_loss: 0.4033 - val_accuracy: 0.8035\n",
      "Epoch 82/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.7963 - val_loss: 0.4037 - val_accuracy: 0.8035\n",
      "Epoch 83/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.7912 - val_loss: 0.4027 - val_accuracy: 0.8035\n",
      "Epoch 84/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7921 - val_loss: 0.4031 - val_accuracy: 0.8035\n",
      "Epoch 85/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.7960 - val_loss: 0.4030 - val_accuracy: 0.8035\n",
      "Epoch 86/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.7932 - val_loss: 0.4033 - val_accuracy: 0.8035\n",
      "Epoch 87/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.7925 - val_loss: 0.4027 - val_accuracy: 0.8035\n",
      "Epoch 88/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4404 - accuracy: 0.7933 - val_loss: 0.4030 - val_accuracy: 0.8035\n",
      "Epoch 89/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.7959 - val_loss: 0.4028 - val_accuracy: 0.8035\n",
      "Epoch 90/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.7938 - val_loss: 0.4029 - val_accuracy: 0.8035\n",
      "Epoch 91/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.7962 - val_loss: 0.4026 - val_accuracy: 0.8035\n",
      "Epoch 92/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.7957 - val_loss: 0.4023 - val_accuracy: 0.8035\n",
      "Epoch 93/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.7958 - val_loss: 0.4023 - val_accuracy: 0.8035\n",
      "Epoch 94/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7879 - val_loss: 0.4023 - val_accuracy: 0.8035\n",
      "Epoch 95/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.7927 - val_loss: 0.4024 - val_accuracy: 0.8035\n",
      "Epoch 96/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.7946 - val_loss: 0.4022 - val_accuracy: 0.8035\n",
      "Epoch 97/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7977 - val_loss: 0.4021 - val_accuracy: 0.8035\n",
      "Epoch 98/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.7964 - val_loss: 0.4027 - val_accuracy: 0.8035\n",
      "Epoch 99/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.7953 - val_loss: 0.4020 - val_accuracy: 0.8035\n",
      "Epoch 100/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.7963 - val_loss: 0.4021 - val_accuracy: 0.8035\n",
      "Epoch 101/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.7909 - val_loss: 0.4015 - val_accuracy: 0.8035\n",
      "Epoch 102/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.7970 - val_loss: 0.4019 - val_accuracy: 0.8035\n",
      "Epoch 103/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4351 - accuracy: 0.7905 - val_loss: 0.4018 - val_accuracy: 0.8035\n",
      "Epoch 104/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4289 - accuracy: 0.7951 - val_loss: 0.4022 - val_accuracy: 0.8035\n",
      "Epoch 105/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.7928 - val_loss: 0.4018 - val_accuracy: 0.8035\n",
      "Epoch 106/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.7978 - val_loss: 0.4017 - val_accuracy: 0.8035\n",
      "Epoch 107/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.7891 - val_loss: 0.4008 - val_accuracy: 0.8035\n",
      "Epoch 108/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7885 - val_loss: 0.4003 - val_accuracy: 0.8035\n",
      "Epoch 109/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4281 - accuracy: 0.7998 - val_loss: 0.4015 - val_accuracy: 0.8035\n",
      "Epoch 110/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.7970 - val_loss: 0.4019 - val_accuracy: 0.8035\n",
      "Epoch 111/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8031 - val_loss: 0.4021 - val_accuracy: 0.8035\n",
      "Epoch 112/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.7985 - val_loss: 0.4017 - val_accuracy: 0.8035\n",
      "Epoch 113/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.7901 - val_loss: 0.4011 - val_accuracy: 0.8035\n",
      "Epoch 114/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.7978 - val_loss: 0.4029 - val_accuracy: 0.8035\n",
      "Epoch 115/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.7959 - val_loss: 0.4013 - val_accuracy: 0.8035\n",
      "Epoch 116/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.7910 - val_loss: 0.4014 - val_accuracy: 0.8035\n",
      "Epoch 117/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.7902 - val_loss: 0.4013 - val_accuracy: 0.8035\n",
      "Epoch 118/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7904 - val_loss: 0.4011 - val_accuracy: 0.8035\n",
      "Epoch 119/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.7918 - val_loss: 0.4007 - val_accuracy: 0.8035\n",
      "Epoch 120/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4384 - accuracy: 0.7949 - val_loss: 0.4010 - val_accuracy: 0.8035\n",
      "Epoch 121/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.7861 - val_loss: 0.4007 - val_accuracy: 0.8035\n",
      "Epoch 122/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4259 - accuracy: 0.8023 - val_loss: 0.4006 - val_accuracy: 0.8035\n",
      "Epoch 123/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.7999 - val_loss: 0.4009 - val_accuracy: 0.8035\n",
      "Epoch 124/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.7903 - val_loss: 0.4007 - val_accuracy: 0.8035\n",
      "Epoch 125/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7920 - val_loss: 0.4009 - val_accuracy: 0.8035\n",
      "Epoch 126/600\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.7978 - val_loss: 0.4009 - val_accuracy: 0.8035\n",
      "Epoch 127/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8023 - val_loss: 0.4011 - val_accuracy: 0.8035\n",
      "Epoch 128/600\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4344 - accuracy: 0.7959 - val_loss: 0.4013 - val_accuracy: 0.8035\n",
      "Epoch 00128: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x170878a9130>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the ANN to the data\n",
    "model.fit(X_scaled_train,y_train,batch_size=128,epochs=600,validation_data=(X_scaled_test,y_test),\n",
    "         callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.681084</td>\n",
       "      <td>0.790375</td>\n",
       "      <td>0.661110</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.617324</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.548465</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.505342</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.449836</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.462099</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.430156</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.452403</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.424655</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.681084  0.790375  0.661110        0.8035\n",
       "1  0.617324  0.794500  0.548465        0.8035\n",
       "2  0.505342  0.794500  0.449836        0.8035\n",
       "3  0.462099  0.794500  0.430156        0.8035\n",
       "4  0.452403  0.794500  0.424655        0.8035"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Losses and Accuracy from Model\n",
    "loss_accuracy = pd.DataFrame(model.history.history)\n",
    "loss_accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = loss_accuracy[['loss','val_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.681084</td>\n",
       "      <td>0.661110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.617324</td>\n",
       "      <td>0.548465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.505342</td>\n",
       "      <td>0.449836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.462099</td>\n",
       "      <td>0.430156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.452403</td>\n",
       "      <td>0.424655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss\n",
       "0  0.681084  0.661110\n",
       "1  0.617324  0.548465\n",
       "2  0.505342  0.449836\n",
       "3  0.462099  0.430156\n",
       "4  0.452403  0.424655"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAySElEQVR4nO3deXxU9b3/8ddn9qxAFhJIWIKyiCBoWUQFba2Ce229iqi1avVa69ZerXr91dre9nbh1mpbW0u91LpdtK5UEbVuqAXKvu/IkgTIAtmX2b6/P76TEEIgAwSSnPk8H495JHPmnMlnAnmf7/mcTYwxKKWUci5XZxeglFLq+NKgV0oph9OgV0oph9OgV0oph9OgV0oph/N0dgFtycrKMgMHDuzsMpRSqttYsmRJmTEmu63XumTQDxw4kMWLF3d2GUop1W2IyPZDvaatG6WUcjgNeqWUcjgNeqWUcrgu2aNXSiWeUChEYWEhDQ0NnV1KlxYIBMjPz8fr9ca9jAa9UqpLKCwsJC0tjYEDByIinV1Ol2SMoby8nMLCQgoKCuJeTls3SqkuoaGhgczMTA35wxARMjMzj3irR4NeKdVlaMi372h+R44K+t9+sIlPNpZ2dhlKKdWlOCroZ8zbyicbNOiVUkcnNTW1s0s4LhwV9Cl+NzWNoc4uQymluhRHBX2q30NtY6Szy1BKdXPGGO6//35GjBjByJEjeemllwDYtWsXkyZNYvTo0YwYMYJPP/2USCTCt771reZ5f/Ob33Ry9Qdz1OGVqQEv1Y3hzi5DKXWMfvz3NawtrurQ9xzeN50fXXZqXPO+9tprLF++nBUrVlBWVsbYsWOZNGkSL774IpMnT+bhhx8mEolQV1fH8uXLKSoqYvXq1QBUVFR0aN0dwWEjejc1Ddq6UUodm88++4xrr70Wt9tNTk4O5557LosWLWLs2LH85S9/4dFHH2XVqlWkpaUxaNAgtm7dyl133cXcuXNJT0/v7PIP4qwRvd9DWXWws8tQSh2jeEfex4sxps3pkyZNYt68ebz99tvccMMN3H///Xzzm99kxYoVvPvuuzz55JO8/PLLzJw58wRXfHgOG9F7qdHWjVLqGE2aNImXXnqJSCRCaWkp8+bNY9y4cWzfvp3evXtz6623csstt7B06VLKysqIRqN84xvf4L/+679YunRpZ5d/EIeN6N0a9EqpY3bllVcyf/58Ro0ahYjwq1/9itzcXP76178yffp0vF4vqampPPvssxQVFXHTTTcRjUYB+PnPf97J1R/MWUEf8FDTGMYYo2fYKaWOWE1NDWDPPp0+fTrTp08/4PUbb7yRG2+88aDluuIoviXHtW4iUUNDKNrZpSilVJfhsKB3A2j7RimlWnBW0AdsJ0qDXiml9nNW0PvthfhrGjTolVKqiaOC/qStzzFO1umIXimlWnBU0A9Y/mu+6l6qQa+UUi04Kuhx+/ET1CtYKqVUC84Keq8fPyFq9AqWSqnj7HDXrt+2bRsjRow4gdUcnqOCXjwBfBLWnbFKKdWCo86MFY+fgIS0daNUd/fOg7B7Vce+Z+5IuOgXh3z5gQceYMCAAdxxxx0APProo4gI8+bNY9++fYRCIX76059yxRVXHNGPbWho4Dvf+Q6LFy/G4/Hw2GOP8eUvf5k1a9Zw0003EQwGiUajvPrqq/Tt25err76awsJCIpEIP/zhD7nmmmuO6WNDnEEvIlOAJwA38LQx5qDfloicBzwOeIEyY8y5senbgGogAoSNMWOOuepD1enxk+IK6c1HlFJHbOrUqdx7773NQf/yyy8zd+5cvve975Genk5ZWRlnnnkml19++RFdYuXJJ58EYNWqVaxfv54LL7yQjRs38tRTT3HPPfdw3XXXEQwGiUQizJkzh759+/L2228DUFlZ2SGfrd2gFxE38CRwAVAILBKR2caYtS3m6Qn8AZhijNkhIr1bvc2XjTFlHVLx4XgCJLkaqdbWjVLd22FG3sfL6aefTklJCcXFxZSWltKrVy/69OnD9773PebNm4fL5aKoqIg9e/aQm5sb9/t+9tln3HXXXQAMGzaMAQMGsHHjRiZMmMDPfvYzCgsL+frXv87gwYMZOXIk9913Hw888ACXXnopEydO7JDPFk+Pfhyw2Riz1RgTBGYBrbddpgGvGWN2ABhjSjqkuiPl8ZMkYW3dKKWOylVXXcUrr7zCSy+9xNSpU3nhhRcoLS1lyZIlLF++nJycHBoaGo7oPQ91bftp06Yxe/ZskpKSmDx5Mh9++CFDhgxhyZIljBw5koceeoif/OQnHfGx4gr6PGBni+eFsWktDQF6icjHIrJERL7Z4jUDvBebftuhfoiI3CYii0VkcWlpabz1HyjWo9fWjVLqaEydOpVZs2bxyiuvcNVVV1FZWUnv3r3xer189NFHbN++/Yjfc9KkSbzwwgsAbNy4kR07djB06FC2bt3KoEGDuPvuu7n88stZuXIlxcXFJCcnc/3113Pfffd12FUx4+nRt9WMar2K8gBfAs4HkoD5IrLAGLMRONsYUxxr57wvIuuNMfMOekNjZgAzAMaMGdP2KrA9ngB+Cel9Y5VSR+XUU0+lurqavLw8+vTpw3XXXcdll13GmDFjGD16NMOGDTvi97zjjju4/fbbGTlyJB6Ph2eeeQa/389LL73E888/j9frJTc3l0ceeYRFixZx//3343K58Hq9/PGPf+yQzxVP0BcC/Vo8zweK25inzBhTC9SKyDxgFLDRGFMMtp0jIq9jW0EHBX2H8PjxE6ZWg14pdZRWrdp/tE9WVhbz589vc76ma9e3ZeDAgc03Cw8EAjzzzDMHzfPQQw/x0EMPHTBt8uTJTJ48+SiqPrx4WjeLgMEiUiAiPmAqMLvVPG8CE0XEIyLJwHhgnYikiEgagIikABcCqzuu/FY8AXwE9Th6pZRqod0RvTEmLCJ3Au9iD6+caYxZIyK3x15/yhizTkTmAiuBKPYQzNUiMgh4PXYokgd40Rgz93h9GNw+vCZITVCDXil1/K1atYobbrjhgGl+v5+FCxd2UkVti+s4emPMHGBOq2lPtXo+HZjeatpWbAvnxPAE8JogtcEw0ajB5dLbCSrVnXS324COHDmS5cuXn9CfeaijeA7HUZdAwOPHbUIYA3UhPfJGqe4kEAhQXl5+VEGWKIwxlJeXEwgEjmg5R10CAU8ATzQIGGoawqT6nfXxlHKy/Px8CgsLOerDqxNEIBAgPz//iJZxVhJ6fACxK1hqn16p7sTr9VJQUNDZZTiSw1o3dnNGg14ppfZzWND7AfChlypWSqkmDgv6phF9UEf0SikV46ygd9sRvV+0daOUUk2cFfQHtG70CpZKKQWOC/r9rZvaoB5Hr5RS4LigtyP6FHdEbz6ilFIxjgz6nr6I3nxEKaViHBn06Z6o3nxEKaViHBb0tkef7o1q60YppWIcFvR2RJ/m0ZuPKKVUE2cFvbsp6CN6HL1SSsU4K+hjrZsUtwa9Uko1cVjQNx1eGdagV0qpGIcFvR3RJ0uYOg16pZQCnBb0bg+IC7+ECEainV2NUkp1Cc4KegBPAB9hQhFDJKq3JFNKKQcGvR8fQQCCYR3VK6WUA4M+0Bz0jWE9O1YppZwX9G4fPmN3xDbqiF4ppRwY9J4AXqOtG6WUauLAoPfjMdq6UUqpJg4M+kBz0DeEdESvlFJxBb2ITBGRDSKyWUQePMQ854nIchFZIyKfHMmyHeqAEb0GvVJKedqbQUTcwJPABUAhsEhEZhtj1raYpyfwB2CKMWaHiPSOd9kO5/HjiVYB2rpRSimIb0Q/DthsjNlqjAkCs4ArWs0zDXjNGLMDwBhTcgTLdixPAFekEdCdsUopBfEFfR6ws8Xzwti0loYAvUTkYxFZIiLfPIJlARCR20RksYgsLi0tja/6trh9uKPaulFKqSbttm4AaWNa62sLeIAvAecDScB8EVkQ57J2ojEzgBkAY8aMOfprF3gCuDTolVKqWTxBXwj0a/E8HyhuY54yY0wtUCsi84BRcS7bsTz+5tZNY0h79EopFU/rZhEwWEQKRMQHTAVmt5rnTWCiiHhEJBkYD6yLc9mO5QkgTUGvI3qllGp/RG+MCYvIncC7gBuYaYxZIyK3x15/yhizTkTmAiuBKPC0MWY1QFvLHqfPYnl8SETPjFVKqSbxtG4wxswB5rSa9lSr59OB6fEse1x5Aki4ATA6oldKKRx5Zqy9naCPsB5Hr5RSODLo7e0EU90RHdErpRRODHq3D4BUT0R79EophRODPjaiT3Nr60YppcDBQZ/qjtCoV69USiknBr3dGZvq0R69UkqBg4M+RVs3SikFODjok91h3RmrlFI4Muhtjz7FFdbWjVJK4cigj43oNeiVUgpwYtC7bdAnuSLao1dKKZwY9J6moA/p4ZVKKYUjg9726JMkRDCiQa+UUo4N+oDoCVNKKQWODHp7rZuAhLRHr5RSODLom0b0IT3qRimlcGLQuzwgLnwE9YQppZTCiUEvAp4AfgkTjhrCukNWKZXgnBf0AG4fPhMC0CNvlFIJz5lB7wngw94gXI+8UUolOocGvb95RK87ZJVSic6hQR/AQ6x1o0GvlEpwDg16H14Ta93osfRKqQTn0KAP4Ik2Bb2O6JVSic25QW8aAR3RK6WUQ4Pejzsa2xmrR90opRJcXEEvIlNEZIOIbBaRB9t4/TwRqRSR5bHHIy1e2yYiq2LTF3dk8Yfk9uOOxkb0ehy9UirBedqbQUTcwJPABUAhsEhEZhtj1raa9VNjzKWHeJsvG2PKjq3UI+Dx447ocfRKKQXxjejHAZuNMVuNMUFgFnDF8S3rGHkCuKJ61I1SSkF8QZ8H7GzxvDA2rbUJIrJCRN4RkVNbTDfAeyKyRERuO4Za4+fx44o07YzVEb1SKrG127oBpI1pptXzpcAAY0yNiFwMvAEMjr12tjGmWER6A++LyHpjzLyDfohdCdwG0L9//3jrb5vHj0QaAA16pZSKZ0RfCPRr8TwfKG45gzGmyhhTE/t+DuAVkazY8+LY1xLgdWwr6CDGmBnGmDHGmDHZ2dlH/EEO4AkgYTui1zNjlVKJLp6gXwQMFpECEfEBU4HZLWcQkVwRkdj342LvWy4iKSKSFpueAlwIrO7ID9AmTwCJNCJEtUevlEp47bZujDFhEbkTeBdwAzONMWtE5PbY608BVwHfEZEwUA9MNcYYEckBXo+tAzzAi8aYucfps+zntXeZ8hPSo26UUgkvnh59UztmTqtpT7X4/vfA79tYbisw6hhrPHKeJADS3BHt0SulEp4zz4yNjejTPWHt0SulEp4zg75pRO8JaY9eKZXwnBn0sRG9tm6UUsqpQe+xQZ/qDmnQK6USnvODPqStG6VUYnNm0Httjz7FFSKoV69USiU4ZwZ9bESf4orocfRKqYTnzKCPjeiTXXrUjVJKOTPoYyP6ZAnqzlilVMJzdNAnufSoG6WUcmbQx46jT5KQnhmrlEp4zgz62JmxSQS1R6+USnjODHq3B1weAtqjV0ophwY9gCdJL1OslFI4Oei9AfwE9YQppVTCc27QewL4TZBI1BDWsFdKJTBHB70Pe99Y7dMrpRKZc4PeG8AbDQIa9EqpxObcoPck4TNNI3o9xFIplbicG/TeAB5jR/R60pRSKpE5N+g9ATxR7dErpVRiBL0eS6+USmDODXpvEu6I9uiVUsq5Qe8J4I42ANq6UUolNucGfYsRve6MVUolMucGvSeAhO2IvkFvEK6USmCODnpXNIgQpbox3NnVKKVUp4kr6EVkiohsEJHNIvJgG6+fJyKVIrI89ngk3mWPm9jNR/yE2FcbPGE/VimluhpPezOIiBt4ErgAKAQWichsY8zaVrN+aoy59CiX7Xixm4+ku8PsrdOgV0olrnhG9OOAzcaYrcaYIDALuCLO9z+WZY9NbESfk2x0RK+USmjxBH0esLPF88LYtNYmiMgKEXlHRE49wmURkdtEZLGILC4tLY2jrHbERvS9kwx7a0PH/n5KKdVNxRP00sY00+r5UmCAMWYU8DvgjSNY1k40ZoYxZowxZkx2dnYcZbUjNqLPDkTZW9t47O+nlFLdVDxBXwj0a/E8HyhuOYMxpsoYUxP7fg7gFZGseJY9bjw26DP9hn11OqJXSiWueIJ+ETBYRApExAdMBWa3nEFEckVEYt+Pi71veTzLHjfNQR9lr/bolVIJrN2jbowxYRG5E3gXcAMzjTFrROT22OtPAVcB3xGRMFAPTDXGGKDNZY/TZzmQ1/boe/oiVNaHCEeieNzOPW1AKaUOpd2gh+Z2zJxW055q8f3vgd/Hu+wJERvR9/Las2Ir6kNkpfpPeBlKKdXZnDvEjY3oe8SCXg+xVEolKucGfWxEn+axlz/QPr1SKlE5PuhTXRr0SqnE5tygjx1Hn+KOBb1eBkEplaCcG/SxM2OTxAa89uiVUonKuUHv9oDLgzfaSIrPrZdBUEolLOcGPdhRfbiBXik+9mnrRimVoJwd9N4AhOrJTPHpzlilVMJydtB7AjqiV0olPOcHfaiejGQf5TUa9EqpxOTsoPcGINyoI3qlVEJzdtB7kiBcT0aKj7pghIZQpLMrUkqpE87ZQe8NQKiBXsk+AB3VK6USkrOD3hNoHtGDXgZBKZWYnB/0oYbmoN+nJ00ppRKQs4Pe29Sj9wJ6vRulVGJydtB7YkfdxHr0e2v0JuFKqcTj7KD3JkGogR5JXkRgr94kXCmVgJwd9LGdsR63ix5JXr2CpVIqITk/6CNBiEbISPFpj14plZCcHfSxm48QbiAnLcDW0trOrUcppTqBs4M+dvMRQg18dXgO63ZVsbmkpnNrUkqpE8zZQd88oq/n0tP6IAKzVxR3bk1KKXWCOTvom0b04UZy0gNMGJTJ7OVFGGM6ty6llDqBnB30TSP6UD0AV4zuy7byOlYWVnZiUUopdWI5O+g9+3fGAkw5tQ8+t4s3l9v2TUlVA+FItLOqU0qpEyKuoBeRKSKyQUQ2i8iDh5lvrIhEROSqFtO2icgqEVkuIos7oui4eQ4c0fdI9nLe0GxeWbKT83/9MeP++wPunrVMWzlKKUdrN+hFxA08CVwEDAeuFZHhh5jvl8C7bbzNl40xo40xY46x3iPjberRNzRPuv7MAQDk9UrmG2fkM2fVbn7/4eYTWpZSSp1InjjmGQdsNsZsBRCRWcAVwNpW890FvAqM7dAKj0WrET3ApCHZrHx0MgDGGKLG8Ov3NzKsTzoXDM/pjCqVUuq4iqd1kwfsbPG8MDatmYjkAVcCT7WxvAHeE5ElInLboX6IiNwmIotFZHFpaWkcZcUhJdt+rd59qJ/Jz78+ktPye3DHC0v4n3c36F2olFKOE0/QSxvTWje1HwceMMa0lZJnG2POwLZ+visik9r6IcaYGcaYMcaYMdnZ2XGUFYe0XAj0hJI1h5wl4HXz15vGcdmovvz+o81Mfnwezy3YTmX9wRdAq6wPEY0evp8fikRZv7tK+/5KqS4jnqAvBPq1eJ4PtD7raAwwS0S2AVcBfxCRrwEYY4pjX0uA17GtoBNDBHJOhZJ1h52tV4qPx64ezYvfHk+yz8MP31jNuJ/9g3tmLeOzTWXsrmzgkTdXM+an7/Nvf5rPjvI6olHDiwt3cOFvPuHjDSWAbQXd97cVTHn8U27562J2lNediE+plFKHJe2NPEXEA2wEzgeKgEXANGNMm8NkEXkGeMsY84qIpAAuY0x17Pv3gZ8YY+Ye7meOGTPGLF7cQQfovP0fsPJleHCHDf52GGNYXVTF35bs5I1lRVQ1hAHwuISLR/bhow0lRKOGgVkprCmuIsXnJmIMz98yniXb9/Hzd9ZzwfAc/rm5jHDUMHZgBv0ykslO8+N1CelJXq76Uj4p/nh2jyilVHxEZMmhDnhpN22MMWERuRN7NI0bmGmMWSMit8deb6sv3yQHeF1swHqAF9sL+Q7X+xRorIKqIuiR3+7sIsLI/B6MzO/Bf158Cu+t3cO6XVVcM6YfA7NSKNxXx31/W8Hmklp+c80ozjk5m2v+NJ9v/WURtcEwl4zsw++nnc6eqkZ+9+Em1hRX8d6a3ZS3uETy26t28cxNY0n2eSitbmR1USXnDc1G2lgRGWPanK6UUvFqd0TfGTp0RL/9n/CXi+C6V2DwBR3ylvZoHXC7bAAX7qvj6qfm0yvFx99un0Cy7+D1pzGGSNQwZ/Vu7p21jLEDM5g0JJs/fLSZ2mCEa8b046dXjsDrdhGNGj7fUsbzC7bz0fpSBuekctZJmVx5ej7D+6YDsHNvHT/++xqG9+3BzWcPpGeyD2MMhfvqWVVUyfpdVYwryOScwVkd8plbK6luID3gJeB1H5f3P94q60Ok+j3N/4ZKdXeHG9E7P+jr98EvB8JXfwzn3Nsx79mGumAYt0vwe9oPvjeXF/G9l5YTNXDB8BwGZibz50+/YNKQbAZlpTB39W52V9mbmk8ZkcvW0hqWbq8gYgy3nFPAuIEZ3PfKChpCERpCUVJ8bkbk9WDdrqrmVhPYTtWPLz+Vb04Y2GGf0xjD8wt38NO31jIirwezbjsTr7vzT7B+a2Uxm0tqcIlQkJUSu4jdwSFeXtPI7z7czAsLt3PN2H789GsjO6Fa59pbG6Q+FCGvZ1Jnl5Jwjql10+0l9YK0vu3ukD1WbY3iD+WK0Xlkp/rxuF2MK8gA4KTsVB5+YzULt5Zz7pBsHjptGFNG5DavOCrrQvxi7npmzNvKjHlbGZKTyowbxtAQjvDHj7ewrbyOS07ry4i8dEb07cGAzGTu+9tKHnlzDct3VpCTHqA+GKFXso+8Xkm4XbCtrI7iinoiUXPAYVS9kn1MG9+Pk3unYYxh+c4KNu6pprYxwj+3lPGPdSWMyEu3+yTmrOeRy4bTGI7w9spdrCmuYmtpDSXVjdQFIwS8bp6YOpohOWkH/R7KaxrJSPHF1Zr6dFMpD7++mru+cjL/NqbfAa/9ed5WfjbnwH/ff24p4ydXjDhgJfTJxlK++8JS6oJhTu6dyosLd3DDmQMZmntwbWBXag2hKEm+g1feJVUN/PbDTYddvqsIRaK8ubyYSYOz6J0eOG4/py4Y5ht//CcVdUE++I/zyEjxHbefdbwZY9hbGyQz1X/CfubOvXWs3VXF5FNzO/y9nT+iB3ju61BbCrd/2nHveRyUVjeS7HMfdkftwq3lfL6lnH+fNKjdHbqRqOEnf1/Dswu243W58HtdVLca8eekBfB6bNBK7EjaPVUNNIajnHNyFtvKaynct/+EM7/Hxf2Th3Lz2QX85K21PPPPbdw6sYC5a3azc289Aa+LQVmp9OkRINnvYf6WMtICXt6882zSA97m91m6Yx9XPzWf2yYN4gdThh1Qd2VdiOWFFQQ8LgbnpPH2ql08OnsNbpcQjkT5w3VnMGVEHwBeXrSTH7y6kktG9uHxqaMxBp74YCNPfrSFiYOzeOzq0WSn+Vmxs4Jr/7yAAZkp/O7a0WSm+Dl3+kecMaAXz9y0/0AwYwwzP99mtxD21FDdGGZkXg8mDcniitF5DMlJo6iinuv+vIBt5XX0SPIy81tj+NKAjOb3WLerindW72bKqbnNrbbjZd2uKsIRw8j8Hm2+vnNvHXfPWsayHRWML8jg/249E5dLiEQNy3bs4/T+vQ7ZvmraeqsPhrlt0knt1vL/3ljFCwt34Bbh8tF9eezq0Yed3xjDY+9vZG9tkEcuG37YreGaxjCpJ/AAhl/OXc8fP97Cr646jatbDSyOh21ltUz78wKCkSgf3//lo/qsid26AXj3YfjXn+HhXeDqnj3lYxGJmuY/5oZQhOKKeqIG+mUktfnHVV7TyHMLtvPKkkJOyk7lslF9OXNQBml+L8l+d/MoORiOMnXGfJbuqOCUPuk8eNEwJp6chatFcPzri71M+/MCzhvamxk3fKk5ZK548jNWF1UhAi98ezxnnZTFp5tK+elb69iwp/qgmr4yrDe/+MZIbn9uCauLqrj+zAFs2FPF/C3lnH1yFv9741h8nv2j95cX7eQ/X1+F1+1i2vj+vLGsiCSfm9fuOIveaXZU27Ql8Nwt45g4OJto1DSvvEbl9+C0/J70TPYyf0s5y3ZWEIkazj45k21ldVTVh/jvr4/k1+9tYHdVA1PH9qeyPsS6XVWs323rz0n389ZdE8lOO/yosKohxPdfWsEpfdK4/dyT2l2BB8NRPt9cxv9+9gWfbS4D4NaJBfxgyjDW76rmtx9uYufeOvxeN1tKahCBi0bk8vLiQn525QimjevPf76+iv/7107OH9ab3157OgGvmxcWbufzzWWcf0oO55ycxY9mr+H9tXsA+O21p3P5qL6HrOmjDSXc9JdF3DZpED63i99/tJnnbxl/wD6iXZX1fFFWy9iBGXhcwo//bn/XAGefnMmfbhjDht1VzPx8G1kpPm48y+57euz9Dby4cAffnjiIhy4aFtcWYH0wQmV9iIaQ3YrtkWwHGet2VfHcgu1kpfr5xhl59E4L8N7a3czbWMaFp+Zw4fAcXltaxH/8bQUZKT4q6oL8ftoZXDyyDyXVDUSjkNuj/a2ikuoGps/dwLxNpQhCks/NPecP5mun5x0075bSGhvy4SgvfPvMox4caNAvewHevAPuXAJZJ3fc+yr21QZZUVjBpMHZBwR8S898/gWP/n0t08b355FLh/O3JYX88I3V/PIbI/nTJ1upC0a4YcIAfv3eBgqyUrjy9DzO6N+LYCTK5pIa/B4X08YPwO0SKutCTHt6Aet3VzMsN40JgzL5/oVD2mydbS2t4YkPNjF7RTE9k7y8+p2zGJSd2vx6YzjC+b/+hIZQlHNOzqS6IcwH60u4dWIB/3nxKQcEyt7aILMW7eC5+dtpCEV49ubxjMzvQWl1I//+3GJWF1eRneonv1cSl5zWh5N7p3LzM4s4Lb8nz948jteWFvHXf24jYgwpPjfnDM7i7vMH4xLhpr8sYv7WciJRQ+80Pw9dPIyvjc5DRDDGMHtFMauLKglHDXuqGvh0YxnVjWF6p/m56ewCiivqeW7BdvJ6JlFUUU/PZC9jB2bQGI7SI8nLDyYPJb9XEtc9vZBVhZVcclofZi3ayZeHZjNvUxlDc9Lwelys2FlBr2Qv++rsyYIel/DgRcN4Z/VuNuyuZs7dE8nrlcT8LeUMyEymX0YyYLcAL/3dZ2Sm+HjzzrMxBi564lPC0Sg3ThhIepKXjzeU8O6aPUSihuw0P8P7pPPJxlJuOaeA4X3S+cGrK0kPeNhXF6Jnspe6xgjBSJRkn5vGcJTT8nuwbEcFD0wZxnfOO/zWxaY91Xzjj/88YH/VsNw0MlJ8/HNLOQGvi8ZwFGMgyeumPhQh4HXREIoyriCD5TsqGDOwF3+8/kvc8swiVhRWkN8rmS/KanG7hLu+cjLf/fLJeN0uiivqWbJ9HysLK/iirI6MFC/JPg+vLCmkMRzhohF9CHhdrN9dzcrCSr511kCuG9+fJdv3sWxHBet324FBqt/DC7eOZ1ju0W8BatAXL4MZ58HVz8LwKzrufVVcjDH8/B27f+Gk7BTKaoIM75POi7eOZ3VRFVf+4XPCUcOUU3P59dWj2h3RHq533patpTV43a7mYGppyfa9PPHBZjbtqaasppF7vzqEO8476ZCjxnAkGgugA2ts6zDYN5YVce9Ly+mR5KWyPsSofj3p2yPAvrogC7bu5dS+6QzMSuHtlbv4n38bRUFWCj95ay0rdlbw1VNyeGDKUH717gbeX7uHgNeF1+0iPeBl4uAszj8lh0lDspq3yP6+opjH/7GRy0b15ZZzCkhr0SZrsqO8jgsftyu2GycM4NHLT+WTjaXc+eIy/B4Xj1w2nMtH9WXpjn18tL6U80/pzen9e1G4r46Ln/iUrDQ/jaEoRRX1ZKT4eO6WcQzMTOHqP81nW1ktr95xVnNQLdxazrefXdzcKuyZ7OWasf04La8nry0t5KMNJXzrrAJ+eKldoX60voTH3t/IFaP7Mm18f2obIzy/YDuF++q5/dxBnJSdyr0vLWf2imKmnJrLnuoGSqoaGZGXzriCTK4Y3ZesVD/GGL4581+s2FnBAxcNI+BxU1xRz7+27WXn3jq+fkY+N04YSF0ozGtLi9hVWc8lI/vypQG9eHHhdh57fyOZqX5ev+Mseib7qGoI8dCrq2gIRRg/KIO1xVW8sbyYoTlpBCNRviiz96H2eVwMzEymsj7E3togEwdn88NLh1OQlQLY/SS/fGc9T3/2RfO/R69kL8P7pjMsN53rzxzQPO/R0qAP1sF/94XzHrQP1Sk+3VTK/X9bSVlNI3Pumdi8g/atlcWUVTfyzQkDD7lVcCK0bHF1lOnvrueDdSV874IhXDg8p3ll8P7aPfzglRXsqwtx9/mD+f4FQ5prmPnZF0x/bwPBcBSvW3hgyjBuPrugQ343c1btYuOeau7+yuDm9yupbiDJ625z5dBk7upd3PniMs4clMllo/rwxD82UdMYZlhuOkt27ON/bxzDeUN7H7CMMYaqhjD7aoPk9ggccChuXTB8RAcwgA3L77+8ggVbyzk5O5XMVB8rCivYubee/hnJzLrtTNYUV3Hrs4v50WXDuensgiN6f7D7AoDD9sjnrNrF4//YSH6vZM46KZPxBZkMzU1rbh0e7tyXTzeVUlxRz5iBGQzKSunQc2Q06AF+ezpkDYFpL3Xs+6ojUtUQoqSqkZN7p7Y/s8OVVDWwaNs+Lh6Ze9Af/MY91cz87AuuP3MAI/La3tF6ogXD0eYwK9xXx3VPL2R7eR0/u3IE140f0Gl1Ldm+l2/NXESv2FE+Po+Ld+6Z2CUO+z2RNOgB3v8RfP443Pwu9D+zY99bqQRUXtPIul3Vx+2kvCOxfGcFNzy9kOrGMM/ePI5JQzrowojdiAY9QGMNPDkeAunw7/PAfejNVKVU97NuVxWriipPyOGQXdHhgj5xtm38qXDxdChZC/Of7OxqlFId7JQ+6Qkb8u1JnKAHGHYxDL0EPv4FfNG1T55SSqmOklhBD3DpY9CzPzz/dVj9amdXo5RSx13iBX1aLtw8F/K+BK/cDLPvgt2rOrsqpZQ6bhIv6AGSM+CGN2DMLfamJE+dAzMvgm2fdXZlSinV4RIz6AG8AdvG+f46mPzfsO8LeOYSewG01a9CfUVnV6iUUh3C+Zcpbk9yBkz4Loy5Gf41Az7/LWz5AMQN/cbDkAuhYBL408Htg/Q8cOuvTSnVfSTOcfTxikagcDFseg82vXtw/96fDgMnQt4Z9lr3Sb3s9z0HxHVPWqWUOh70hKljUVVsgz/cCKE6KFoCWz+Giu0HzpeeDxkF9kQsl9d+dXuh/wQY+W92y0EppY4TDfrjIVhnbzpeswd2/svuyK3eDdEwREMQCUOwxq4Q3D572YW0vpCWAzkjoO8Z0GuAnqGrlOoQiX0rwePFl2wfabnQZxSMu7Xt+XavguUvws6F9kblNbshEtz/utsPqb3t5ZNHTYWsoTb8tQ2klOogOqI/0SJhKF1vr5FfvduO+ss2wqb37ZYAAAKBHvbErh797OUbvEmQmgOZJ0PGSZB5EiT1tLM3/RvqykGphKUj+q7E7YHcEfbRUm05rH/L3ts23Ah15VC5E/ZtsyuDUB3UlkHL23gnZdjnjdX2KKEeeXbF0KMf9OwH/jRAwOODtD6Q3te2j1KywUTtIaXVu6Dv6bF5j0Go3u6/yBpit1CUUl2GBn1XkZIJX7rx8POEGmzw790C5Zth7xf2Hrj+dLtvoLLQPrZ8YLcWOMTWmssDyP4tCLfPHknUs5/d9xCqg2CtfTR9L7GtjEBPuyUR6Ame2L0zqwph84cQqrXv22889BsLvlS7TO5IuzLxHdsddNoUjdradGtGqUPSoO9OvAHoPcw+2hMO2pDG2BVE9S77qCq2D7Cj7+RM+OIT2Bg7lNSXDN6U2NdkO/r3JdstgIZKeyJZ5U77NRJbUQTS7f6Fk74Ce9bA+r/DwhkQadxfj7hiKwe/3fqINNrlswZD3hhI72MvJR2ssVsowVr7c9Pz7HLhevs5fMn2eeVO2PIhFC+3n1HcdgXm8th6eg+HnFPtPpSkDNv6MlG7A33nIihaDL0K4LSrYcgU+75gVxyVO6BknX1UFdlDaFN62/dweexnSOplV3hNh9j603Vlo7os7dGr4ycShvq9dn9E0RLbjgo32sD1+AGxYVq8zAY5YrcC/Kl29N9YDTUltLllIm7oN84evur27T/aKRqxLa49a+y+kOb9Hi0kZdhrHe1ZbVd+YFdoyZlQsTO2ZRIT6GHrMNHDf9aUbHtiXd/TbRur6cxqt9fW5/bZFXX2KfvPwQjWQl3Z/q2zSNAempucYXfwZ5xkP1dduf2dtNdeMwbq9tqVYFqufaiEccw9ehGZAjwBuIGnjTG/OMR8Y4EFwDXGmFeOZFnlQG6P7dcPmWwfhxIJQ7jBbkG4Wl2VIxKyo3xvsg3LUJ0NUX+aHbkfTjQKjZU2/MKNdqvCG9h/cls0Ats+hR0L7FZOXTkMOg96n2IDufcwG/TRSOw9GmzwhhtsDfX77KOu3G4NfTFv/xVRvSn250WCB27ZNBFX+ysPt+/AI7R6FdgtlZ797I75ih12JVlZaFdooYYDf1b+WNtGq9lj5zHGfv5oxK5AG6shY5Dd8vH47QqyscquZL3J9rOG6u3KKrU3pObar2m5tm0XDbd6ROxXt8+uNJMz7ffisu29PWuhoQL6jLYrO0+S/Xxur21BHkqo3v77pGTZf4+jEY3s/z/WesurZD0se862L4dMduSWWbsjehFxAxuBC4BCYBFwrTFmbRvzvQ80ADONMa/Eu2xrOqJX3ZIxNvh9qXYHeMvp0bAN1j2roWipHc37U+3IPuMkezSVx2/nq94Fu1ZC2Qb7XskZULcP9qyC0g1QWQTBaht6fUbbsPb4Y5fo6GvbXWUbYN1bdssmva/dQS9iw07ckJoNvjS7NbFnjf25qb3tz2vaL+P22kCPhOzKoq2to47g9tk2Ys/+dgVUWWjr8Sbbn9m01QV2ayy1t/0MvmTIHxfbkhptV37hRruPasM7diupqtAe6BCstst7U+yKKnso5J5mX1/+4v6Vbs5IG/aRRjsA8fhty86bZFdMgXTokW9XeqE6u8UqbltTcpZdkbp99v9BVbGdp6m1Fw3F9oHV263GUH1sP1i9nS9UZ7foJtxxVL/GYzphSkQmAI8aYybHnj8EYIz5eav57gVCwFjgrVjQx7Vsaxr0SrWjscaOvNsbfRoT3zxw+PmaVmLVu2PngoTtKNzliY3IPfufhxvt1kFd+f7Rflqu3Rrxp0PxUrtvpamFV7/PttkqdtrA7NkvtvXWYGvqOcCGa10Z7N1qt65M1C5XuHj/VowvdsP5YI3dj9P7FLuSS+lt96d4/FBTave7lK63hzW7PDD2Vjj7HrvP57Pf2OlN+2PCjW1vkR0vyZnwg61Hteixtm7ygJ0tnhcC41v9gDzgSuAr2KCPe9kW73EbcBtA//794yhLqQTmT41vvnjaEPHOk5xhHznD4/vZh9JeK+9IhBrsjvWSdXbrJBKEYZdAwbntn3UerLXzJ/Wyz0dfax+tV45NbZ9QvV25VBbaFZ4vxf4+mlphdeV2vkjQrmjS+9h56ivsgQzNWwcpsa+xky5bTzsO4gn6tv4XtN4MeBx4wBgTkQP/08SzrJ1ozAxgBtgRfRx1KaUSnTcAA8+xjyPlSwHaOOS39YrP5bbz+lLsfoKswUdVameKJ+gLgZZ33M0HilvNMwaYFQv5LOBiEQnHuaxSSqnjKJ6gXwQMFpECoAiYCkxrOYMxpqDpexF5Btujf0NEPO0tq5RS6vhqN+iNMWERuRN4F3uI5ExjzBoRuT32+lNHumzHlK6UUioeesKUUko5wOGOuknce8YqpVSC0KBXSimH06BXSimH06BXSimH65I7Y0WkFNje7oxtywLKOrCcE03r71xaf+fpzrVD59c/wBiT3dYLXTLoj4WILD7UnufuQOvvXFp/5+nOtUPXrl9bN0op5XAa9Eop5XBODPoZnV3AMdL6O5fW33m6c+3Qhet3XI9eKaXUgZw4oldKKdWCBr1SSjmcY4JeRKaIyAYR2SwiD3Z2Pe0RkX4i8pGIrBORNSJyT2x6hoi8LyKbYl97dXathyMibhFZJiJvxZ53m/pFpKeIvCIi62P/DhO6Wf3fi/3fWS0i/yciga5cv4jMFJESEVndYtoh6xWRh2J/zxtEpINuSXX0DlH/9Nj/n5Ui8rqI9GzxWpep3xFBH7sJ+ZPARcBw4FoROcb7nR13YeA/jDGnAGcC343V/CDwgTFmMPBB7HlXdg+wrsXz7lT/E8BcY8wwYBT2c3SL+mO377wbGGOMGYG9DPhUunb9zwBTWk1rs97Y38JU4NTYMn+I/Z13pmc4uP73gRHGmNOAjcBD0PXqd0TQA+OAzcaYrcaYIDALuKKTazosY8wuY8zS2PfV2JDJw9b919hsfwW+1ikFxkFE8oFLgKdbTO4W9YtIOjAJ+F8AY0zQGFNBN6k/xgMkxW7wk4y9e1uXrd8YMw/Y22ryoeq9AphljGk0xnwBbMb+nXeatuo3xrxnjAnHni7A3kUPulj9Tgn6tm5CntdJtRwxERkInA4sBHKMMbvArgyA3p1YWnseB34ARFtM6y71DwJKgb/EWk9Pi0gK3aR+Y0wR8D/ADmAXUGmMeY9uUn8Lh6q3O/5N3wy8E/u+S9XvlKCP+ybkXY2IpAKvAvcaY6o6u554icilQIkxZkln13KUPMAZwB+NMacDtXStNsdhxXrZVwAFQF8gRUSu79yqOlS3+psWkYex7dgXmia1MVun1e+UoO+WNyEXES825F8wxrwWm7xHRPrEXu8DlHRWfe04G7hcRLZhW2VfEZHn6T71FwKFxpiFseevYIO/u9T/VeALY0ypMSYEvAacRfepv8mh6u02f9MiciNwKXCd2X9iUpeq3ylB33wDcxHxYXeCzO7kmg5LRATbH15njHmsxUuzgRtj398IvHmia4uHMeYhY0y+MWYg9vf9oTHmerpP/buBnSIyNDbpfGAt3aR+bMvmTBFJjv1fOh+7n6e71N/kUPXOBqaKiF9ECoDBwL86ob7DEpEpwAPA5caYuhYvda36jTGOeAAXY/d6bwEe7ux64qj3HOym3EpgeexxMZCJPfpgU+xrRmfXGsdnOQ94K/Z9t6kfGA0sjv0bvAH06mb1/xhYD6wGngP8Xbl+4P+w+xNC2BHvLYerF3g49ve8Abioi9a/GduLb/obfqor1q+XQFBKKYdzSutGKaXUIWjQK6WUw2nQK6WUw2nQK6WUw2nQK6WUw2nQK6WUw2nQK6WUw/1/sUuEtuG6op4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "\n",
    "Tensorboard is a visualisation tool from Google, desgined to work in conjuction with Tensorflow to visualise various aspects of your model.\n",
    "\n",
    "To run it on browser:-\n",
    "\n",
    "1. Open Command prompt and change the directory to the file till logs\\fit\n",
    "2. Type 'tensorflow --logdir logs\\fit'\n",
    "3. http://localhost:6006/ (Open it on browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Tensorboard\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TensorBoard in module keras.callbacks:\n",
      "\n",
      "class TensorBoard(Callback, keras.utils.version_utils.TensorBoardVersionSelector)\n",
      " |  TensorBoard(*args, **kwargs)\n",
      " |  \n",
      " |  Enable visualizations for TensorBoard.\n",
      " |  \n",
      " |  TensorBoard is a visualization tool provided with TensorFlow.\n",
      " |  \n",
      " |  This callback logs events for TensorBoard, including:\n",
      " |  \n",
      " |  * Metrics summary plots\n",
      " |  * Training graph visualization\n",
      " |  * Activation histograms\n",
      " |  * Sampled profiling\n",
      " |  \n",
      " |  When used in `Model.evaluate`, in addition to epoch summaries, there will be\n",
      " |  a summary that records evaluation metrics vs `Model.optimizer.iterations`\n",
      " |  written. The metric names will be prepended with `evaluation`, with\n",
      " |  `Model.optimizer.iterations` being the step in the visualized TensorBoard.\n",
      " |  \n",
      " |  If you have installed TensorFlow with pip, you should be able\n",
      " |  to launch TensorBoard from the command line:\n",
      " |  \n",
      " |  ```\n",
      " |  tensorboard --logdir=path_to_your_logs\n",
      " |  ```\n",
      " |  \n",
      " |  You can find more information about TensorBoard\n",
      " |  [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
      " |  \n",
      " |  Args:\n",
      " |      log_dir: the path of the directory where to save the log files to be\n",
      " |        parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs')\n",
      " |        This directory should not be reused by any other callbacks.\n",
      " |      histogram_freq: frequency (in epochs) at which to compute activation and\n",
      " |        weight histograms for the layers of the model. If set to 0, histograms\n",
      " |        won't be computed. Validation data (or split) must be specified for\n",
      " |        histogram visualizations.\n",
      " |      write_graph: whether to visualize the graph in TensorBoard. The log file\n",
      " |        can become quite large when write_graph is set to True.\n",
      " |      write_images: whether to write model weights to visualize as image in\n",
      " |        TensorBoard.\n",
      " |      write_steps_per_second: whether to log the training steps per second into\n",
      " |        Tensorboard. This supports both epoch and batch frequency logging.\n",
      " |      update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n",
      " |        writes the losses and metrics to TensorBoard after each batch. The same\n",
      " |        applies for `'epoch'`. If using an integer, let's say `1000`, the\n",
      " |        callback will write the metrics and losses to TensorBoard every 1000\n",
      " |        batches. Note that writing too frequently to TensorBoard can slow down\n",
      " |        your training.\n",
      " |      profile_batch: Profile the batch(es) to sample compute characteristics.\n",
      " |        profile_batch must be a non-negative integer or a tuple of integers.\n",
      " |        A pair of positive integers signify a range of batches to profile.\n",
      " |        By default, it will profile the second batch. Set profile_batch=0\n",
      " |        to disable profiling.\n",
      " |      embeddings_freq: frequency (in epochs) at which embedding layers will be\n",
      " |        visualized. If set to 0, embeddings won't be visualized.\n",
      " |      embeddings_metadata: a dictionary which maps layer name to a file name in\n",
      " |        which metadata for this embedding layer is saved. See the\n",
      " |        [details](\n",
      " |          https://www.tensorflow.org/how_tos/embedding_viz/#metadata_optional)\n",
      " |        about metadata files format. In case if the same metadata file is\n",
      " |        used for all embedding layers, string can be passed.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  Basic usage:\n",
      " |  \n",
      " |  ```python\n",
      " |  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
      " |  model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n",
      " |  # Then run the tensorboard command to view the visualizations.\n",
      " |  ```\n",
      " |  \n",
      " |  Custom batch-level summaries in a subclassed Model:\n",
      " |  \n",
      " |  ```python\n",
      " |  class MyModel(tf.keras.Model):\n",
      " |  \n",
      " |    def build(self, _):\n",
      " |      self.dense = tf.keras.layers.Dense(10)\n",
      " |  \n",
      " |    def call(self, x):\n",
      " |      outputs = self.dense(x)\n",
      " |      tf.summary.histogram('outputs', outputs)\n",
      " |      return outputs\n",
      " |  \n",
      " |  model = MyModel()\n",
      " |  model.compile('sgd', 'mse')\n",
      " |  \n",
      " |  # Make sure to set `update_freq=N` to log a batch-level summary every N batches.\n",
      " |  # In addition to any `tf.summary` contained in `Model.call`, metrics added in\n",
      " |  # `Model.compile` will be logged every N batches.\n",
      " |  tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
      " |  model.fit(x_train, y_train, callbacks=[tb_callback])\n",
      " |  ```\n",
      " |  \n",
      " |  Custom batch-level summaries in a Functional API Model:\n",
      " |  \n",
      " |  ```python\n",
      " |  def my_summary(x):\n",
      " |    tf.summary.histogram('x', x)\n",
      " |    return x\n",
      " |  \n",
      " |  inputs = tf.keras.Input(10)\n",
      " |  x = tf.keras.layers.Dense(10)(inputs)\n",
      " |  outputs = tf.keras.layers.Lambda(my_summary)(x)\n",
      " |  model = tf.keras.Model(inputs, outputs)\n",
      " |  model.compile('sgd', 'mse')\n",
      " |  \n",
      " |  # Make sure to set `update_freq=N` to log a batch-level summary every N batches.\n",
      " |  # In addition to any `tf.summary` contained in `Model.call`, metrics added in\n",
      " |  # `Model.compile` will be logged every N batches.\n",
      " |  tb_callback = tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
      " |  model.fit(x_train, y_train, callbacks=[tb_callback])\n",
      " |  ```\n",
      " |  \n",
      " |  Profiling:\n",
      " |  \n",
      " |  ```python\n",
      " |  # Profile a single batch, e.g. the 5th batch.\n",
      " |  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
      " |      log_dir='./logs', profile_batch=5)\n",
      " |  model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n",
      " |  \n",
      " |  # Profile a range of batches, e.g. from 10 to 20.\n",
      " |  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
      " |      log_dir='./logs', profile_batch=(10,20))\n",
      " |  model.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TensorBoard\n",
      " |      Callback\n",
      " |      keras.utils.version_utils.TensorBoardVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, log_dir='logs', histogram_freq=0, write_graph=True, write_images=False, write_steps_per_second=False, update_freq='epoch', profile_batch=2, embeddings_freq=0, embeddings_metadata=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          epoch: Integer, index of epoch.\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Runs metrics and histogram summaries at epoch end.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to\n",
      " |            `on_test_batch_end()` is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.train_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently the output of the last call to `on_epoch_end()`\n",
      " |            is passed to this argument for this method but that may change in\n",
      " |            the future.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |      Sets Keras model and writes graph if specified.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.predict_step`,\n",
      " |            it typically returns a dict with a key 'outputs' containing\n",
      " |            the model's outputs.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs: Dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict, contains the return value of `model.test_step`. Typically,\n",
      " |            the values of the `Model`'s metrics are returned.  Example:\n",
      " |            `{'loss': 0.2, 'accuracy': 0.7}`.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Note that if the `steps_per_execution` argument to `compile` in\n",
      " |      `tf.keras.Model` is set to `N`, this method will only be called every `N`\n",
      " |      batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch: Integer, index of batch within the current epoch.\n",
      " |          logs: Dict. Aggregated metric results up until this batch.\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from keras.utils.version_utils.TensorBoardVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Pulkit\\\\Downloads\\\\Customer-Exit-Prediction-From-Bank'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding log directory\n",
    "log_directory = 'logs\\\\fit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-05-27--2338'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y-%m-%d--%H%M')\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding timestamp to log directory to get unique directory folder name\n",
    "log_directory = log_directory + '\\\\' + timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logs\\\\fit\\\\2021-05-27--2338'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising Tensorboard \n",
    "board = TensorBoard(log_dir = log_directory,histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "#Adding the input layer and the first hidden layer\n",
    "\n",
    "#input_dim is the total features to predict the label\n",
    "#units are the neurons in the first hidden layer\n",
    "# kernel_initializer is a fancy term for which statistical distribution or function to use for initialising the weights. (0 to 1)\n",
    "#In case of statistical distribution, the library will generate numbers from that statistical distribution and use as starting weights.\n",
    "model.add(Dense(activation = 'relu', input_dim=11 , units = 6, kernel_initializer = 'uniform'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adding second hidden layer\n",
    "model.add(Dense(activation = 'relu', units=6 , kernel_initializer='uniform'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Adding the output layer\n",
    "model.add(Dense(activation='sigmoid', units=1, kernel_initializer='uniform'))\n",
    "\n",
    "#Compiling the ANN\n",
    "#(binary_crossentropy used when just binary label is there to predict)(just one output)\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "63/63 [==============================] - 3s 30ms/step - loss: 0.6869 - accuracy: 0.7708 - val_loss: 0.6584 - val_accuracy: 0.8035\n",
      "Epoch 2/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.6388 - accuracy: 0.8030 - val_loss: 0.5471 - val_accuracy: 0.8035\n",
      "Epoch 3/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7922 - val_loss: 0.4463 - val_accuracy: 0.8035\n",
      "Epoch 4/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.7926 - val_loss: 0.4268 - val_accuracy: 0.8035\n",
      "Epoch 5/300\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.4576 - accuracy: 0.7844 - val_loss: 0.4218 - val_accuracy: 0.8035\n",
      "Epoch 6/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7997 - val_loss: 0.4204 - val_accuracy: 0.8035\n",
      "Epoch 7/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.7971 - val_loss: 0.4182 - val_accuracy: 0.8035\n",
      "Epoch 8/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4431 - accuracy: 0.7904 - val_loss: 0.4168 - val_accuracy: 0.8035\n",
      "Epoch 9/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.7882 - val_loss: 0.4158 - val_accuracy: 0.8035\n",
      "Epoch 10/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.7946 - val_loss: 0.4153 - val_accuracy: 0.8035\n",
      "Epoch 11/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4497 - accuracy: 0.7908 - val_loss: 0.4140 - val_accuracy: 0.8035\n",
      "Epoch 12/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4466 - accuracy: 0.7922 - val_loss: 0.4131 - val_accuracy: 0.8035\n",
      "Epoch 13/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4407 - accuracy: 0.7931 - val_loss: 0.4125 - val_accuracy: 0.8035\n",
      "Epoch 14/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4380 - accuracy: 0.7945 - val_loss: 0.4122 - val_accuracy: 0.8035\n",
      "Epoch 15/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4372 - accuracy: 0.7932 - val_loss: 0.4119 - val_accuracy: 0.8035\n",
      "Epoch 16/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4369 - accuracy: 0.7948 - val_loss: 0.4123 - val_accuracy: 0.8035\n",
      "Epoch 17/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4422 - accuracy: 0.7888 - val_loss: 0.4112 - val_accuracy: 0.8035\n",
      "Epoch 18/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4394 - accuracy: 0.7928 - val_loss: 0.4110 - val_accuracy: 0.8035\n",
      "Epoch 19/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4414 - accuracy: 0.7904 - val_loss: 0.4099 - val_accuracy: 0.8035\n",
      "Epoch 20/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4291 - accuracy: 0.8004 - val_loss: 0.4097 - val_accuracy: 0.8035\n",
      "Epoch 21/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4278 - accuracy: 0.8002 - val_loss: 0.4090 - val_accuracy: 0.8035\n",
      "Epoch 22/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4375 - accuracy: 0.7990 - val_loss: 0.4091 - val_accuracy: 0.8035\n",
      "Epoch 23/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4296 - accuracy: 0.7921 - val_loss: 0.4085 - val_accuracy: 0.8035\n",
      "Epoch 24/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4352 - accuracy: 0.7914 - val_loss: 0.4085 - val_accuracy: 0.8035\n",
      "Epoch 25/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4412 - accuracy: 0.7893 - val_loss: 0.4077 - val_accuracy: 0.8035\n",
      "Epoch 26/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4437 - accuracy: 0.7924 - val_loss: 0.4075 - val_accuracy: 0.8035\n",
      "Epoch 27/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4307 - accuracy: 0.7928 - val_loss: 0.4079 - val_accuracy: 0.8035\n",
      "Epoch 28/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4279 - accuracy: 0.7952 - val_loss: 0.4077 - val_accuracy: 0.8035\n",
      "Epoch 29/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4329 - accuracy: 0.7962 - val_loss: 0.4082 - val_accuracy: 0.8035\n",
      "Epoch 30/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.7788 - val_loss: 0.4065 - val_accuracy: 0.8035\n",
      "Epoch 31/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4428 - accuracy: 0.7907 - val_loss: 0.4069 - val_accuracy: 0.8035\n",
      "Epoch 32/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4484 - accuracy: 0.7899 - val_loss: 0.4068 - val_accuracy: 0.8035\n",
      "Epoch 33/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4230 - accuracy: 0.8029 - val_loss: 0.4074 - val_accuracy: 0.8035\n",
      "Epoch 34/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4431 - accuracy: 0.7949 - val_loss: 0.4070 - val_accuracy: 0.8035\n",
      "Epoch 35/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4396 - accuracy: 0.7950 - val_loss: 0.4066 - val_accuracy: 0.8035\n",
      "Epoch 36/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4515 - accuracy: 0.7838 - val_loss: 0.4062 - val_accuracy: 0.8035\n",
      "Epoch 37/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4450 - accuracy: 0.7923 - val_loss: 0.4064 - val_accuracy: 0.8035\n",
      "Epoch 38/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.7841 - val_loss: 0.4058 - val_accuracy: 0.8035\n",
      "Epoch 39/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.7960 - val_loss: 0.4063 - val_accuracy: 0.8035\n",
      "Epoch 40/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4333 - accuracy: 0.7933 - val_loss: 0.4063 - val_accuracy: 0.8035\n",
      "Epoch 41/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.7946 - val_loss: 0.4059 - val_accuracy: 0.8035\n",
      "Epoch 42/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4293 - accuracy: 0.7960 - val_loss: 0.4051 - val_accuracy: 0.8035\n",
      "Epoch 43/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8012 - val_loss: 0.4065 - val_accuracy: 0.8035\n",
      "Epoch 44/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.7973 - val_loss: 0.4053 - val_accuracy: 0.8035\n",
      "Epoch 45/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.7997 - val_loss: 0.4053 - val_accuracy: 0.8035\n",
      "Epoch 46/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4312 - accuracy: 0.7932 - val_loss: 0.4060 - val_accuracy: 0.8035\n",
      "Epoch 47/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.7968 - val_loss: 0.4061 - val_accuracy: 0.8035\n",
      "Epoch 48/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.7975 - val_loss: 0.4045 - val_accuracy: 0.8035\n",
      "Epoch 49/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4522 - accuracy: 0.7838 - val_loss: 0.4055 - val_accuracy: 0.8035\n",
      "Epoch 50/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.7970 - val_loss: 0.4052 - val_accuracy: 0.8035\n",
      "Epoch 51/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4264 - accuracy: 0.7958 - val_loss: 0.4052 - val_accuracy: 0.8035\n",
      "Epoch 52/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.7963 - val_loss: 0.4052 - val_accuracy: 0.8035\n",
      "Epoch 53/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.7947 - val_loss: 0.4047 - val_accuracy: 0.8035\n",
      "Epoch 54/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.7964 - val_loss: 0.4054 - val_accuracy: 0.8035\n",
      "Epoch 55/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.7956 - val_loss: 0.4047 - val_accuracy: 0.8035\n",
      "Epoch 56/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.7907 - val_loss: 0.4042 - val_accuracy: 0.8035\n",
      "Epoch 57/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.7974 - val_loss: 0.4048 - val_accuracy: 0.8035\n",
      "Epoch 58/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.7930 - val_loss: 0.4041 - val_accuracy: 0.8035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.7985 - val_loss: 0.4042 - val_accuracy: 0.8035\n",
      "Epoch 60/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8010 - val_loss: 0.4044 - val_accuracy: 0.8035\n",
      "Epoch 61/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.7962 - val_loss: 0.4044 - val_accuracy: 0.8035\n",
      "Epoch 62/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.7969 - val_loss: 0.4041 - val_accuracy: 0.8035\n",
      "Epoch 63/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.7921 - val_loss: 0.4035 - val_accuracy: 0.8035\n",
      "Epoch 64/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.7907 - val_loss: 0.4034 - val_accuracy: 0.8035\n",
      "Epoch 65/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4272 - accuracy: 0.7981 - val_loss: 0.4032 - val_accuracy: 0.8035\n",
      "Epoch 66/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4428 - accuracy: 0.7912 - val_loss: 0.4034 - val_accuracy: 0.8035\n",
      "Epoch 67/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.7909 - val_loss: 0.4031 - val_accuracy: 0.8035\n",
      "Epoch 68/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.7930 - val_loss: 0.4032 - val_accuracy: 0.8035\n",
      "Epoch 69/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.7899 - val_loss: 0.4030 - val_accuracy: 0.8035\n",
      "Epoch 70/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.7942 - val_loss: 0.4033 - val_accuracy: 0.8035\n",
      "Epoch 71/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4322 - accuracy: 0.7917 - val_loss: 0.4033 - val_accuracy: 0.8035\n",
      "Epoch 72/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.7944 - val_loss: 0.4032 - val_accuracy: 0.8035\n",
      "Epoch 73/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4288 - accuracy: 0.7934 - val_loss: 0.4035 - val_accuracy: 0.8035\n",
      "Epoch 74/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.7958 - val_loss: 0.4029 - val_accuracy: 0.8035\n",
      "Epoch 75/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4408 - accuracy: 0.7941 - val_loss: 0.4030 - val_accuracy: 0.8035\n",
      "Epoch 76/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.7904 - val_loss: 0.4027 - val_accuracy: 0.8035\n",
      "Epoch 77/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.7985 - val_loss: 0.4031 - val_accuracy: 0.8035\n",
      "Epoch 78/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.7922 - val_loss: 0.4023 - val_accuracy: 0.8035\n",
      "Epoch 79/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4481 - accuracy: 0.7928 - val_loss: 0.4023 - val_accuracy: 0.8035\n",
      "Epoch 80/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.8002 - val_loss: 0.4019 - val_accuracy: 0.8035\n",
      "Epoch 81/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.7953 - val_loss: 0.4023 - val_accuracy: 0.8035\n",
      "Epoch 82/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4230 - accuracy: 0.8021 - val_loss: 0.4033 - val_accuracy: 0.8035\n",
      "Epoch 83/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4236 - accuracy: 0.7953 - val_loss: 0.4027 - val_accuracy: 0.8035\n",
      "Epoch 84/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4308 - accuracy: 0.7962 - val_loss: 0.4015 - val_accuracy: 0.8035\n",
      "Epoch 85/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4395 - accuracy: 0.7894 - val_loss: 0.4022 - val_accuracy: 0.8035\n",
      "Epoch 86/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4336 - accuracy: 0.7954 - val_loss: 0.4023 - val_accuracy: 0.8035\n",
      "Epoch 87/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4295 - accuracy: 0.7948 - val_loss: 0.4021 - val_accuracy: 0.8035\n",
      "Epoch 88/300\n",
      "63/63 [==============================] - 1s 8ms/step - loss: 0.4435 - accuracy: 0.7915 - val_loss: 0.4021 - val_accuracy: 0.8035\n",
      "Epoch 89/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4310 - accuracy: 0.7976 - val_loss: 0.4017 - val_accuracy: 0.8035\n",
      "Epoch 90/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4271 - accuracy: 0.7958 - val_loss: 0.4022 - val_accuracy: 0.8035\n",
      "Epoch 91/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4391 - accuracy: 0.7933 - val_loss: 0.4013 - val_accuracy: 0.8035\n",
      "Epoch 92/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.7913 - val_loss: 0.4016 - val_accuracy: 0.8035\n",
      "Epoch 93/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4420 - accuracy: 0.7910 - val_loss: 0.4012 - val_accuracy: 0.8035\n",
      "Epoch 94/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4288 - accuracy: 0.7906 - val_loss: 0.4013 - val_accuracy: 0.8035\n",
      "Epoch 95/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4405 - accuracy: 0.7922 - val_loss: 0.4011 - val_accuracy: 0.8035\n",
      "Epoch 96/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4254 - accuracy: 0.7960 - val_loss: 0.4009 - val_accuracy: 0.8035\n",
      "Epoch 97/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4234 - accuracy: 0.7979 - val_loss: 0.4017 - val_accuracy: 0.8035\n",
      "Epoch 98/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4329 - accuracy: 0.7999 - val_loss: 0.4010 - val_accuracy: 0.8035\n",
      "Epoch 99/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4416 - accuracy: 0.7921 - val_loss: 0.4010 - val_accuracy: 0.8035\n",
      "Epoch 100/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4507 - accuracy: 0.8071 - val_loss: 0.4007 - val_accuracy: 0.8035\n",
      "Epoch 101/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4290 - accuracy: 0.7980 - val_loss: 0.4012 - val_accuracy: 0.8035\n",
      "Epoch 102/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4435 - accuracy: 0.7937 - val_loss: 0.4005 - val_accuracy: 0.8035\n",
      "Epoch 103/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4323 - accuracy: 0.7967 - val_loss: 0.4011 - val_accuracy: 0.8275\n",
      "Epoch 104/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4354 - accuracy: 0.8103 - val_loss: 0.4005 - val_accuracy: 0.8035\n",
      "Epoch 105/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4357 - accuracy: 0.7988 - val_loss: 0.4007 - val_accuracy: 0.8035\n",
      "Epoch 106/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4341 - accuracy: 0.7962 - val_loss: 0.4003 - val_accuracy: 0.8035\n",
      "Epoch 107/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4415 - accuracy: 0.7879 - val_loss: 0.4007 - val_accuracy: 0.8035\n",
      "Epoch 108/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4294 - accuracy: 0.7984 - val_loss: 0.4007 - val_accuracy: 0.8035\n",
      "Epoch 109/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4337 - accuracy: 0.7934 - val_loss: 0.4004 - val_accuracy: 0.8035\n",
      "Epoch 110/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4384 - accuracy: 0.7958 - val_loss: 0.4003 - val_accuracy: 0.8035\n",
      "Epoch 111/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8021 - val_loss: 0.3996 - val_accuracy: 0.8035\n",
      "Epoch 112/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4319 - accuracy: 0.8123 - val_loss: 0.4002 - val_accuracy: 0.8290\n",
      "Epoch 113/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.8146 - val_loss: 0.4002 - val_accuracy: 0.8035\n",
      "Epoch 114/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4414 - accuracy: 0.7913 - val_loss: 0.3997 - val_accuracy: 0.8290\n",
      "Epoch 115/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4373 - accuracy: 0.8129 - val_loss: 0.4007 - val_accuracy: 0.8295\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4296 - accuracy: 0.8218 - val_loss: 0.4001 - val_accuracy: 0.8035\n",
      "Epoch 117/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4350 - accuracy: 0.8166 - val_loss: 0.4006 - val_accuracy: 0.8315\n",
      "Epoch 118/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4187 - accuracy: 0.8164 - val_loss: 0.4007 - val_accuracy: 0.8315\n",
      "Epoch 119/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8260 - val_loss: 0.3994 - val_accuracy: 0.8280\n",
      "Epoch 120/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8243 - val_loss: 0.4000 - val_accuracy: 0.8280\n",
      "Epoch 121/300\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.4508 - accuracy: 0.8102 - val_loss: 0.3998 - val_accuracy: 0.8035\n",
      "Epoch 122/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4382 - accuracy: 0.8024 - val_loss: 0.4000 - val_accuracy: 0.8280\n",
      "Epoch 123/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4314 - accuracy: 0.8121 - val_loss: 0.3999 - val_accuracy: 0.8290\n",
      "Epoch 124/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.8271 - val_loss: 0.4004 - val_accuracy: 0.8290\n",
      "Epoch 125/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4207 - accuracy: 0.8259 - val_loss: 0.4006 - val_accuracy: 0.8290\n",
      "Epoch 126/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8218 - val_loss: 0.4003 - val_accuracy: 0.8280\n",
      "Epoch 127/300\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.4287 - accuracy: 0.8177 - val_loss: 0.4006 - val_accuracy: 0.8285\n",
      "Epoch 128/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4325 - accuracy: 0.8244 - val_loss: 0.3998 - val_accuracy: 0.8285\n",
      "Epoch 129/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8152 - val_loss: 0.3998 - val_accuracy: 0.8305\n",
      "Epoch 130/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4344 - accuracy: 0.8176 - val_loss: 0.4005 - val_accuracy: 0.8315\n",
      "Epoch 131/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.8107 - val_loss: 0.3992 - val_accuracy: 0.8035\n",
      "Epoch 132/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4266 - accuracy: 0.8064 - val_loss: 0.3997 - val_accuracy: 0.8305\n",
      "Epoch 133/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4423 - accuracy: 0.8174 - val_loss: 0.3989 - val_accuracy: 0.8285\n",
      "Epoch 134/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8145 - val_loss: 0.3992 - val_accuracy: 0.8300\n",
      "Epoch 135/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4352 - accuracy: 0.8182 - val_loss: 0.3994 - val_accuracy: 0.8285\n",
      "Epoch 136/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4322 - accuracy: 0.8179 - val_loss: 0.3998 - val_accuracy: 0.8320\n",
      "Epoch 137/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4252 - accuracy: 0.8268 - val_loss: 0.3989 - val_accuracy: 0.8300\n",
      "Epoch 138/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4200 - accuracy: 0.8227 - val_loss: 0.3996 - val_accuracy: 0.8315\n",
      "Epoch 139/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4351 - accuracy: 0.8193 - val_loss: 0.3988 - val_accuracy: 0.8035\n",
      "Epoch 140/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4359 - accuracy: 0.8092 - val_loss: 0.3986 - val_accuracy: 0.8295\n",
      "Epoch 141/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4205 - accuracy: 0.8070 - val_loss: 0.3995 - val_accuracy: 0.8315\n",
      "Epoch 142/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8218 - val_loss: 0.3984 - val_accuracy: 0.8305\n",
      "Epoch 143/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4324 - accuracy: 0.8187 - val_loss: 0.3993 - val_accuracy: 0.8315\n",
      "Epoch 144/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.8220 - val_loss: 0.3988 - val_accuracy: 0.8325\n",
      "Epoch 145/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8264 - val_loss: 0.3987 - val_accuracy: 0.8335\n",
      "Epoch 146/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4350 - accuracy: 0.8304 - val_loss: 0.3982 - val_accuracy: 0.8340\n",
      "Epoch 147/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4243 - accuracy: 0.8289 - val_loss: 0.3987 - val_accuracy: 0.8335\n",
      "Epoch 148/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.8231 - val_loss: 0.3986 - val_accuracy: 0.8310\n",
      "Epoch 149/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4237 - accuracy: 0.8359 - val_loss: 0.3992 - val_accuracy: 0.8320\n",
      "Epoch 150/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.8134 - val_loss: 0.3984 - val_accuracy: 0.8295\n",
      "Epoch 151/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.8236 - val_loss: 0.3980 - val_accuracy: 0.8305\n",
      "Epoch 152/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4221 - accuracy: 0.8345 - val_loss: 0.3989 - val_accuracy: 0.8325\n",
      "Epoch 153/300\n",
      "63/63 [==============================] - 0s 8ms/step - loss: 0.4320 - accuracy: 0.8243 - val_loss: 0.3982 - val_accuracy: 0.8320\n",
      "Epoch 154/300\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8331 - val_loss: 0.3997 - val_accuracy: 0.8305\n",
      "Epoch 155/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4297 - accuracy: 0.8245 - val_loss: 0.3990 - val_accuracy: 0.8320\n",
      "Epoch 156/300\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.4268 - accuracy: 0.8273 - val_loss: 0.3996 - val_accuracy: 0.8315\n",
      "Epoch 157/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.8254 - val_loss: 0.3988 - val_accuracy: 0.8310\n",
      "Epoch 158/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4343 - accuracy: 0.8292 - val_loss: 0.3986 - val_accuracy: 0.8325\n",
      "Epoch 159/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8252 - val_loss: 0.3990 - val_accuracy: 0.8330\n",
      "Epoch 160/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4384 - accuracy: 0.8201 - val_loss: 0.3982 - val_accuracy: 0.8315\n",
      "Epoch 161/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4296 - accuracy: 0.8146 - val_loss: 0.3989 - val_accuracy: 0.8320\n",
      "Epoch 162/300\n",
      "63/63 [==============================] - 0s 7ms/step - loss: 0.4335 - accuracy: 0.8217 - val_loss: 0.3996 - val_accuracy: 0.8315\n",
      "Epoch 163/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8219 - val_loss: 0.3984 - val_accuracy: 0.8305\n",
      "Epoch 164/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4280 - accuracy: 0.8265 - val_loss: 0.3984 - val_accuracy: 0.8310\n",
      "Epoch 165/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4302 - accuracy: 0.8266 - val_loss: 0.3987 - val_accuracy: 0.8310\n",
      "Epoch 166/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4361 - accuracy: 0.8248 - val_loss: 0.3986 - val_accuracy: 0.8300\n",
      "Epoch 167/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4274 - accuracy: 0.8247 - val_loss: 0.3985 - val_accuracy: 0.8305\n",
      "Epoch 168/300\n",
      "63/63 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.8222 - val_loss: 0.3991 - val_accuracy: 0.8310\n",
      "Epoch 169/300\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 0.4251 - accuracy: 0.8248 - val_loss: 0.3990 - val_accuracy: 0.8325\n",
      "Epoch 170/300\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 0.4390 - accuracy: 0.8245 - val_loss: 0.3993 - val_accuracy: 0.8330\n",
      "Epoch 171/300\n",
      "63/63 [==============================] - 0s 5ms/step - loss: 0.4296 - accuracy: 0.8270 - val_loss: 0.3989 - val_accuracy: 0.8320\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1708e2bcdc0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting the ANN to the data with Early Stopping and Tensorboard\n",
    "model.fit(X_scaled_train,y_train,batch_size=128,epochs=300,validation_data=(X_scaled_test,y_test),\n",
    "         callbacks=[early_stop,board])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.679953</td>\n",
       "      <td>0.7885</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.615881</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.547120</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501823</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.446258</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.461926</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.426776</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449892</td>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.8035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  0.679953    0.7885  0.658377        0.8035\n",
       "1  0.615881    0.7945  0.547120        0.8035\n",
       "2  0.501823    0.7945  0.446258        0.8035\n",
       "3  0.461926    0.7945  0.426776        0.8035\n",
       "4  0.449892    0.7945  0.421848        0.8035"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Losses and Accuracy from Model\n",
    "loss_accuracy = pd.DataFrame(model.history.history)\n",
    "loss_accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = loss_accuracy[['loss','val_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.679953</td>\n",
       "      <td>0.658377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.615881</td>\n",
       "      <td>0.547120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.501823</td>\n",
       "      <td>0.446258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.461926</td>\n",
       "      <td>0.426776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449892</td>\n",
       "      <td>0.421848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  val_loss\n",
       "0  0.679953  0.658377\n",
       "1  0.615881  0.547120\n",
       "2  0.501823  0.446258\n",
       "3  0.461926  0.426776\n",
       "4  0.449892  0.421848"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pulkit\\anaconda3\\lib\\site-packages\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "#Predicting the test results\n",
    "y_pred = model.predict_classes(X_scaled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as above\n",
    "y_pred = (model.predict(X_scaled_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1573   34]\n",
      " [ 302   91]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90      1607\n",
      "           1       0.73      0.23      0.35       393\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.78      0.61      0.63      2000\n",
      "weighted avg       0.82      0.83      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.832"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.3989 - accuracy: 0.8320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39894792437553406, 0.8320000171661377]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_scaled_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model is reaching loss of 40% and accuracy of approx. 83%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('customer_exit_predictor_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = load_model('customer_exit_predictor_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'accuracy']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 2ms/step - loss: 0.3989 - accuracy: 0.8320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39894792437553406, 0.8320000171661377]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.evaluate(X_scaled_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Germany</th>\n",
       "      <th>Spain</th>\n",
       "      <th>Male</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>596</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>96709.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41788.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>623</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146379.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>58561.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "      <td>119152.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170679.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>560</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>124995.98</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>114669.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CreditScore  Germany  Spain  Male  Age  Tenure    Balance  \\\n",
       "6252          596        1      0     1   32       3   96709.07   \n",
       "4684          623        0      0     1   43       1       0.00   \n",
       "1731          601        0      1     0   44       4       0.00   \n",
       "4742          506        1      0     1   59       8  119152.10   \n",
       "4521          560        0      1     0   27       7  124995.98   \n",
       "\n",
       "      NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  \n",
       "6252              2          0               0         41788.37  \n",
       "4684              2          1               1        146379.30  \n",
       "1731              2          1               0         58561.31  \n",
       "4742              2          1               1        170679.74  \n",
       "4521              1          1               1        114669.79  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditScore          596.00\n",
       "Germany                1.00\n",
       "Spain                  0.00\n",
       "Male                   1.00\n",
       "Age                   32.00\n",
       "Tenure                 3.00\n",
       "Balance            96709.07\n",
       "NumOfProducts          2.00\n",
       "HasCrCard              0.00\n",
       "IsActiveMember         0.00\n",
       "EstimatedSalary    41788.37\n",
       "Name: 6252, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of new data\n",
    "new_data = X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_scaled = scaler.transform([new_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57749609,  1.72572313, -0.57638802,  0.91324755, -0.6557859 ,\n",
       "        -0.69539349,  0.32993735,  0.80843615, -1.54035103, -1.02583358,\n",
       "        -1.01960511]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pulkit\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting on new data\n",
    "trained_model.predict_classes(new_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same as above\n",
    "(model.predict(new_data_scaled) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**According to our model prediction, customer will not exit the bank**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
